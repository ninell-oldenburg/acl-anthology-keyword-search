{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1760bcb8",
   "metadata": {},
   "source": [
    "# importing acl-anthology bib file & creating csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# download anthology file\n",
    "url = \"https://aclanthology.org/anthology+abstracts.bib.gz\"\n",
    "r = requests.get(url)\n",
    "open('anthology+abstracts.bib.gz', 'wb').write(r.content)\n",
    "\n",
    "# open and unpack gz\n",
    "with gzip.open('anthology+abstracts.bib.gz', 'rb') as f_in:\n",
    "    with open('anthology+abstracts.bib', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "os.remove(\"anthology+abstracts.bib.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch out: computes very long\n",
    "# if you've run this + the following cell once, there should e a csv in your directory\n",
    "# in that case you can uncomment the cell after the next one and read straight from the csv\n",
    "import bibtexparser\n",
    "\n",
    "# parse bib file, output: list of dicts\n",
    "with open('anthology+abstracts.bib') as bibtex_file:   \n",
    "    bib_database = bibtexparser.bparser.BibTexParser(common_strings=True).parse_file(bibtex_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define keywords & words to exclude\n",
    "keywords = ['fair', 'fairness', 'race', 'gender', 'bias', 'biases', 'protected attribute', 'protected categor']\n",
    "excludes = ['hate', 'hate speech']\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for paper in bib_database.entries:\n",
    "    if 'abstract' in paper.keys():\n",
    "        if any(keyword in paper['abstract'] for keyword in keywords):\n",
    "            if not any(exclude in paper['abstract'] for exclude in excludes):\n",
    "                temp_row = {}\n",
    "\n",
    "                # select only select papers from 2016 on\n",
    "                if 'year' in paper.keys() and int(paper['year']) >= 2016:\n",
    "\n",
    "                    for cat in paper.keys():\n",
    "                        temp_row[cat] = paper[cat]\n",
    "\n",
    "                    df = df.append(temp_row, ignore_index=True)\n",
    "        \n",
    "df.to_csv('bias_paper.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe988cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if csv is already in directory (and save time parsing the bib file)\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv('bias_paper.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73c33c",
   "metadata": {},
   "source": [
    "# tf-idf & clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b35e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration from: https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \n",
    "    # remove links, special characters, numbers, stopwords, whitespaces\n",
    "    # also remove words that are actually filters\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    #stopwords_list.extend(['approach', 'bias', 'biases', 'data', 'fair', 'fairness', 'human', \n",
    "     #                      'language', 'languages', 'method', 'paper', 'task', 'tasks', 'well', 'word',\n",
    "      #                    'words'])\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    if remove_stopwords:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [w for w in tokens if not w.lower() in stopwords_list]\n",
    "        text = \" \".join(tokens)\n",
    "    \n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62473dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cleaned column\n",
    "df['cleaned'] = df['abstract'].apply(lambda x: preprocess_text(x, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.2, max_features=80)\n",
    "X = vectorizer.fit_transform(df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# vary here the number of clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# initialize PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
    "pca_vecs = pca.fit_transform(X.toarray())\n",
    "# save our two dimensions into x0 and x1\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c8cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign clusters and pca vectors to our dataframe \n",
    "df['cluster'] = clusters\n",
    "df['x0'] = x0\n",
    "df['x1'] = x1\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f6bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = vectorizer.get_feature_names() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb31424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map clusters to appropriate labels \n",
    "cluster_map = {0: \"cluster 01\", \n",
    "               1: \"cluster 02\", \n",
    "               2: \"cluster 03\", \n",
    "               3: \"cluster 04\", \n",
    "               4: \"cluster 05\", \n",
    "               5: \"cluster 06\", \n",
    "               #6: \"cluster 07\", \n",
    "               #7: \"cluster 08\", \n",
    "               #8: \"cluster 09\", \n",
    "               #9: \"cluster 10\"\n",
    "              }\n",
    "# apply mapping\n",
    "df['cluster'] = df['cluster'].map(cluster_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcc8d2",
   "metadata": {},
   "source": [
    "# visualizing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb502c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set image size\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.title(\"TF-IDF + KMeans bias abstracts clustering\", fontdict={\"fontsize\": 18})\n",
    "# set axes names\n",
    "plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
    "plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
    "# create scatter plot with seaborn, where hue is the class used to group the data\n",
    "sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand crafted bias list (feel free to change)\n",
    "bias_list = ['race', 'gender', 'social', 'ethnic', 'religion']\n",
    "\n",
    "for i in bias_list:\n",
    "    df[i] = df['cleaned'].str.contains(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880993f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster per bias\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "for i in bias_list:\n",
    "    temp_df[i] = pd.crosstab(df[i],df['cluster']).loc[True]\n",
    "    \n",
    "temp_df.transpose().plot.bar();\n",
    "plt.title('Paper per bias term by cluster')\n",
    "plt.ylabel(\"# of paper\")\n",
    "plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffed1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year per cluster\n",
    "pd.crosstab(df['year'],df['cluster']).plot.bar();\n",
    "plt.title('Paper per year by cluster', fontdict={\"fontsize\": 10})\n",
    "plt.xlabel(\"year\", fontdict={\"fontsize\": 12})\n",
    "plt.ylabel(\"# of paper\")\n",
    "plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias per year\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "for i in bias_list:\n",
    "    temp_df[i] = pd.crosstab(df[i],df['year']).loc[True]\n",
    "    \n",
    "temp_df.transpose().plot.bar();\n",
    "plt.title('Paper per bias term by year')\n",
    "plt.ylabel(\"# of paper\")\n",
    "plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d13fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
