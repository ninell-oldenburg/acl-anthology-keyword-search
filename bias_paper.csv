,ENTRYTYPE,ID,abstract,address,author,booktitle,doi,month,pages,publisher,title,url,year,journal,volume,number,language,isbn
0,inproceedings,chuang-etal-2021-mitigating,"Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.",Online,"Chuang, Yung-Sung  and
Gao, Mingye  and
Luo, Hongyin  and
Glass, James  and
Lee, Hung-yi  and
Chen, Yun-Nung  and
Li, Shang-Wen",Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),10.18653/v1/2021.woah-1.12,August,114--120,Association for Computational Linguistics,Mitigating Biases in Toxic Language Detection through Invariant Rationalization,https://aclanthology.org/2021.woah-1.12,2021,,,,,
1,inproceedings,lent-sogaard-2021-common,"Large-scale language models such as ELMo and BERT have pushed the horizon of what is possible in semantic role labeling (SRL), solving the out-of-vocabulary problem and enabling end-to-end systems, but they have also introduced significant biases. We evaluate three SRL parsers on very simple transitive sentences with verbs usually associated with animate subjects and objects, such as, {``}Mary babysat Tom{''}: a state-of-the-art parser based on BERT, an older parser based on GloVe, and an even older parser from before the days of word embeddings. When arguments are word forms predominantly used as person names, aligning with common sense expectations of animacy, the BERT-based parser is unsurprisingly superior; yet, with abstract or random nouns, the opposite picture emerges. We refer to this as {``}common sense bias{''} and present a challenge dataset for evaluating the extent to which parsers are sensitive to such a bias. Our code and challenge dataset are available here: github.com/coastalcph/comte",Online,"Lent, Heather  and
S{\o}gaard, Anders",Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),10.18653/v1/2021.wnut-1.14,November,114--119,Association for Computational Linguistics,Common Sense Bias in Semantic Role Labeling,https://aclanthology.org/2021.wnut-1.14,2021,,,,,
2,inproceedings,murayama-etal-2021-mitigation,"Fake news causes significant damage to society. To deal with these fake news, several studies on building detection models and arranging datasets have been conducted. Most of the fake news datasets depend on a specific time period. Consequently, the detection models trained on such a dataset have difficulty detecting novel fake news generated by political changes and social changes; they may possibly result in biased output from the input, including specific person names and organizational names. We refer to this problem as Diachronic Bias because it is caused by the creation date of news in each dataset. In this study, we confirm the bias, especially proper nouns including person names, from the deviation of phrase appearances in each dataset. Based on these findings, we propose masking methods using Wikidata to mitigate the influence of person names and validate whether they make fake news detection models robust through experiments with in-domain and out-of-domain data.",Online,"Murayama, Taichi  and
Wakamiya, Shoko  and
Aramaki, Eiji",Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),10.18653/v1/2021.wnut-1.21,November,182--188,Association for Computational Linguistics,Mitigation of Diachronic Bias in Fake News Detection Dataset,https://aclanthology.org/2021.wnut-1.21,2021,,,,,
3,inproceedings,ghosh-etal-2021-detecting,"Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.",Online,"Ghosh, Sayan  and
Baker, Dylan  and
Jurgens, David  and
Prabhakaran, Vinodkumar",Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),10.18653/v1/2021.wnut-1.35,November,313--328,Association for Computational Linguistics,Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media,https://aclanthology.org/2021.wnut-1.35,2021,,,,,
4,inproceedings,bertsch-bethard-2021-detection,"On Wikipedia, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia{'}s editorial policies. Wikipedia{'}s policy on maintaining a neutral point of view has inspired recent research on bias detection, including {``}weasel words{''} and {``}hedges{''}. Yet to date, little work has been done on identifying {``}puffery,{''} phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a dataset by combining Wikipedia editorial annotations and information retrieval techniques. We compare several approaches to predicting puffery, and achieve 0.963 f1 score by incorporating citation features into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia{'}s public infrastructure to give back to the Wikipedia editor community.",Online,"Bertsch, Amanda  and
Bethard, Steven",Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),10.18653/v1/2021.wnut-1.36,November,329--333,Association for Computational Linguistics,Detection of Puffery on the {E}nglish {W}ikipedia,https://aclanthology.org/2021.wnut-1.36,2021,,,,,
5,inproceedings,subramanian-etal-2021-nvidia,"This paper provides an overview of NVIDIA NeMo{'}s neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model (CITATION). Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English $\leftrightarrow$ Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT{'}20 En-De test set outperforming the best submission from last year{'}s task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT{'}20 Biomedical Task Test set, outperforming the previous year{'}s best submissions.",Online,"Subramanian, Sandeep  and
Hrinchuk, Oleksii  and
Adams, Virginia  and
Kuchaiev, Oleksii",Proceedings of the Sixth Conference on Machine Translation,,November,197--204,Association for Computational Linguistics,{NVIDIA} {N}e{M}o{'}s Neural Machine Translation Systems for {E}nglish-{G}erman and {E}nglish-{R}ussian News and Biomedical Tasks at {WMT}21,https://aclanthology.org/2021.wmt-1.18,2021,,,,,
6,inproceedings,wang-etal-2021-tencent,"This paper describes Tencent Translation systems for the WMT21 shared task. We participate in the news translation task on three language pairs: Chinese-English, English-Chinese and German-English. Our systems are built on various Transformer models with novel techniques adapted from our recent research work. First, we combine different data augmentation methods including back-translation, forward-translation and right-to-left training to enlarge the training data. We also apply language coverage bias, data rejuvenation and uncertainty-based sampling approaches to select content-relevant and high-quality data from large parallel and monolingual corpora. Expect for in-domain fine-tuning, we also propose a fine-grained {``}one model one domain{''} approach to model characteristics of different news genres at fine-tuning and decoding stages. Besides, we use greed-based ensemble algorithm and transductive ensemble method to further boost our systems. Based on our success in the last WMT, we continuously employed advanced techniques such as large batch training, data selection and data filtering. Finally, our constrained Chinese-English system achieves 33.4 case-sensitive BLEU score, which is the highest among all submissions. The German-English system is ranked at second place accordingly.",Online,"Wang, Longyue  and
Li, Mu  and
Liu, Fangxu  and
Shi, Shuming  and
Tu, Zhaopeng  and
Wang, Xing  and
Wu, Shuangzhi  and
Zeng, Jiali  and
Zhang, Wen",Proceedings of the Sixth Conference on Machine Translation,,November,216--224,Association for Computational Linguistics,Tencent Translation System for the {WMT}21 News Translation Task,https://aclanthology.org/2021.wmt-1.20,2021,,,,,
7,inproceedings,troles-schmid-2021-extending,"Human gender bias is reflected in language and text production. Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT. For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System. Recent research showed that MT systems are biased towards stereotypical translation of occupations. In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published. In this set measurement of gender bias is solely based on the translation of occupations. With our paper we present an extension of this challenge set, called WiBeMT, which adds gender-biased adjectives and sentences with gender-biased verbs. The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate. Results show a gender bias for all three MT systems. This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.",Online,"Troles, Jonas-Dario  and
Schmid, Ute",Proceedings of the Sixth Conference on Machine Translation,,November,531--541,Association for Computational Linguistics,Extending Challenge Sets to Uncover Gender Bias in Machine Translation: Impact of Stereotypical Verbs and Adjectives,https://aclanthology.org/2021.wmt-1.61,2021,,,,,
8,inproceedings,kumar-etal-2021-learning-feature,"Large web-crawled corpora represent an excellent resource for improving the performance of Neural Machine Translation (NMT) systems across several language pairs. However, since these corpora are typically extremely noisy, their use is fairly limited. Current approaches to deal with this problem mainly focus on filtering using heuristics or single features such as language model scores or bi-lingual similarity. This work presents an alternative approach which learns weights for multiple sentence-level features. These feature weights which are optimized directly for the task of improving translation performance, are used to score and filter sentences in the noisy corpora more effectively. We provide results of applying this technique to building NMT systems using the Paracrawl corpus for Estonian-English and show that it beats strong single feature baselines and hand designed combinations. Additionally, we analyze the sensitivity of this method to different types of noise and explore if the learned weights generalize to other language pairs using the Maltese-English Paracrawl corpus.",Online,"Kumar, Gaurav  and
Koehn, Philipp  and
Khudanpur, Sanjeev",Proceedings of the Sixth Conference on Machine Translation,,November,1100--1109,Association for Computational Linguistics,Learning Feature Weights using Reward Modeling for Denoising Parallel Corpora,https://aclanthology.org/2021.wmt-1.118,2021,,,,,
9,inproceedings,dayanik-pado-2021-disentangling,"Text classification is a central tool in NLP. However, when the target classes are strongly correlated with other textual attributes, text classification models can pick up {``}wrong{''} features, leading to bad generalization and biases. In social media analysis, this problem surfaces for demographic user classes such as language, topic, or gender, which influence the generate text to a substantial extent. Adversarial training has been claimed to mitigate this problem, but thorough evaluation is missing. In this paper, we experiment with text classification of the correlated attributes of document topic and author gender, using a novel multilingual parallel corpus of TED talk transcripts. Our findings are: (a) individual classifiers for topic and author gender are indeed biased; (b) debiasing with adversarial training works for topic, but breaks down for author gender; (c) gender debiasing results differ across languages. We interpret the result in terms of feature space overlap, highlighting the role of linguistic surface realization of the target classes.",Online,"Dayanik, Erenay  and
Pad{\'o}, Sebastian","Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",,April,50--61,Association for Computational Linguistics,Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing,https://aclanthology.org/2021.wassa-1.6,2021,,,,,
10,inproceedings,culnan-etal-2021-ire,"In deployment, systems that use speech as input must make use of automated transcriptions. Yet, typically when these systems are evaluated, gold transcriptions are assumed. We explicitly examine the impact of transcription errors on the downstream performance of a multi-modal system on three related tasks from three datasets: emotion, sarcasm, and personality detection. We include three separate transcription tools and show that while all automated transcriptions propagate errors that substantially impact downstream performance, the open-source tools fair worse than the paid tool, though not always straightforwardly, and word error rates do not correlate well with downstream performance. We further find that the inclusion of audio features partially mitigates transcription errors, but that a naive usage of a multi-task setup does not.",Online,"Culnan, John  and
Park, Seongjin  and
Krishnaswamy, Meghavarshini  and
Sharp, Rebecca","Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",,April,250--256,Association for Computational Linguistics,"Me, myself, and ire: Effects of automatic transcription quality on emotion, sarcasm, and personality detection",https://aclanthology.org/2021.wassa-1.26,2021,,,,,
11,inproceedings,seelawi-etal-2021-alue,"The emergence of Multi-task learning (MTL)models in recent years has helped push thestate of the art in Natural Language Un-derstanding (NLU). We strongly believe thatmany NLU problems in Arabic are especiallypoised to reap the benefits of such models. Tothis end we propose the Arabic Language Un-derstanding Evaluation Benchmark (ALUE),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark.We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels.Our initial experiments show thatMTL models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community,we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inArabic NLU. We hope that ALUE will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online,and publicly accessible leaderboard.","Kyiv, Ukraine (Virtual)","Seelawi, Haitham  and
Tuffaha, Ibraheem  and
Gzawi, Mahmoud  and
Farhan, Wael  and
Talafha, Bashar  and
Badawi, Riham  and
Sober, Zyad  and
Al-Dweik, Oday  and
Freihat, Abed Alhakim  and
Al-Natsheh, Hussein",Proceedings of the Sixth Arabic Natural Language Processing Workshop,,April,173--184,Association for Computational Linguistics,{ALUE}: {A}rabic Language Understanding Evaluation,https://aclanthology.org/2021.wanlp-1.18,2021,,,,,
12,inproceedings,azarpanah-farhadloo-2021-measuring,"Word embeddings are widely used in Natural Language Processing (NLP) for a vast range of applications. However, it has been consistently proven that these embeddings reflect the same human biases that exist in the data used to train them. Most of the introduced bias indicators to reveal word embeddings{'} bias are average-based indicators based on the cosine similarity measure. In this study, we examine the impacts of different similarity measures as well as other descriptive techniques than averaging in measuring the biases of contextual and non-contextual word embeddings. We show that the extent of revealed biases in word embeddings depends on the descriptive statistics and similarity measures used to measure the bias. We found that over the ten categories of word embedding association tests, Mahalanobis distance reveals the smallest bias, and Euclidean distance reveals the largest bias in word embeddings. In addition, the contextual models reveal less severe biases than the non-contextual word embedding models.",Online,"Azarpanah, Hossein  and
Farhadloo, Mohsen",Proceedings of the First Workshop on Trustworthy Natural Language Processing,10.18653/v1/2021.trustnlp-1.2,June,8--14,Association for Computational Linguistics,Measuring Biases of Word Embeddings: What Similarity Measures and Descriptive Statistics to Use?,https://aclanthology.org/2021.trustnlp-1.2,2021,,,,,
13,inproceedings,matthews-etal-2021-gender,"Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. Gender bias in NLP has been well studied in English, but has been less studied in other languages. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages that have grammatically gendered nouns including different feminine, masculine, and neuter profession words. We discuss future work that would benefit immensely from a computational linguistics perspective.",Online,"Matthews, Abigail  and
Grasso, Isabella  and
Mahoney, Christopher  and
Chen, Yan  and
Wali, Esma  and
Middleton, Thomas  and
Njie, Mariama  and
Matthews, Jeanna",Proceedings of the First Workshop on Trustworthy Natural Language Processing,10.18653/v1/2021.trustnlp-1.6,June,45--54,Association for Computational Linguistics,Gender Bias in Natural Language Processing Across Human Languages,https://aclanthology.org/2021.trustnlp-1.6,2021,,,,,
14,inproceedings,kumar-etal-2021-interpreting,"Many existing approaches for interpreting text classification models focus on providing importance scores for parts of the input text, such as words, but without a way to test or improve the interpretation method itself. This has the effect of compounding the problem of understanding or building trust in the model, with the interpretation method itself adding to the opacity of the model. Further, importance scores on individual examples are usually not enough to provide a sufficient picture of model behavior. To address these concerns, we propose MOXIE (MOdeling conteXt-sensitive InfluencE of words) with an aim to enable a richer interface for a user to interact with the model being interpreted and to produce testable predictions. In particular, we aim to make predictions for importance scores, counterfactuals and learned biases with MOXIE. In addition, with a global learning objective, MOXIE provides a clear path for testing and improving itself. We evaluate the reliability and efficiency of MOXIE on the task of sentiment analysis.",Online,"Kumar, Sawan  and
Dixit, Kalpit  and
Shah, Kashif",Proceedings of the First Workshop on Trustworthy Natural Language Processing,10.18653/v1/2021.trustnlp-1.7,June,55--67,Association for Computational Linguistics,Interpreting Text Classifiers by Learning Context-sensitive Influence of Words,https://aclanthology.org/2021.trustnlp-1.7,2021,,,,,
15,article,bogin-etal-2021-latent,"Abstract Answering questions that involve multi-step reasoning requires decomposing them and using the answers of intermediate steps to reach the final answer. However, state-of-the-art models in grounded question answering often do not explicitly perform decomposition, leading to difficulties in generalization to out-of-distribution examples. In this work, we propose a model that computes a representation and denotation for all question spans in a bottom-up, compositional manner using a CKY-style parser. Our model induces latent trees, driven by end-to-end (the answer) supervision only. We show that this inductive bias towards tree structures dramatically improves systematic generalization to out-of- distribution examples, compared to strong baselines on an arithmetic expressions benchmark as well as on C losure, a dataset that focuses on systematic generalization for grounded question answering. On this challenging dataset, our model reaches an accuracy of 96.1{\%}, significantly higher than prior models that almost perfectly solve the task on a random, in-distribution split.","Cambridge, MA","Bogin, Ben  and
Subramanian, Sanjay  and
Gardner, Matt  and
Berant, Jonathan",,10.1162/tacl_a_00361,,195--210,MIT Press,Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering,https://aclanthology.org/2021.tacl-1.12,2021,Transactions of the Association for Computational Linguistics,9,,,
16,article,ghaddar-etal-2021-context,"Abstract In this work, we examine the ability of NER models to use contextual information when predicting the type of an ambiguous entity. We introduce NRB, a new testbed carefully designed to diagnose Name Regularity Bias of NER models. Our results indicate that all state-of-the-art models we tested show such a bias; BERT fine-tuned models significantly outperforming feature-based (LSTM-CRF) ones on NRB, despite having comparable (sometimes lower) performance on standard benchmarks. To mitigate this bias, we propose a novel model-agnostic training method that adds learnable adversarial noise to some entity mentions, thus enforcing models to focus more strongly on the contextual signal, leading to significant gains on NRB. Combining it with two other training strategies, data augmentation and parameter freezing, leads to further gains.","Cambridge, MA","Ghaddar, Abbas  and
Langlais, Philippe  and
Rashid, Ahmad  and
Rezagholizadeh, Mehdi",,10.1162/tacl_a_00386,,586--604,MIT Press,Context-aware Adversarial Training for Name Regularity Bias in Named Entity Recognition,https://aclanthology.org/2021.tacl-1.36,2021,Transactions of the Association for Computational Linguistics,9,,,
17,article,savoldi-etal-2021-gender,"AbstractMachine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","Cambridge, MA","Savoldi, Beatrice  and
Gaido, Marco  and
Bentivogli, Luisa  and
Negri, Matteo  and
Turchi, Marco",,10.1162/tacl_a_00401,,845--874,MIT Press,Gender Bias in Machine Translation,https://aclanthology.org/2021.tacl-1.51,2021,Transactions of the Association for Computational Linguistics,9,,,
18,article,hahn-etal-2021-sensitivity,"Abstract We introduce a theoretical framework for understanding and predicting the complexity of sequence classification tasks, using a novel extension of the theory of Boolean function sensitivity. The sensitivity of a function, given a distribution over input sequences, quantifies the number of disjoint subsets of the input sequence that can each be individually changed to change the output. We argue that standard sequence classification methods are biased towards learning low-sensitivity functions, so that tasks requiring high sensitivity are more difficult. To that end, we show analytically that simple lexical classifiers can only express functions of bounded sensitivity, and we show empirically that low-sensitivity functions are easier to learn for LSTMs. We then estimate sensitivity on 15 NLP tasks, finding that sensitivity is higher on challenging tasks collected in GLUE than on simple text classification tasks, and that sensitivity predicts the performance both of simple lexical classifiers and of vanilla BiLSTMs without pretrained contextualized embeddings. Within a task, sensitivity predicts which inputs are hard for such simple models. Our results suggest that the success of massively pretrained contextual representations stems in part because they provide representations from which information can be extracted by low-sensitivity decoders.","Cambridge, MA","Hahn, Michael  and
Jurafsky, Dan  and
Futrell, Richard",,10.1162/tacl_a_00403,,891--908,MIT Press,Sensitivity as a Complexity Measure for Sequence Classification Tasks,https://aclanthology.org/2021.tacl-1.53,2021,Transactions of the Association for Computational Linguistics,9,,,
19,article,czarnowska-etal-2021-quantifying,"Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model{'}s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.","Cambridge, MA","Czarnowska, Paula  and
Vyas, Yogarshi  and
Shah, Kashif",,10.1162/tacl_a_00425,,1249--1267,MIT Press,Quantifying Social Biases in {NLP}: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics,https://aclanthology.org/2021.tacl-1.74,2021,Transactions of the Association for Computational Linguistics,9,,,
20,article,effland-collins-2021-partially,"Abstract We study learning named entity recognizers in the presence of missing entity annotations. We approach this setting as tagging with latent variables and propose a novel loss, the Expected Entity Ratio, to learn models in the presence of systematically missing tags. We show that our approach is both theoretically sound and empirically useful. Experimentally, we find that it meets or exceeds performance of strong and state-of-the-art baselines across a variety of languages, annotation scenarios, and amounts of labeled data. In particular, we find that it significantly outperforms the previous state-of-the-art methods from Mayhew et al. (2019) and Li et al. (2021) by +12.7 and +2.3 F1 score in a challenging setting with only 1,000 biased annotations, averaged across 7 datasets. We also show that, when combined with our approach, a novel sparse annotation scheme outperforms exhaustive annotation for modest annotation budgets.1","Cambridge, MA","Effland, Thomas  and
Collins, Michael",,10.1162/tacl_a_00429,,1320--1335,MIT Press,Partially Supervised Named Entity Recognition via the Expected Entity Ratio Loss,https://aclanthology.org/2021.tacl-1.78,2021,Transactions of the Association for Computational Linguistics,9,,,
21,article,schick-etal-2021-self,"Abstract âš  This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model{'}s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1","Cambridge, MA","Schick, Timo  and
Udupa, Sahana  and
Sch{\""u}tze, Hinrich",,10.1162/tacl_a_00434,,1408--1424,MIT Press,Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in {NLP},https://aclanthology.org/2021.tacl-1.84,2021,Transactions of the Association for Computational Linguistics,9,,,
22,inproceedings,he-etal-2021-distiller,"Knowledge Distillation (KD) offers a natural way to reduce the latency and memory/energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets/tasks, such as the data augmentation policy, the loss function, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component{'}s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MI-objective functions with better bias/variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following: 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks. Moreover, we find that different datasets/tasks prefer different KD algorithms, and thus propose a simple AutoDistiller algorithm that can recommend a good KD pipeline for a new dataset.",Virtual,"He, Haoyu  and
Shi, Xingjian  and
Mueller, Jonas  and
Zha, Sheng  and
Li, Mu  and
Karypis, George",Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing,10.18653/v1/2021.sustainlp-1.13,November,119--133,Association for Computational Linguistics,Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing,https://aclanthology.org/2021.sustainlp-1.13,2021,,,,,
23,inproceedings,huang-etal-2021-globally,"In this paper, we propose a globally normalized model for context-free grammar (CFG)-based semantic parsing. Instead of predicting a probability, our model predicts a real-valued score at each step and does not suffer from the label bias problem. Experiments show that our approach outperforms locally normalized models on small datasets, but it does not yield improvement on a large dataset.",Online,"Huang, Chenyang  and
Yang, Wei  and
Cao, Yanshuai  and
Za{\""\i}ane, Osmar  and
Mou, Lili",Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021),10.18653/v1/2021.spnlp-1.7,August,61--66,Association for Computational Linguistics,A Globally Normalized Neural Model for Semantic Parsing,https://aclanthology.org/2021.spnlp-1.7,2021,,,,,
24,inproceedings,di-giovanni-brambilla-2021-content,"On September 2020 a constitutional referendum was held in Italy. In this work we collect a dataset of 1.2M tweets related to this event, with particular interest to the textual content shared, and we design a hashtag-based semi-automatic approach to label them as Supporters or Against the referendum. We use the labelled dataset to train a classifier based on transformers, unsupervisedly pre-trained on Italian corpora. Our model generalizes well on tweets that cannot be labeled by the hashtag-based approach. We check that no length-, lexicon- and sentiment-biases are present to affect the performance of the classifier. Finally, we discuss the discrepancy between the magnitudes of tweets expressing a specific stance, obtained using both the hashtag-based approach and our trained classifier, and the real outcome of the referendum: the referendum was approved by 70{\%} of the voters, while the number of tweets against the referendum is four times greater than the number of tweets supporting it. We conclude that the Italian referendum was an example of event where the minority was very loud on social media, highly influencing the perception of the event. Analyzing only the activity on social media is dangerous and can lead to extremely wrong forecasts.",Online,"Di Giovanni, Marco  and
Brambilla, Marco",Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,10.18653/v1/2021.socialnlp-1.2,June,14--23,Association for Computational Linguistics,Content-based Stance Classification of Tweets about the 2020 {I}talian Constitutional Referendum,https://aclanthology.org/2021.socialnlp-1.2,2021,,,,,
25,inproceedings,gjurkovic-etal-2021-pandora,"Personality and demographics are important variables in social sciences and computational sociolinguistics. However, datasets with both personality and demographic labels are scarce. To address this, we present PANDORA, the first dataset of Reddit comments of 10k users partially labeled with three personality models and demographics (age, gender, and location), including 1.6k users labeled with the well-established Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables.",Online,"Gjurkovi{\'c}, Matej  and
Karan, Mladen  and
Vukojevi{\'c}, Iva  and
Bo{\v{s}}njak, Mihaela  and
Snajder, Jan",Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,10.18653/v1/2021.socialnlp-1.12,June,138--152,Association for Computational Linguistics,{PANDORA} Talks: Personality and Demographics on {R}eddit,https://aclanthology.org/2021.socialnlp-1.12,2021,,,,,
26,inproceedings,tian-etal-2021-identifying,"Discrepancies exist among different cultures or languages. A lack of mutual understanding among different colingual groups about the perspectives on specific values or events may lead to uninformed decisions or biased opinions. Thus, automatically understanding the group perspectives can provide essential back-ground for many natural language processing tasks. In this paper, we study colingual groups and use language corpora as a proxy to identify their distributional perspectives. We present a novel computational approach to learn shared understandings, and benchmark our method by building culturally-aware models for the English, Chinese, and Japanese languages. Ona held out set of diverse topics, including marriage, corruption, democracy, etc., our model achieves high correlation with human judgements regarding intra-group values and inter-group differences",Online,"Tian, Yufei  and
Chakrabarty, Tuhin  and
Morstatter, Fred  and
Peng, Nanyun",Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,10.18653/v1/2021.socialnlp-1.16,June,178--190,Association for Computational Linguistics,Identifying Distributional Perspectives from Colingual Groups,https://aclanthology.org/2021.socialnlp-1.16,2021,,,,,
27,inproceedings,sahai-sharma-2021-predicting,"Grammatical gender may be determined by semantics, orthography, phonology, or could even be arbitrary. Identifying patterns in the factors that govern noun genders can be useful for language learners, and for understanding innate linguistic sources of gender bias. Traditional manual rule-based approaches may be substituted by more accurate and scalable but harder-to-interpret computational approaches for predicting gender from typological information. In this work, we propose interpretable gender classification models for French, which obtain the best of both worlds. We present high accuracy neural approaches which are augmented by a novel global surrogate based approach for explaining predictions. We introduce {`}auxiliary attributes{'} to provide tunable explanation complexity.",Online,"Sahai, Saumya  and
Sharma, Dravyansh",Proceedings of the Third Workshop on Computational Typology and Multilingual NLP,10.18653/v1/2021.sigtyp-1.9,June,90--96,Association for Computational Linguistics,Predicting and Explaining {F}rench Grammatical Gender,https://aclanthology.org/2021.sigtyp-1.9,2021,,,,,
28,inproceedings,qian-etal-2021-annotation,"MultiWOZ (Budzianowski et al., 2018) is one of the most popular multi-domain taskoriented dialog datasets, containing 10K+ annotated dialogs covering eight domains. It has been widely accepted as a benchmark for various dialog tasks, e.g., dialog state tracking (DST), natural language generation (NLG) and end-to-end (E2E) dialog modeling. In this work, we identify an overlooked issue with dialog state annotation inconsistencies in the dataset, where a slot type is tagged inconsistently across similar dialogs leading to confusion for DST modeling. We propose an automated correction for this issue, which is present in 70{\%} of the dialogs. Additionally, we notice that there is significant entity bias in the dataset (e.g., {``}cambridge{''} appears in 50{\%} of the destination cities in the train domain). The entity bias can potentially lead to named entity memorization in generative models, which may go unnoticed as the test set suffers from a similar entity bias as well. We release a new test set with all entities replaced with unseen entities. Finally, we benchmark joint goal accuracy (JGA) of the state-of-theart DST baselines on these modified versions of the data. Our experiments show that the annotation inconsistency corrections lead to 7-10{\%} improvement in JGA. On the other hand, we observe a 29{\%} drop in JGA when models are evaluated on the new test set with unseen entities.",Singapore and Online,"Qian, Kun  and
Beirami, Ahmad  and
Lin, Zhouhan  and
De, Ankita  and
Geramifard, Alborz  and
Yu, Zhou  and
Sankar, Chinnadhurai",Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,326--337,Association for Computational Linguistics,Annotation Inconsistency and Entity Bias in {M}ulti{WOZ},https://aclanthology.org/2021.sigdial-1.35,2021,,,,,
29,inproceedings,mehri-eskenazi-2021-gensf,"In transfer learning, it is imperative to achieve strong alignment between a pre-trained model and a downstream task. Prior work has done this by proposing task-specific pre-training objectives, which sacrifices the inherent scalability of the transfer learning paradigm. We instead achieve strong alignment by simultaneously modifying both the pre-trained model and the formulation of the downstream task, which is more efficient and preserves the scalability of transfer learning. We present GenSF (Generative Slot Filling), which leverages a generative pre-trained open-domain dialog model for slot filling. GenSF (1) adapts the pre-trained model by incorporating inductive biases about the task and (2) adapts the downstream task by reformulating slot filling to better leverage the pre-trained model{'}s capabilities. GenSF achieves state-of-the-art results on two slot filling datasets with strong gains in few-shot and zero-shot settings. We achieve a 9 F1 score improvement in zero-shot slot filling. This highlights the value of strong alignment between the pre-trained model and the downstream task.",Singapore and Online,"Mehri, Shikib  and
Eskenazi, Maxine",Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,489--498,Association for Computational Linguistics,{G}en{SF}: Simultaneous Adaptation of Generative Pre-trained Models and Slot Filling,https://aclanthology.org/2021.sigdial-1.51,2021,,,,,
30,inproceedings,karan-etal-2021-mitigating,"This work revisits the task of detecting decision-related utterances in multi-party dialogue. We explore performance of a traditional approach and a deep learning-based approach based on transformer language models, with the latter providing modest improvements. We then analyze topic bias in the models using topic information obtained by manual annotation. Our finding is that when detecting some types of decisions in our data, models rely more on topic specific words that decisions are about rather than on words that more generally indicate decision making. We further explore this by removing topic information from the train data. We show that this resolves the bias issues to an extent and, surprisingly, sometimes even boosts performance.",Singapore and Online,"Karan, Mladen  and
Khare, Prashant  and
Healey, Patrick  and
Purver, Matthew",Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,542--547,Association for Computational Linguistics,Mitigating Topic Bias when Detecting Decisions in Dialogue,https://aclanthology.org/2021.sigdial-1.56,2021,,,,,
31,inproceedings,chen-etal-2021-integrated,"Due to the recent advances of natural language processing, several works have applied the pre-trained masked language model (MLM) of BERT to the post-correction of speech recognition. However, existing pre-trained models only consider the semantic correction while the phonetic features of words is neglected. The semantic-only post-correction will consequently decrease the performance since homophonic errors are fairly common in Chinese ASR. In this paper, we proposed a novel approach to collectively exploit the contextualized representation and the phonetic information between the error and its replacing candidates to alleviate the error rate of Chinese ASR. Our experiment results on real world speech recognition datasets showed that our proposed method has evidently lower CER than the baseline model, which utilized a pre-trained BERT MLM as the corrector.","Taoyuan, Taiwan","Chen, Yi-Chang  and
Cheng, Chun-Yen  and
Chen, Chien-An  and
Sung, Ming-Chieh  and
Yeh, Yi-Ren",Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),,October,95--102,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),Integrated Semantic and Phonetic Post-correction for {C}hinese Speech Recognition,https://aclanthology.org/2021.rocling-1.13,2021,,,,,
32,inproceedings,prokhorov-etal-2021-learning,"It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP. Additionally, NLP is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information.",Online,"Prokhorov, Victor  and
Li, Yingzhen  and
Shareghi, Ehsan  and
Collier, Nigel",Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),10.18653/v1/2021.repl4nlp-1.5,August,34--46,Association for Computational Linguistics,Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders,https://aclanthology.org/2021.repl4nlp-1.5,2021,,,,,
33,inproceedings,zhang-etal-2021-unsupervised-representation,"To highlight the challenges of achieving representation disentanglement for text domain in an unsupervised setting, in this paper we select a representative set of successfully applied models from the image domain. We evaluate these models on 6 disentanglement metrics, as well as on downstream classification tasks and homotopy. To facilitate the evaluation, we propose two synthetic datasets with known generative factors. Our experiments highlight the existing gap in the text domain and illustrate that certain elements such as representation sparsity (as an inductive bias), or representation coupling with the decoder could impact disentanglement. To the best of our knowledge, our work is the first attempt on the intersection of unsupervised representation disentanglement and text, and provides the experimental framework and datasets for examining future developments in this direction.",Online,"Zhang, Lan  and
Prokhorov, Victor  and
Shareghi, Ehsan",Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),10.18653/v1/2021.repl4nlp-1.14,August,128--140,Association for Computational Linguistics,Unsupervised Representation Disentanglement of Text: An Evaluation on Synthetic Datasets,https://aclanthology.org/2021.repl4nlp-1.14,2021,,,,,
34,inproceedings,basile-etal-2021-probabilistic,"Emotion Classification is the task of automatically associating a text with a human emotion. State-of-the-art models are usually learned using annotated corpora or rely on hand-crafted affective lexicons. We present an emotion classification model that does not require a large annotated corpus to be competitive. We experiment with pretrained language models in both a zero-shot and few-shot configuration. We build several of such models and consider them as biased, noisy annotators, whose individual performance is poor. We aggregate the predictions of these models using a Bayesian method originally developed for modelling crowdsourced annotations. Next, we show that the resulting system performs better than the strongest individual model. Finally, we show that when trained on few labelled data, our systems outperform fully-supervised models.",Held Online,"Basile, Angelo  and
P{\'e}rez-Torr{\'o}, Guillermo  and
Franco-Salvador, Marc",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,128--137,INCOMA Ltd.,Probabilistic Ensembles of Zero- and Few-Shot Learning Models for Emotion Classification,https://aclanthology.org/2021.ranlp-1.16,2021,,,,,
35,inproceedings,feng-etal-2021-improving,"Character-aware neural language models can capture the relationship between words by exploiting character-level information and are particularly effective for languages with rich morphology. However, these models are usually biased towards information from surface forms. To alleviate this problem, we propose a simple and effective method to improve a character-aware neural language model by forcing a character encoder to produce word-based embeddings under Skip-gram architecture in a warm-up step without extra training data. We empirically show that the resulting character-aware neural language model achieves obvious improvements of perplexity scores on typologically diverse languages, that contain many low-frequency or unseen words.",Held Online,"Feng, Yukun  and
Hu, Chenlong  and
Kamigaito, Hidetaka  and
Takamura, Hiroya  and
Okumura, Manabu",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,421--427,INCOMA Ltd.,Improving Character-Aware Neural Language Model by Warming up Character Encoder under Skip-gram Architecture,https://aclanthology.org/2021.ranlp-1.48,2021,,,,,
36,inproceedings,kahla-etal-2021-cross,"While abstractive summarization in certain languages, like English, has already reached fairly good results due to the availability of trend-setting resources, like the CNN/Daily Mail dataset, and considerable progress in generative neural models, progress in abstractive summarization for Arabic, the fifth most-spoken language globally, is still in baby shoes. While some resources for extractive summarization have been available for some time, in this paper, we present the first corpus of human-written abstractive news summaries in Arabic, hoping to lay the foundation of this line of research for this important language. The dataset consists of more than 21 thousand items. We used this dataset to train a set of neural abstractive summarization systems for Arabic by fine-tuning pre-trained language models such as multilingual BERT, AraBERT, and multilingual BART-50. As the Arabic dataset is much smaller than e.g. the CNN/Daily Mail dataset, we also applied cross-lingual knowledge transfer to significantly improve the performance of our baseline systems. The setups included two M-BERT-based summarization models originally trained for Hungarian/English and a similar system based on M-BART-50 originally trained for Russian that were further fine-tuned for Arabic. Evaluation of the models was performed in terms of ROUGE, and a manual evaluation of fluency and adequacy of the models was also performed.",Held Online,"Kahla, Mram  and
Yang, Zijian Gy{\H{o}}z{\H{o}}  and
Nov{\'a}k, Attila",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,655--663,INCOMA Ltd.,Cross-lingual Fine-tuning for Abstractive {A}rabic Text Summarization,https://aclanthology.org/2021.ranlp-1.74,2021,,,,,
37,inproceedings,kameswari-mamidi-2021-towards,"Media bias is a predominant phenomenon present in most forms of print and electronic media such as news articles, blogs, tweets, etc. Since media plays a pivotal role in shaping public opinion towards political happenings, both political parties and media houses often use such sources as outlets to propagate their own prejudices to the public. There has been some research on detecting political bias in news articles. However, none of it attempts to analyse the nature of bias or quantify the magnitude ofthe bias in a given text. This paper presents a political bias annotated corpus viz. PoBiCo-21, which is annotated using a schema specifically designed with 10 labels to capture various techniques used to create political bias in news. We create a ranking of these techniques based on their contribution to bias. After validating the ranking, we propose methods to use it to quantify the magnitude of bias in political news articles.",Held Online,"Kameswari, Lalitha  and
Mamidi, Radhika",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,671--678,INCOMA Ltd.,Towards Quantifying Magnitude of Political Bias in News Articles Using a Novel Annotation Schema,https://aclanthology.org/2021.ranlp-1.76,2021,,,,,
38,inproceedings,mekki-etal-2021-tremolo-tweets,"The casual, neutral, and formal language registers are highly perceptible in discourse productions. However, they are still poorly studied in Natural Language Processing (NLP), especially outside English, and for new textual types like tweets. To stimulate research, this paper introduces a large corpus of 228,505 French tweets (6M words) annotated in language registers. Labels are provided by a multi-label CamemBERT classifier trained and checked on a manually annotated subset of the corpus, while the tweets are selected to avoid undesired biases. Based on the corpus, an initial analysis of linguistic traits from either human annotators or automatic extractions is provided to describe the corpus and pave the way for various NLP tasks. The corpus, annotation guide and classifier are available on http://tremolo.irisa.fr.",Held Online,"Mekki, Jade  and
Lecorv{\'e}, Gw{\'e}nol{\'e}  and
Battistelli, Delphine  and
B{\'e}chet, Nicolas",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,950--958,INCOMA Ltd.,{TREM}o{L}o-Tweets: A Multi-Label Corpus of {F}rench Tweets for Language Register Characterization,https://aclanthology.org/2021.ranlp-1.108,2021,,,,,
39,inproceedings,sanchez-junquera-etal-2021-masking,"Hyperpartisan news show an extreme manipulation of reality based on an underlying and extreme ideological orientation. Because of its harmful effects at reinforcing one{'}s bias and the posterior behavior of people, hyperpartisan news detection has become an important task for computational linguists. In this paper, we evaluate two different approaches to detect hyperpartisan news. First, a text masking technique that allows us to compare style vs. topic-related features in a different perspective from previous work. Second, the transformer-based models BERT, XLM-RoBERTa, and M-BERT, known for their ability to capture semantic and syntactic patterns in the same representation. Our results corroborate previous research on this task in that topic-related features yield better results than style-based ones, although they also highlight the relevance of using higher-length n-grams. Furthermore, they show that transformer-based models are more effective than traditional methods, but this at the cost of greater computational complexity and lack of transparency. Based on our experiments, we conclude that the beginning of the news show relevant information for the transformers at distinguishing effectively between left-wing, mainstream, and right-wing orientations.",Held Online,"S{\'a}nchez-Junquera, Javier  and
Rosso, Paolo  and
Montes-y-G{\'o}mez, Manuel  and
Ponzetto, Simone Paolo",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,1244--1251,INCOMA Ltd.,Masking and Transformer-based Models for Hyperpartisanship Detection in News,https://aclanthology.org/2021.ranlp-1.140,2021,,,,,
40,inproceedings,virk-etal-2021-data,"FrameNet is a lexical semantic resource based on the linguistic theory of frame semantics. A number of framenet development strategies have been reported previously and all of them involve exploration of corpora and a fair amount of manual work. Despite previous efforts, there does not exist a well-thought-out automatic/semi-automatic methodology for frame construction. In this paper we propose a data-driven methodology for identification and semi-automatic construction of frames. As a proof of concept, we report on our initial attempts to build a wider-scale framenet for the legal domain (LawFN) using the proposed methodology. The constructed frames are stored in a lexical database and together with the annotated example sentences they have been made available through a web interface.",Held Online,"Virk, Shafqat Mumtaz  and
Dann{\'e}lls, Dana  and
Borin, Lars  and
Forsberg, Markus",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,1471--1479,INCOMA Ltd.,A Data-Driven Semi-Automatic Framenet Development Methodology,https://aclanthology.org/2021.ranlp-1.165,2021,,,,,
41,inproceedings,virk-etal-2021-deep,"Linguistic typology is an area of linguistics concerned with analysis of and comparison between natural languages of the world based on their certain linguistic features. For that purpose, historically, the area has relied on manual extraction of linguistic feature values from textural descriptions of languages. This makes it a laborious and time expensive task and is also bound by human brain capacity. In this study, we present a deep learning system for the task of automatic extraction of linguistic features from textual descriptions of natural languages. First, textual descriptions are manually annotated with special structures called semantic frames. Those annotations are learned by a recurrent neural network, which is then used to annotate un-annotated text. Finally, the annotations are converted to linguistic feature values using a separate rule based module. Word embeddings, learned from general purpose text, are used as a major source of knowledge by the recurrent neural network. We compare the proposed deep learning system to a previously reported machine learning based system for the same task, and the deep learning system wins in terms of F1 scores with a fair margin. Such a system is expected to be a useful contribution for the automatic curation of typological databases, which otherwise are manually developed.",Held Online,"Virk, Shafqat Mumtaz  and
Foster, Daniel  and
Sheikh Muhammad, Azam  and
Saleem, Raheela",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),,September,1480--1489,INCOMA Ltd.,A Deep Learning System for Automatic Extraction of Typological Linguistic Information from Descriptive Grammars,https://aclanthology.org/2021.ranlp-1.166,2021,,,,,
42,inproceedings,lucy-bamman-2021-gender,"Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3{'}s perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.",Virtual,"Lucy, Li  and
Bamman, David",Proceedings of the Third Workshop on Narrative Understanding,10.18653/v1/2021.nuse-1.5,June,48--55,Association for Computational Linguistics,Gender and Representation Bias in {GPT}-3 Generated Stories,https://aclanthology.org/2021.nuse-1.5,2021,,,,,
43,inproceedings,talman-etal-2021-nli,"Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models{'} meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models{'} reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.","Reykjavik, Iceland (Online)","Talman, Aarne  and
Apidianaki, Marianna  and
Chatzikyriakidis, Stergios  and
Tiedemann, J{\""o}rg",Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),,May 31--2 June,276--287,"Link{\""o}ping University Electronic Press, Sweden",{NLI} Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance,https://aclanthology.org/2021.nodalida-main.28,2021,,,,,
44,inproceedings,hansson-etal-2021-swedish,"We introduce the SweWinogender test set, a diagnostic dataset to measure gender bias in coreference resolution. It is modelled after the English Winogender benchmark, and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material. The paper discusses the design and creation of the dataset, and presents a small investigation of the supplementary statistics.","Reykjavik, Iceland (Online)","Hansson, Saga  and
Mavromatakis, Konstantinos  and
Adesam, Yvonne  and
Bouma, Gerlof  and
Dann{\'e}lls, Dana",Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),,May 31--2 June,452--459,"Link{\""o}ping University Electronic Press, Sweden",The {S}wedish {W}inogender Dataset,https://aclanthology.org/2021.nodalida-main.52,2021,,,,,
45,inproceedings,caselli-etal-2021-guiding,"We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1{--}3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4{--}6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection {\&} annotation, and the deployment {\&} evaluation. Finally, principles 7{--}9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.",Online,"Caselli, Tommaso  and
Cibin, Roberto  and
Conforti, Costanza  and
Encinas, Enrique  and
Teli, Maurizio",Proceedings of the 1st Workshop on NLP for Positive Impact,10.18653/v1/2021.nlp4posimpact-1.4,August,27--35,Association for Computational Linguistics,Guiding Principles for Participatory Design-inspired Natural Language Processing,https://aclanthology.org/2021.nlp4posimpact-1.4,2021,,,,,
46,inproceedings,dixon-birks-2021-improving,"This article explores the potential for Natural Language Processing (NLP) to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing (POP) is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events - tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge may either allow or perpetuate bias within the data potentially leading to unfavourable outcomes.",Online,"Dixon, Anthony  and
Birks, Daniel",Proceedings of the 1st Workshop on NLP for Positive Impact,10.18653/v1/2021.nlp4posimpact-1.13,August,115--124,Association for Computational Linguistics,Improving Policing with Natural Language Processing,https://aclanthology.org/2021.nlp4posimpact-1.13,2021,,,,,
47,inproceedings,hamalainen-etal-2021-never,"This study presents a new dataset on rumor detection in Finnish language news headlines. We have evaluated two different LSTM based models and two different BERT models, and have found very significant differences in the results. A fine-tuned FinBERT reaches the best overall accuracy of 94.3{\%} and rumor label accuracy of 96.0{\%} of the time. However, a model fine-tuned on Multilingual BERT reaches the best factual label accuracy of 97.2{\%}. Our results suggest that the performance difference is due to a difference in the original training data. Furthermore, we find that a regular LSTM model works better than one trained with a pretrained word2vec model. These findings suggest that more work needs to be done for pretrained models in Finnish language as they have been trained on small and biased corpora.",Online,"H{\""a}m{\""a}l{\""a}inen, Mika  and
Alnajjar, Khalid  and
Partanen, Niko  and
Rueter, Jack","Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",10.18653/v1/2021.nlp4if-1.6,June,39--44,Association for Computational Linguistics,Never guess what {I} heard... Rumor Detection in {F}innish News: a Dataset and a Baseline,https://aclanthology.org/2021.nlp4if-1.6,2021,,,,,
48,inproceedings,li-goldwasser-2021-mean,"The way information is generated and disseminated has changed dramatically over the last decade. Identifying the political perspective shaping the way events are discussed in the media becomes more important due to the sharp increase in the number of news outlets and articles. Previous approaches usually only leverage linguistic information. However, news articles attempt to maintain credibility and seem impartial. Therefore, bias is introduced in subtle ways, usually by emphasizing different aspects of the story. In this paper, we propose a novel framework that considers entities mentioned in news articles and external knowledge about them, capturing the bias with respect to those entities. We explore different ways to inject entity information into the text model. Experiments show that our proposed framework achieves significant improvements over the standard text models, and is capable of identifying the difference in news narratives with different perspectives.",Online,"Li, Chang  and
Goldwasser, Dan","Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",10.18653/v1/2021.nlp4if-1.10,June,66--75,Association for Computational Linguistics,{MEAN}: Multi-head Entity Aware Attention Networkfor Political Perspective Detection in News Media,https://aclanthology.org/2021.nlp4if-1.10,2021,,,,,
49,inproceedings,drawzeski-etal-2021-corpus,"We present the first annotated corpus for multilingual analysis of potentially unfair clauses in online Terms of Service. The data set comprises a total of 100 contracts, obtained from 25 documents annotated in four different languages: English, German, Italian, and Polish. For each contract, potentially unfair clauses for the consumer are annotated, for nine different unfairness categories. We show how a simple yet efficient annotation projection technique based on sentence embeddings could be used to automatically transfer annotations across languages.","Punta Cana, Dominican Republic","Drawzeski, Kasper  and
Galassi, Andrea  and
Jablonowska, Agnieszka  and
Lagioia, Francesca  and
Lippi, Marco  and
Micklitz, Hans Wolfgang  and
Sartor, Giovanni  and
Tagiuri, Giacomo  and
Torroni, Paolo",Proceedings of the Natural Legal Language Processing Workshop 2021,10.18653/v1/2021.nllp-1.1,November,1--8,Association for Computational Linguistics,A Corpus for Multilingual Analysis of Online Terms of Service,https://aclanthology.org/2021.nllp-1.1,2021,,,,,
50,inproceedings,mistica-etal-2021-semi,"Free legal assistance is critically under-resourced, and many of those who seek legal help have their needs unmet. A major bottleneck in the provision of free legal assistance to those most in need is the determination of the precise nature of the legal problem. This paper describes a collaboration with a major provider of free legal assistance, and the deployment of natural language processing models to assign area-of-law categories to real-world requests for legal assistance. In particular, we focus on an investigation of models to generate efficiencies in the triage process, but also the risks associated with naive use of model predictions, including fairness across different user demographics.","Punta Cana, Dominican Republic","Mistica, Meladel  and
Lau, Jey Han  and
Merrifield, Brayden  and
Fazio, Kate  and
Baldwin, Timothy",Proceedings of the Natural Legal Language Processing Workshop 2021,10.18653/v1/2021.nllp-1.23,November,217--227,Association for Computational Linguistics,Semi-automatic Triage of Requests for Free Legal Assistance,https://aclanthology.org/2021.nllp-1.23,2021,,,,,
51,inproceedings,jorgensen-sogaard-2021-evaluation,"Summarization systems are ultimately evaluated by human annotators and raters. Usually, annotators and raters do not reflect the demographics of end users, but are recruited through student populations or crowdsourcing platforms with skewed demographics. For two different evaluation scenarios {--} evaluation against gold summaries and system output ratings {--} we show that summary evaluation is sensitive to protected attributes. This can severely bias system development and evaluation, leading us to build models that cater for some groups rather than others.",Online and in Dominican Republic,"J{\o}rgensen, Anna  and
S{\o}gaard, Anders",Proceedings of the Third Workshop on New Frontiers in Summarization,10.18653/v1/2021.newsum-1.6,November,51--56,Association for Computational Linguistics,"Evaluation of Summarization Systems across Gender, Age, and Race",https://aclanthology.org/2021.newsum-1.6,2021,,,,,
52,inproceedings,grove-etal-2021-compositional,"Formal semantics in the Montagovian tradition provides precise meaning characterisations, but usually without a formal theory of the pragmatics of contextual parameters and their sensitivity to background knowledge. Meanwhile, formal pragmatic theories make explicit predictions about meaning in context, but generally without a well-defined compositional semantics. We propose a combined framework for the semantic and pragmatic interpretation of sentences in the face of probabilistic knowledge. We do so by (1) extending a Montagovian interpretation scheme to generate a distribution over possible meanings, and (2) generating a posterior for this distribution using a variant of the Rational Speech Act (RSA) models, but generalised to arbitrary propositions. These aspects of our framework are tied together by evaluating entailment under probabilistic uncertainty. We apply our model to anaphora resolution and show that it provides expected biases under suitable assumptions about the distributions of lexical and world-knowledge. Further, we observe that the model{'}s output is robust to variations in its parameters within reasonable ranges.","Groningen, the Netherlands (online)","Grove, Julian  and
Bernardy, Jean-Philippe  and
Chatzikyriakidis, Stergios",Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA),,June,60--70,Association for Computational Linguistics,From compositional semantics to {B}ayesian pragmatics via logical inference,https://aclanthology.org/2021.naloma-1.8,2021,,,,,
53,inproceedings,christopoulou-etal-2021-distantly,"We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into theVAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results.",Online,"Christopoulou, Fenia  and
Miwa, Makoto  and
Ananiadou, Sophia",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.2,June,11--26,Association for Computational Linguistics,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,https://aclanthology.org/2021.naacl-main.2,2021,,,,,
54,inproceedings,sheng-etal-2021-nice,"Ad hominem attacks are those that target some feature of a person{'}s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person{'}s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities ({\#}BlackLivesMatter, {\#}MeToo) versus other topics ({\#}Vegan, {\#}WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.",Online,"Sheng, Emily  and
Chang, Kai-Wei  and
Natarajan, Prem  and
Peng, Nanyun",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.60,June,750--767,Association for Computational Linguistics,"{``}Nice Try, Kiddo{''}: Investigating Ad Hominems in Dialogue Responses",https://aclanthology.org/2021.naacl-main.60,2021,,,,,
55,inproceedings,ross-etal-2021-measuring,"We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.",Online,"Ross, Candace  and
Katz, Boris  and
Barbu, Andrei",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.78,June,998--1008,Association for Computational Linguistics,Measuring Social Biases in Grounded Vision and Language Embeddings,https://aclanthology.org/2021.naacl-main.78,2021,,,,,
56,inproceedings,zhang-stratos-2021-understanding,"The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives{---}highest-scoring incorrect examples under the model{---}are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking.",Online,"Zhang, Wenzheng  and
Stratos, Karl",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.86,June,1090--1101,Association for Computational Linguistics,Understanding Hard Negatives in Noise Contrastive Estimation,https://aclanthology.org/2021.naacl-main.86,2021,,,,,
57,inproceedings,gowda-etal-2021-macro,"While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods{'} outputs.",Online,"Gowda, Thamme  and
You, Weiqiu  and
Lignos, Constantine  and
May, Jonathan",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.90,June,1138--1157,Association for Computational Linguistics,Macro-Average: Rare Types Are Important Too,https://aclanthology.org/2021.naacl-main.90,2021,,,,,
58,inproceedings,longpre-etal-2021-transferability,"Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and {``}dataset bias{''} (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples.",Online,"Longpre, Shayne  and
Lu, Yi  and
DuBois, Chris",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.101,June,1288--1300,Association for Computational Linguistics,On the Transferability of Minimal Prediction Preserving Inputs in Question Answering,https://aclanthology.org/2021.naacl-main.101,2021,,,,,
59,inproceedings,ferracane-etal-2021-answer,"Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective{\_}discourse.",Online,"Ferracane, Elisa  and
Durrett, Greg  and
Li, Junyi Jessy  and
Erk, Katrin",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.129,June,1626--1644,Association for Computational Linguistics,Did they answer? Subjective acts and intents in conversational discourse,https://aclanthology.org/2021.naacl-main.129,2021,,,,,
60,inproceedings,shen-etal-2021-explicitly,"Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.",Online,"Shen, Yikang  and
Tan, Shawn  and
Sordoni, Alessandro  and
Reddy, Siva  and
Courville, Aaron",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.132,June,1660--1672,Association for Computational Linguistics,Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle,https://aclanthology.org/2021.naacl-main.132,2021,,,,,
61,inproceedings,valvoda-etal-2021-precedent,"In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury{'}s, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart{'}s, who believes that what matters most is the precedent{'}s facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges{'} arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent{'}s arguments share 0.38 nats of information with the case{'}s outcome, whereas precedent{'}s facts only share 0.18 nats of information (i.e.,58{\%} less); suggesting Halsbury{'}s view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart{'}s view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.",Online,"Valvoda, Josef  and
Pimentel, Tiago  and
Stoehr, Niklas  and
Cotterell, Ryan  and
Teufel, Simone",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.181,June,2275--2288,Association for Computational Linguistics,What About the Precedent: An Information-Theoretic Analysis of Common Law,https://aclanthology.org/2021.naacl-main.181,2021,,,,,
62,inproceedings,silva-etal-2021-towards,"The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",Online,"Silva, Andrew  and
Tambwekar, Pradyumna  and
Gombolay, Matthew",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.189,June,2383--2389,Association for Computational Linguistics,Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers,https://aclanthology.org/2021.naacl-main.189,2021,,,,,
63,inproceedings,nozza-etal-2021-honest,"Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3{\%} of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9{\%} of the time, and in 4{\%} to homosexuality when the target is male. The results raise questions about the use of these models in production settings.",Online,"Nozza, Debora  and
Bianchi, Federico  and
Hovy, Dirk",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.191,June,2398--2406,Association for Computational Linguistics,{HONEST}: Measuring Hurtful Sentence Completion in Language Models,https://aclanthology.org/2021.naacl-main.191,2021,,,,,
64,inproceedings,sadhu-etal-2021-video,"Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models{'} application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.",Online,"Sadhu, Arka  and
Chen, Kan  and
Nevatia, Ram",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.196,June,2460--2478,Association for Computational Linguistics,Video Question Answering with Phrases via Semantic Roles,https://aclanthology.org/2021.naacl-main.196,2021,,,,,
65,inproceedings,le-scao-rush-2021-many,"When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.",Online,"Le Scao, Teven  and
Rush, Alexander",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.208,June,2627--2636,Association for Computational Linguistics,How many data points is a prompt worth?,https://aclanthology.org/2021.naacl-main.208,2021,,,,,
66,inproceedings,peters-martins-2021-smoothing,"Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax{---}the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs.",Online,"Peters, Ben  and
Martins, Andr{\'e} F. T.",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.210,June,2642--2654,Association for Computational Linguistics,Smoothing and Shrinking the Sparse {S}eq2{S}eq Search Space,https://aclanthology.org/2021.naacl-main.210,2021,,,,,
67,inproceedings,wang-etal-2021-dynamically,"Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance.",Online,"Wang, Liwen  and
Yan, Yuanmeng  and
He, Keqing  and
Wu, Yanan  and
Xu, Weiran",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.293,June,3740--3750,Association for Computational Linguistics,Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack,https://aclanthology.org/2021.naacl-main.293,2021,,,,,
68,inproceedings,sawhney-etal-2021-empirical,"Volatility prediction is complex due to the stock market{'}s stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives{'} speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender-sensitive audio features and fewer female executives in earnings calls of one of the world{'}s biggest stock indexes, the S{\&}P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes.",Online,"Sawhney, Ramit  and
Aggarwal, Arshiya  and
Shah, Rajiv Ratn",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.294,June,3751--3757,Association for Computational Linguistics,An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls,https://aclanthology.org/2021.naacl-main.294,2021,,,,,
69,inproceedings,shmueli-etal-2021-beyond,"The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.",Online,"Shmueli, Boaz  and
Fell, Jan  and
Ray, Soumya  and
Ku, Lun-Wei",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.295,June,3758--3769,Association for Computational Linguistics,Beyond Fair Pay: Ethical Implications of {NLP} Crowdsourcing,https://aclanthology.org/2021.naacl-main.295,2021,,,,,
70,inproceedings,amir-etal-2021-impact,"Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III {---}{---} the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes.",Online,"Amir, Silvio  and
van de Meent, Jan-Willem  and
Wallace, Byron",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.299,June,3808--3823,Association for Computational Linguistics,On the Impact of Random Seeds on the Fairness of Clinical Classifiers,https://aclanthology.org/2021.naacl-main.299,2021,,,,,
71,inproceedings,kaushal-etal-2021-twt,"The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT{--}WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.",Online,"Kaushal, Ayush  and
Saha, Avirup  and
Ganguly, Niloy",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.303,June,3879--3889,Association for Computational Linguistics,t{WT}{--}{WT}: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets,https://aclanthology.org/2021.naacl-main.303,2021,,,,,
72,inproceedings,zhang-etal-2021-double,"Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a {``}double perturbation{''} framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models{'} robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0{\%}-99.8{\%}) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.",Online,"Zhang, Chong  and
Zhao, Jieyu  and
Zhang, Huan  and
Chang, Kai-Wei  and
Hsieh, Cho-Jui",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.305,June,3899--3916,Association for Computational Linguistics,Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation,https://aclanthology.org/2021.naacl-main.305,2021,,,,,
73,inproceedings,zhang-etal-2021-multi,"Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior {--} human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy.",Online,"Zhang, Long  and
Zhang, Tong  and
Zhang, Haibo  and
Yang, Baosong  and
Ye, Wei  and
Zhang, Shikun",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.309,June,3953--3963,Association for Computational Linguistics,Multi-Hop Transformer for Document-Level Machine Translation,https://aclanthology.org/2021.naacl-main.309,2021,,,,,
74,inproceedings,cao-etal-2021-continual,"Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings.",Online,"Cao, Yue  and
Wei, Hao-Ran  and
Chen, Boxing  and
Wan, Xiaojun",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.310,June,3964--3974,Association for Computational Linguistics,Continual Learning for Neural Machine Translation,https://aclanthology.org/2021.naacl-main.310,2021,,,,,
75,inproceedings,pryzant-etal-2021-causal,"We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer{'}s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest{---}e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.",Online,"Pryzant, Reid  and
Card, Dallas  and
Jurafsky, Dan  and
Veitch, Victor  and
Sridhar, Dhanya",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.323,June,4095--4109,Association for Computational Linguistics,Causal Effects of Linguistic Properties,https://aclanthology.org/2021.naacl-main.323,2021,,,,,
76,inproceedings,rios-etal-2021-biasing,"Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior.",Online,"Rios, Annette  and
Amrhein, Chantal  and
Aepli, No{\""e}mi  and
Sennrich, Rico",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.354,June,4474--4488,Association for Computational Linguistics,On Biasing Transformer Attention Towards Monotonicity,https://aclanthology.org/2021.naacl-main.354,2021,,,,,
77,inproceedings,huang-etal-2021-seq2emo,"Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval{'}18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting.",Online,"Huang, Chenyang  and
Trabelsi, Amine  and
Qin, Xuebin  and
Farruque, Nawshad  and
Mou, Lili  and
Za{\""\i}ane, Osmar",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.375,June,4717--4724,Association for Computational Linguistics,{S}eq2{E}mo: A Sequence to Multi-Label Emotion Classification Model,https://aclanthology.org/2021.naacl-main.375,2021,,,,,
78,inproceedings,jiang-etal-2021-enriching,"Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs.(Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)",Online,"Jiang, Yichen  and
Celikyilmaz, Asli  and
Smolensky, Paul  and
Soulos, Paul  and
Rao, Sudha  and
Palangi, Hamid  and
Fernandez, Roland  and
Smith, Caitlin  and
Bansal, Mohit  and
Gao, Jianfeng",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.381,June,4780--4793,Association for Computational Linguistics,Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization,https://aclanthology.org/2021.naacl-main.381,2021,,,,,
79,inproceedings,bowman-dahl-2021-will,"Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",Online,"Bowman, Samuel R.  and
Dahl, George",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.385,June,4843--4855,Association for Computational Linguistics,What Will it Take to Fix Benchmarking in Natural Language Understanding?,https://aclanthology.org/2021.naacl-main.385,2021,,,,,
80,inproceedings,zhang-hashimoto-2021-inductive,"We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).",Online,"Zhang, Tianyi  and
Hashimoto, Tatsunori B.",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.404,June,5131--5146,Association for Computational Linguistics,On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies,https://aclanthology.org/2021.naacl-main.404,2021,,,,,
81,inproceedings,vu-moschitti-2021-ava,"We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7{\%} F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7{\%} at 95{\%} of confidence when measured on several QA systems.",Online,"Vu, Thuy  and
Moschitti, Alessandro",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.412,June,5223--5233,Association for Computational Linguistics,{AVA}: an Automatic e{V}aluation Approach for Question Answering Systems,https://aclanthology.org/2021.naacl-main.412,2021,,,,,
82,inproceedings,lee-etal-2021-unifying,"In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2 learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2{'}s learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and the model{'}s generalizability to unseen events.",Online,"Lee, Nayeon  and
Li, Belinda Z.  and
Wang, Sinong  and
Fung, Pascale  and
Ma, Hao  and
Yih, Wen-tau  and
Khabsa, Madian",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.432,June,5479--5485,Association for Computational Linguistics,On Unifying Misinformation Detection,https://aclanthology.org/2021.naacl-main.432,2021,,,,,
83,inproceedings,zhou-etal-2021-amr,"Abstract Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a target-side action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best Smatch score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding.",Online,"Zhou, Jiawei  and
Naseem, Tahira  and
Fernandez Astudillo, Ram{\'o}n  and
Florian, Radu",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.443,June,5585--5598,Association for Computational Linguistics,{AMR} Parsing with Action-Pointer Transformer,https://aclanthology.org/2021.naacl-main.443,2021,,,,,
84,inproceedings,zhu-etal-2021-mediasum,"This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model{'}s performance on other dialogue summarization tasks.",Online,"Zhu, Chenguang  and
Liu, Yang  and
Mei, Jie  and
Zeng, Michael",Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,10.18653/v1/2021.naacl-main.474,June,5927--5934,Association for Computational Linguistics,{M}edia{S}um: A Large-scale Media Interview Dataset for Dialogue Summarization,https://aclanthology.org/2021.naacl-main.474,2021,,,,,
85,inproceedings,soulos-etal-2021-structural,"Machine translation has seen rapid progress with the advent of Transformer-based models. These models have no explicit linguistic structure built into them, yet they may still implicitly learn structured relationships by attending to relevant tokens. We hypothesize that this structural learning could be made more robust by explicitly endowing Transformers with a structural bias, and we investigate two methods for building in such a bias. One method, the TP-Transformer, augments the traditional Transformer architecture to include an additional component to represent structure. The second method imbues structure at the data level by segmenting the data with morphological tokenization. We test these methods on translating from English into morphologically rich languages, Turkish and Inuktitut, and consider both automatic metrics and human evaluations. We find that each of these two approaches allows the network to achieve better performance, but this improvement is dependent on the size of the dataset. In sum, structural encoding methods make Transformers more sample-efficient, enabling them to perform better from smaller amounts of data.",Virtual,"Soulos, Paul  and
Rao, Sudha  and
Smith, Caitlin  and
Rosen, Eric  and
Celikyilmaz, Asli  and
McCoy, R. Thomas  and
Jiang, Yichen  and
Haley, Coleman  and
Fernandez, Roland  and
Palangi, Hamid  and
Gao, Jianfeng  and
Smolensky, Paul",Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021),,August,52--67,Association for Machine Translation in the Americas,Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages,https://aclanthology.org/2021.mtsummit-loresmt.6,2021,,,,,
86,inproceedings,de-meulder-2021-good,"This paper identifies some common and specific pitfalls in the development of sign language technologies targeted at deaf communities, with a specific focus on signing avatars. It makes the call to urgently interrogate some of the ideologies behind those technologies, including issues of ethical and responsible development. The paper addresses four separate and interlinked issues: ideologies about deaf people and mediated communication, bias in data sets and learning, user feedback, and applications of the technologies. The paper ends with several take away points for both technology developers and deaf NGOs. Technology developers should give more consideration to diversifying their team and working interdisciplinary, and be mindful of the biases that inevitably creep into data sets. There should also be a consideration of the technologies{'} end users. Sign language interpreters are not the end users nor should they be seen as the benchmark for language use. Technology developers and deaf NGOs can engage in a dialogue about how to prioritize application domains and prioritize within application domains. Finally, deaf NGOs policy statements will need to take a longer view, and use avatars to think of a significantly better system compared to what sign language interpreting services can provide.",Virtual,"De Meulder, Maartje",Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),,August,12--22,Association for Machine Translation in the Americas,Is {``}good enough{''} good enough? Ethical and responsible development of sign language technologies,https://aclanthology.org/2021.mtsummit-at4ssl.2,2021,,,,,
87,inproceedings,kuznetsova-etal-2021-using,"This paper presents a study that compares non-manual markers of polar and wh-questions to statements in Kazakh-Russian Sign Language (KRSL) in a dataset collected for NLP tasks. The primary focus of the study is to demonstrate the utility of computer vision solutions for the linguistic analysis of non-manuals in sign languages, although additional corrections are required to account for biases in the output. To this end, we analyzed recordings of 10 triplets of sentences produced by 9 native signers using both manual annotation and computer vision solutions (such as OpenFace). We utilize and improve the computer vision solution, and briefly describe the results of the linguistic analysis.",Virtual,"Kuznetsova, Anna  and
Imashev, Alfarabi  and
Mukushev, Medet  and
Sandygulova, Anara  and
Kimmelman, Vadim",Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),,August,49--59,Association for Machine Translation in the Americas,Using Computer Vision to Analyze Non-manual Marking of Questions in {KRSL},https://aclanthology.org/2021.mtsummit-at4ssl.6,2021,,,,,
88,inproceedings,shinoda-etal-2021-question,"Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question{--}context lexical overlap. This hinders QA models from generalizing to under-represented samples such as questions with low lexical overlap. Question generation (QG), a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low lexical overlap, while improving that on questions with high lexical overlap. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples.","Punta Cana, Dominican Republic","Shinoda, Kazutoshi  and
Sugawara, Saku  and
Aizawa, Akiko",Proceedings of the 3rd Workshop on Machine Reading for Question Answering,10.18653/v1/2021.mrqa-1.6,November,63--72,Association for Computational Linguistics,Can Question Generation Debias Question Answering Models? A Case Study on Question{--}Context Lexical Overlap,https://aclanthology.org/2021.mrqa-1.6,2021,,,,,
89,inproceedings,mao-etal-2021-eliciting,"Question answering (QA) models use retriever and reader systems to answer questions. Reliance on training data by QA systems can amplify or reflect inequity through their responses. Many QA models, such as those for the SQuAD dataset, are trained and tested on a subset of Wikipedia articles which encode their own biases and also reproduce real-world inequality. Understanding how training data affects bias in QA systems can inform methods to mitigate inequity. We develop two sets of questions for closed and open domain questions respectively, which use ambiguous questions to probe QA models for bias. We feed three deep-learning-based QA systems with our question sets and evaluate responses for bias via the metrics. Using our metrics, we find that open-domain QA models amplify biases more than their closed-domain counterparts and propose that biases in the retriever surface more readily due to greater freedom of choice.","Punta Cana, Dominican Republic","Mao, Andrew  and
Raman, Naveen  and
Shu, Matthew  and
Li, Eric  and
Yang, Franklin  and
Boyd-Graber, Jordan",Proceedings of the 3rd Workshop on Machine Reading for Question Answering,10.18653/v1/2021.mrqa-1.9,November,92--99,Association for Computational Linguistics,Eliciting Bias in Question Answering Models through Ambiguity,https://aclanthology.org/2021.mrqa-1.9,2021,,,,,
90,inproceedings,carlsson-etal-2021-gandalf,"This paper introduces a long-range multiple-choice Question Answering (QA) dataset, based on full-length fiction book texts. The questions are formulated as 10-way multiple-choice questions, where the task is to select the correct character name given a character description, or vice-versa. Each character description is formulated in natural text and often contains information from several sections throughout the book. We provide 20,000 questions created from 10,000 manually annotated descriptions of characters from 177 books containing 152,917 words on average. We address the current discourse regarding dataset bias and leakage by a simple anonymization procedure, which in turn enables interesting probing possibilities. Finally, we show that suitable baseline algorithms perform very poorly on this task, with the book size itself making it non-trivial to attempt a Transformer-based QA solution. This leaves ample room for future improvement, and hints at the need for a completely different type of solution.","Punta Cana, Dominican Republic","Carlsson, Fredrik  and
Sahlgren, Magnus  and
Olsson, Fredrik  and
Cuba Gyllensten, Amaru",Proceedings of the 3rd Workshop on Machine Reading for Question Answering,10.18653/v1/2021.mrqa-1.13,November,119--132,Association for Computational Linguistics,{GANDALF}: a General Character Name Description Dataset for Long Fiction,https://aclanthology.org/2021.mrqa-1.13,2021,,,,,
91,inproceedings,risch-etal-2021-semantic,"The evaluation of question answering models compares ground-truth annotations with model predictions. However, as of today, this comparison is mostly lexical-based and therefore misses out on answers that have no lexical overlap but are still semantically similar, thus treating correct answers as false. This underestimation of the true performance of models hinders user acceptance in applications and complicates a fair comparison of different models. Therefore, there is a need for an evaluation metric that is based on semantics instead of pure string similarity. In this short paper, we present SAS, a cross-encoder-based metric for the estimation of semantic answer similarity, and compare it to seven existing metrics. To this end, we create an English and a German three-way annotated evaluation dataset containing pairs of answers along with human judgment of their semantic similarity, which we release along with an implementation of the SAS metric and the experiments. We find that semantic similarity metrics based on recent transformer models correlate much better with human judgment than traditional lexical similarity metrics on our two newly created datasets and one dataset from related work.","Punta Cana, Dominican Republic","Risch, Julian  and
M{\""o}ller, Timo  and
Gutsch, Julian  and
Pietsch, Malte",Proceedings of the 3rd Workshop on Machine Reading for Question Answering,10.18653/v1/2021.mrqa-1.15,November,149--157,Association for Computational Linguistics,Semantic Answer Similarity for Evaluating Question Answering Models,https://aclanthology.org/2021.mrqa-1.15,2021,,,,,
92,inproceedings,cooper-stickland-murray-2021-regularising,"Many recent works use {`}consistency regularisation{'} to improve the generalisation of fine-tuned pre-trained models, both multilingual and English-only. These works encourage model outputs to be similar between a perturbed and normal version of the input, usually via penalising the Kullback{--}Leibler (KL) divergence between the probability distribution of the perturbed and normal model. We believe that consistency losses may be implicitly regularizing the loss landscape. In particular, we build on work hypothesising that implicitly or explicitly regularizing trace of the Fisher Information Matrix (FIM), amplifies the implicit bias of SGD to avoid memorization. Our initial results show both empirically and theoretically that consistency losses are related to the FIM, and show that the flat minima implied by a small trace of the FIM improves performance when fine-tuning a multilingual model on additional languages. We aim to confirm these initial results on more datasets, and use our insights to develop better multilingual fine-tuning techniques.","Punta Cana, Dominican Republic","Cooper Stickland, Asa  and
Murray, Iain",Proceedings of the 1st Workshop on Multilingual Representation Learning,10.18653/v1/2021.mrl-1.20,November,238--241,Association for Computational Linguistics,Regularising Fisher Information Improves Cross-lingual Generalisation,https://aclanthology.org/2021.mrl-1.20,2021,,,,,
93,inproceedings,parcalabescu-etal-2021-seeing,"We investigate the reasoning ability of pretrained vision and language (V{\&}L) models in two tasks that require multimodal integration: (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image. We evaluate three pretrained V{\&}L models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that models solve task (1) very well, as expected, since all models are pretrained on task (1). However, none of the pretrained V{\&}L models is able to adequately solve task (2), our counting probe, and they cannot generalise to out-of-distribution quantities. We propose a number of explanations for these findings: LXMERT (and to some extent ViLBERT 12-in-1) show some evidence of catastrophic forgetting on task (1). Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V{\&}L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.","Groningen, Netherlands (Online)","Parcalabescu, Letitia  and
Gatt, Albert  and
Frank, Anette  and
Calixto, Iacer",Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),,June,32--44,Association for Computational Linguistics,Seeing past words: Testing the cross-modal capabilities of pretrained {V}{\&}{L} models on counting tasks,https://aclanthology.org/2021.mmsr-1.4,2021,,,,,
94,inproceedings,onabola-etal-2021-hbert,"Subtle and overt racism is still present both in physical and online communities today and has impacted many lives in different segments of the society. In this short piece of work, we present how we{'}re tackling this societal issue with Natural Language Processing. We are releasing BiasCorp, a dataset containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually annotated) is ready for publication. We are currently in the final phase of manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has been used widely in several downstream tasks. In this work, we present hBERT, where we modify certain layers of the pretrained BERT model with the new Hopfield Layer. hBert generalizes well across different distributions with the added advantage of a reduced model complexity. We are also releasing a JavaScript library 3 and a Chrome Extension Application, to help developers make use of our trained model in web applications (say chat application) and for users to identify and report racially biased contents on the web respectively",Kyiv,"Onabola, Olawale  and
Ma, Zhuang  and
Yang, Xie  and
Akera, Benjamin  and
Abdulrahman, Ibraheem  and
Xue, Jia  and
Liu, Dianbo  and
Bengio, Yoshua","Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",,April,26--33,Association for Computational Linguistics,h{BERT} + {B}ias{C}orp - Fighting Racism on the Web,https://aclanthology.org/2021.ltedi-1.4,2021,,,,,
95,inproceedings,b-etal-2021-overview,"Data in general encodes human biases by default; being aware of this is a good start, and the research around how to handle it is ongoing. The term {`}bias{'} is extensively used in various contexts in NLP systems. In our research the focus is specific to biases such as gender, racism, religion, demographic and other intersectional views on biases that prevail in text processing systems responsible for systematically discriminating specific population, which is not ethical in NLP. These biases exacerbate the lack of equality, diversity and inclusion of specific population while utilizing the NLP applications. The tools and technology at the intermediate level utilize biased data, and transfer or amplify this bias to the downstream applications. However, it is not enough to be colourblind, gender-neutral alone when designing a unbiased technology {--} instead, we should take a conscious effort by designing a unified framework to measure and benchmark the bias. In this paper, we recommend six measures and one augment measure based on the observations of the bias in data, annotations, text representations and debiasing techniques.",Kyiv,"B, Senthil Kumar  and
Chandrabose, Aravindan  and
Chakravarthi, Bharathi Raja","Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",,April,34--45,Association for Computational Linguistics,An Overview of Fairness in Data {--} Illuminating the Bias in Data Pipeline,https://aclanthology.org/2021.ltedi-1.5,2021,,,,,
96,inproceedings,bleiweiss-2021-finding,"Automatic detection of critical plot information in reviews of media items poses unique challenges to both social computing and computational linguistics. In this paper we propose to cast the problem of discovering spoiler bias in online discourse as a text simplification task. We conjecture that for an item-user pair, the simpler the user review we learn from an item summary the higher its likelihood to present a spoiler. Our neural model incorporates the advanced transformer network to rank the severity of a spoiler in user tweets. We constructed a sustainable high-quality movie dataset scraped from unsolicited review tweets and paired with a title summary and meta-data extracted from a movie specific domain. To a large extent, our quantitative and qualitative results weigh in on the performance impact of named entity presence in plot summaries. Pretrained on a split-and-rephrase corpus with knowledge distilled from English Wikipedia and fine-tuned on our movie dataset, our neural model shows to outperform both a language modeler and monolingual translation baselines.",Kyiv,"Bleiweiss, Avi","Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion",,April,51--60,Association for Computational Linguistics,Finding Spoiler Bias in Tweets by Zero-shot Learning and Knowledge Distilling from Neural Text Simplification,https://aclanthology.org/2021.ltedi-1.7,2021,,,,,
97,inproceedings,prabhakaran-etal-2021-releasing,"A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single {``}ground truth{''} label or score, through majority voting, averaging, or adjudication. While these approaches may be appropriate in certain annotation tasks, such aggregations overlook the socially constructed nature of human perceptions that annotations for relatively more subjective tasks are meant to capture. In particular, systematic disagreements between annotators owing to their socio-cultural backgrounds and/or lived experiences are often obfuscated through such aggregations. In this paper, we empirically demonstrate that label aggregation may introduce representational biases of individual and group perspectives. Based on this finding, we propose a set of recommendations for increased utility and transparency of datasets for downstream use cases.","Punta Cana, Dominican Republic","Prabhakaran, Vinodkumar  and
Mostafazadeh Davani, Aida  and
Diaz, Mark",Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,10.18653/v1/2021.law-1.14,November,133--138,Association for Computational Linguistics,On Releasing Annotator-Level Labels and Information in Datasets,https://aclanthology.org/2021.law-1.14,2021,,,,,
98,inproceedings,schmidt-etal-2021-fairynet,"This paper presents a data set of German fairy tales, manually annotated with character networks which were obtained with high inter rater agreement. The release of this corpus provides an opportunity of training and comparing different algorithms for the extraction of character networks, which so far was barely possible due to heterogeneous interests of previous researchers. We demonstrate the usefulness of our data set by providing baseline experiments for the automatic extraction of character networks, applying a rule-based pipeline as well as a neural approach, and find the neural approach outperforming the rule-approach in most evaluation settings.","Punta Cana, Dominican Republic (online)","Schmidt, David  and
Zehe, Albin  and
Lorenzen, Janne  and
Sergel, Lisa  and
D{\""u}ker, Sebastian  and
Krug, Markus  and
Puppe, Frank","Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",10.18653/v1/2021.latechclfl-1.6,November,49--56,Association for Computational Linguistics,The {F}airy{N}et Corpus - Character Networks for {G}erman Fairy Tales,https://aclanthology.org/2021.latechclfl-1.6,2021,,,,,
99,inproceedings,wockener-etal-2021-end,"In this work, we design an end-to-end model for poetry generation based on conditioned recurrent neural network (RNN) language models whose goal is to learn stylistic features (poem length, sentiment, alliteration, and rhyming) from examples alone. We show this model successfully learns the {`}meaning{'} of length and sentiment, as we can control it to generate longer or shorter as well as more positive or more negative poems. However, the model does not grasp sound phenomena like alliteration and rhyming, but instead exploits low-level statistical cues. Possible reasons include the size of the training data, the relatively low frequency and difficulty of these sublexical phenomena as well as model biases. We show that more recent GPT-2 models also have problems learning sublexical phenomena such as rhyming from examples alone.","Punta Cana, Dominican Republic (online)","W{\""o}ckener, J{\""o}rg  and
Haider, Thomas  and
Miller, Tristan  and
Nguyen, The-Khang  and
Nguyen, Thanh Tung Linh  and
Pham, Minh Vu  and
Belouadi, Jonas  and
Eger, Steffen","Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",10.18653/v1/2021.latechclfl-1.7,November,57--66,Association for Computational Linguistics,End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?,https://aclanthology.org/2021.latechclfl-1.7,2021,,,,,
100,inproceedings,teichmann-venant-2021-generic,"When learned without exploration, local models for structured prediction tasks are subject to exposure bias and cannot be trained without detailed guidance. Active Imitation Learning (AIL), also known in NLP as Dynamic Oracle Learning, is a general technique for working around these issues by allowing the exploration of different outputs at training time. AIL requires oracle feedback: an oracle is any algorithm which can, given a partial candidate solution and gold annotation, find the correct (minimum loss) next output to produce. This paper describes a general finite state technique for deriving oracles. The technique describe is also efficient and will greatly expand the tasks for which AIL can be used.",Online,"Teichmann, Christoph  and
Venant, Antoine",Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),10.18653/v1/2021.iwpt-1.1,August,1--12,Association for Computational Linguistics,Generic Oracles for Structured Prediction,https://aclanthology.org/2021.iwpt-1.1,2021,,,,,
101,inproceedings,bogoychev-chen-2021-highs,"Machine translation systems are vulnerable to domain mismatch, especially in a low-resource scenario. Out-of-domain translations are often of poor quality and prone to hallucinations, due to exposure bias and the decoder acting as a language model. We adopt two approaches to alleviate this problem: lexical shortlisting restricted by IBM statistical alignments, and hypothesis reranking based on similarity. The methods are computationally cheap and show success on low-resource out-of-domain test sets. However, the methods lose advantage when there is sufficient data or too great domain mismatch. This is due to both the IBM model losing its advantage over the implicitly learned neural alignment, and issues with subword segmentation of unseen words.","Online and Punta Cana, Dominican Republic","Bogoychev, Nikolay  and
Chen, Pinzhen",Proceedings of the Second Workshop on Insights from Negative Results in NLP,10.18653/v1/2021.insights-1.12,November,74--80,Association for Computational Linguistics,The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation,https://aclanthology.org/2021.insights-1.12,2021,,,,,
102,inproceedings,bhargava-etal-2021-generalization,"Much of recent progress in NLU was shown to be due to models{'} learning dataset-specific heuristics. We conduct a case study of generalization in NLI (from MNLI to the adversarially constructed HANS dataset) in a range of BERT-based architectures (adapters, Siamese Transformers, HEX debiasing), as well as with subsampling the data and increasing the model size. We report 2 successful and 3 unsuccessful strategies, all providing insights into how Transformer-based models learn to generalize.","Online and Punta Cana, Dominican Republic","Bhargava, Prajjwal  and
Drozd, Aleksandr  and
Rogers, Anna",Proceedings of the Second Workshop on Insights from Negative Results in NLP,10.18653/v1/2021.insights-1.18,November,125--135,Association for Computational Linguistics,Generalization in {NLI}: Ways (Not) To Go Beyond Simple Heuristics,https://aclanthology.org/2021.insights-1.18,2021,,,,,
103,inproceedings,ciora-etal-2021-examining,"As Machine Translation (MT) has become increasingly more powerful, accessible, and widespread, the potential for the perpetuation of bias has grown alongside its advances. While overt indicators of bias have been studied in machine translation, we argue that covert biases expose a problem that is further entrenched. Through the use of the gender-neutral language Turkish and the gendered language English, we examine cases of both overt and covert gender bias in MT models. Specifically, we introduce a method to investigate asymmetrical gender markings. We also assess bias in the attribution of personhood and examine occupational and personality stereotypes through overt bias indicators in MT models. Our work explores a deeper layer of bias in MT models and demonstrates the continued need for language-specific, interdisciplinary methodology in MT model development.","Aberdeen, Scotland, UK","Ciora, Chloe  and
Iren, Nur  and
Alikhani, Malihe",Proceedings of the 14th International Conference on Natural Language Generation,,August,55--63,Association for Computational Linguistics,Examining Covert Gender Bias: A Case Study in {T}urkish and {E}nglish Machine Translation Models,https://aclanthology.org/2021.inlg-1.7,2021,,,,,
104,inproceedings,markl-lai-2021-context,"Commercial Automatic Speech Recognition (ASR) systems tend to show systemic predictive bias for marginalised speaker/user groups. We highlight the need for an interdisciplinary and context-sensitive approach to documenting this bias incorporating perspectives and methods from sociolinguistics, speech {\&} language technology and human-computer interaction in the context of a case study. We argue evaluation of ASR systems should be disaggregated by speaker group, include qualitative error analysis, and consider user experience in a broader sociolinguistic and social context.",Online,"Markl, Nina  and
Lai, Catherine",Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,,April,34--40,Association for Computational Linguistics,Context-sensitive evaluation of automatic speech recognition: considering user experience {\&} language variation,https://aclanthology.org/2021.hcinlp-1.6,2021,,,,,
105,inproceedings,bhat-etal-2021-people,"Recent studies have shown that a bias in thetext suggestions system can percolate in theuser{'}s writing. In this pilot study, we ask thequestion: How do people interact with text pre-diction models, in an inline next phrase sugges-tion interface and how does introducing senti-ment bias in the text prediction model affecttheir writing? We present a pilot study as afirst step to answer this question.",Online,"Bhat, Advait  and
Agashe, Saaket  and
Joshi, Anirudha",Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,,April,116--121,Association for Computational Linguistics,How do people interact with biased text prediction models while writing?,https://aclanthology.org/2021.hcinlp-1.18,2021,,,,,
106,inproceedings,nekvinda-dusek-2021-shades,"The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for benchmarkingcontext-to-response abilities of task-orienteddialogue systems. In this work, we identifyinconsistencies in data preprocessing and re-porting of three corpus-based metrics used onthis dataset, i.e., BLEU score and Inform {\&}Success rates. We point out a few problemsof the MultiWOZ benchmark such as unsat-isfactory preprocessing, insufficient or under-specified evaluation metrics, or rigid database.We re-evaluate 7 end-to-end and 6 policy opti-mization models in as-fair-as-possible setups,and we show that their reported scores cannotbe directly compared. To facilitate compari-son of future systems, we release our stand-alone standardized evaluation scripts. We alsogive basic recommendations for corpus-basedbenchmarking in future works.",Online,"Nekvinda, Tom{\'a}{\v{s}}  and
Du{\v{s}}ek, Ond{\v{r}}ej","Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",10.18653/v1/2021.gem-1.4,August,34--46,Association for Computational Linguistics,"Shades of {BLEU}, Flavours of Success: The Case of {M}ulti{WOZ}",https://aclanthology.org/2021.gem-1.4,2021,,,,,
107,inproceedings,hamalainen-alnajjar-2021-human,"We survey human evaluation in papers presenting work on creative natural language generation that have been published in INLG 2020 and ICCC 2020. The most typical human evaluation method is a scaled survey, typically on a 5 point scale, while many other less common methods exist. The most commonly evaluated parameters are meaning, syntactic correctness, novelty, relevance and emotional value, among many others. Our guidelines for future evaluation include clearly defining the goal of the generative system, asking questions as concrete as possible, testing the evaluation setup, using multiple different evaluation setups, reporting the entire evaluation process and potential biases clearly, and finally analyzing the evaluation results in a more profound way than merely reporting the most typical statistics.",Online,"H{\""a}m{\""a}l{\""a}inen, Mika  and
Alnajjar, Khalid","Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",10.18653/v1/2021.gem-1.9,August,84--95,Association for Computational Linguistics,Human Evaluation of Creative {NLG} Systems: An Interdisciplinary Survey on Recent Papers,https://aclanthology.org/2021.gem-1.9,2021,,,,,
108,inproceedings,jiao-luo-2021-gender,"Gender bias in word embeddings gradually becomes a vivid research field in recent years. Most studies in this field aim at measurement and debiasing methods with English as the target language. This paper investigates gender bias in static word embeddings from a unique perspective, Chinese adjectives. By training word representations with different models, the gender bias behind the vectors of adjectives is assessed. Through a comparison between the produced results and a human scored data set, we demonstrate how gender bias encoded in word embeddings differentiates from people{'}s attitudes.",Online,"Jiao, Meichun  and
Luo, Ziyang",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.2,August,8--15,Association for Computational Linguistics,Gender Bias Hidden Behind {C}hinese Word Embeddings: The Case of {C}hinese Adjectives,https://aclanthology.org/2021.gebnlp-1.2,2021,,,,,
109,inproceedings,ramesh-etal-2021-evaluating,"With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",Online,"Ramesh, Krithika  and
Gupta, Gauri  and
Singh, Sanjay",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.3,August,16--23,Association for Computational Linguistics,Evaluating Gender Bias in {H}indi-{E}nglish Machine Translation,https://aclanthology.org/2021.gebnlp-1.3,2021,,,,,
110,inproceedings,doughman-etal-2021-gender,"Gender inequality represents a considerable loss of human potential and perpetuates a culture of violence, higher gender wage gaps, and a lack of representation of women in higher and leadership positions. Applications powered by Artificial Intelligence (AI) are increasingly being used in the real world to provide critical decisions about who is going to be hired, granted a loan, admitted to college, etc. However, the main pillars of AI, Natural Language Processing (NLP) and Machine Learning (ML) have been shown to reflect and even amplify gender biases and stereotypes, which are mainly inherited from historical training data. In an effort to facilitate the identification and mitigation of gender bias in English text, we develop a comprehensive taxonomy that relies on the following gender bias types: Generic Pronouns, Sexism, Occupational Bias, Exclusionary Bias, and Semantics. We also provide a bottom-up overview of gender bias, from its societal origin to its spillover onto language. Finally, we link the societal implications of gender bias to their corresponding type(s) in the proposed taxonomy. The underlying motivation of our work is to help enable the technical community to identify and mitigate relevant biases from training corpora for improved fairness in NLP systems.",Online,"Doughman, Jad  and
Khreich, Wael  and
El Gharib, Maya  and
Wiss, Maha  and
Berjawi, Zahraa",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.5,August,34--44,Association for Computational Linguistics,"Gender Bias in Text: Origin, Taxonomy, and Implications",https://aclanthology.org/2021.gebnlp-1.5,2021,,,,,
111,inproceedings,baker-gillis-2021-sexism,"We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms{'} inconsistent results are consequences of prior research{'}s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., {`}salary,{'} {`}job,{'} and {`}boss{'} to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers{'} own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method{'}s performance, we regress our results of bias in case law against U.S census data of women{'}s participation in the workforce in the last 100 years.",Online,"Baker Gillis, Noa",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.6,August,45--54,Association for Computational Linguistics,Sexism in the Judiciary: The Importance of Bias Definition in {NLP} and In Our Courts,https://aclanthology.org/2021.gebnlp-1.6,2021,,,,,
112,inproceedings,excell-al-moubayed-2021-towards,"Classifiers tend to propagate biases present in the data on which they are trained. Hence, it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators. We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7{\%} of toxic comments as having been annotated by men. We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8{\%} better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5{\%} while increasing the sensitivity by 0.4{\%}.",Online,"Excell, Elizabeth  and
Al Moubayed, Noura",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.7,August,55--65,Association for Computational Linguistics,Towards Equal Gender Representation in the Annotations of Toxic Language Detection,https://aclanthology.org/2021.gebnlp-1.7,2021,,,,,
113,inproceedings,touileb-etal-2021-using,"In this work we explore the effect of incorporating demographic metadata in a text classifier trained on top of a pre-trained transformer language model. More specifically, we add information about the gender of critics and book authors when classifying the polarity of book reviews, and the polarity of the reviews when classifying the genders of authors and critics. We use an existing data set of Norwegian book reviews with ratings by professional critics, which has also been augmented with gender information, and train a document-level sentiment classifier on top of a recently released Norwegian BERT-model. We show that gender-informed models obtain substantially higher accuracy, and that polarity-informed models obtain higher accuracy when classifying the genders of book authors. For this particular data set, we take this result as a confirmation of the gender bias in the underlying label distribution, but in other settings we believe a similar approach can be used for mitigating bias in the model.",Online,"Touileb, Samia  and
{\O}vrelid, Lilja  and
Velldal, Erik",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.8,August,66--74,Association for Computational Linguistics,Using Gender- and Polarity-Informed Models to Investigate Bias,https://aclanthology.org/2021.gebnlp-1.8,2021,,,,,
114,inproceedings,falenska-cetinoglu-2021-assessing,"Potential gender biases existing in Wikipedia{'}s content can contribute to biased behaviors in a variety of downstream NLP systems. Yet, efforts in understanding what inequalities in portraying women and men occur in Wikipedia focused so far only on *biographies*, leaving open the question of how often such harmful patterns occur in other topics. In this paper, we investigate gender-related asymmetries in Wikipedia titles from *all domains*. We assess that for only half of gender-related articles, i.e., articles with words such as *women* or *male* in their titles, symmetrical counterparts describing the same concept for the other gender (and clearly stating it in their titles) exist. Among the remaining imbalanced cases, the vast majority of articles concern sports- and social-related issues. We provide insights on how such asymmetries can influence other Wikipedia components and propose steps towards reducing the frequency of observed patterns.",Online,"Falenska, Agnieszka  and
{\c{C}}etino{\u{g}}lu, {\""O}zlem",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.9,August,75--85,Association for Computational Linguistics,Assessing Gender Bias in {W}ikipedia: Inequalities in Article Titles,https://aclanthology.org/2021.gebnlp-1.9,2021,,,,,
115,inproceedings,jain-etal-2021-generating,"Gender bias is a frequent occurrence in NLP-based applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation system trained to {`}translate{'} from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results with respect to the automatic generation of gender alternatives for conversational sentences in Spanish.",Online,"Jain, Nishtha  and
Popovi{\'c}, Maja  and
Groves, Declan  and
Vanmassenhove, Eva",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.11,August,93--102,Association for Computational Linguistics,Generating Gender Augmented Data for {NLP},https://aclanthology.org/2021.gebnlp-1.11,2021,,,,,
116,inproceedings,dawkins-2021-second,"We observe an instance of gender-induced bias in a downstream application, despite the absence of explicit gender words in the test cases. We provide a test set, SoWinoBias, for the purpose of measuring such latent gender bias in coreference resolution systems. We evaluate the performance of current debiasing methods on the SoWinoBias test set, especially in reference to the method{'}s design and altered embedding space properties. See https://github.com/hillary-dawkins/SoWinoBias.",Online,"Dawkins, Hillary",Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,10.18653/v1/2021.gebnlp-1.12,August,103--111,Association for Computational Linguistics,Second Order {W}ino{B}ias ({S}o{W}ino{B}ias) Test Set for Latent Gender Bias Detection in Coreference Resolution,https://aclanthology.org/2021.gebnlp-1.12,2021,,,,,
117,inproceedings,liu-etal-2021-dense-hierarchical,"Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations, thus deteriorating the final retrieval result. In this work, we propose Dense Hierarchical Retrieval (DHR), a hierarchical framework which can generate accurate dense representations of passages by utilizing both macroscopic semantics in the document and microscopic semantics specific to each passage. Specifically, a document-level retriever first identifies relevant documents, among which relevant passages are then retrieved by a passage-level retriever. The ranking of the retrieved passages will be further calibrated by examining the document-level relevance. In addition, hierarchical title structure and two negative sampling strategies (i.e., In-Doc and In-Sec negatives) are investigated. We apply DHR to large-scale open-domain QA datasets. DHR significantly outperforms the original dense passage retriever, and helps an end-to-end QA system outperform the strong baselines on multiple open-domain QA benchmarks.","Punta Cana, Dominican Republic","Liu, Ye  and
Hashimoto, Kazuma  and
Zhou, Yingbo  and
Yavuz, Semih  and
Xiong, Caiming  and
Yu, Philip",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.19,November,188--200,Association for Computational Linguistics,Dense Hierarchical Retrieval for Open-domain Question Answering,https://aclanthology.org/2021.findings-emnlp.19,2021,,,,,
118,inproceedings,kuo-etal-2021-compositional-networks,"Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the limitations in the language-understanding abilities of networks. We demonstrate that these limitations can be overcome by addressing the generalization challenges in the gSCAN dataset, which explicitly measures how well an agent is able to interpret novel linguistic commands grounded in vision, e.g., novel pairings of adjectives and nouns. The key principle we employ is compositionality: that the compositional structure of networks should reflect the compositional structure of the problem domain they address, while allowing other parameters to be learned end-to-end. We build a general-purpose mechanism that enables agents to generalize their language understanding to compositional domains. Crucially, our network has the same state-of-the-art performance as prior work while generalizing its knowledge when prior work does not. Our network also provides a level of interpretability that enables users to inspect what each part of networks learns. Robust grounded language understanding without dramatic failures and without corner cases is critical to building safe and fair robots; we demonstrate the significant role that compositionality can play in achieving that goal.","Punta Cana, Dominican Republic","Kuo, Yen-Ling  and
Katz, Boris  and
Barbu, Andrei",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.21,November,216--226,Association for Computational Linguistics,Compositional Networks Enable Systematic Generalization for Grounded Language Understanding,https://aclanthology.org/2021.findings-emnlp.21,2021,,,,,
119,inproceedings,spinde-etal-2021-neural-media,"Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard data set and high context dependencies. This paper presents BABE, a robust and diverse data set created by trained experts, for media bias research. We also analyze why expert labeling is essential within this domain. Our data set offers better annotation quality and higher inter-annotator agreement than existing work. It consists of 3,700 sentences balanced among topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1-score of 0.804, outperforming existing methods.","Punta Cana, Dominican Republic","Spinde, Timo  and
Plank, Manuel  and
Krieger, Jan-David  and
Ruas, Terry  and
Gipp, Bela  and
Aizawa, Akiko",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.101,November,1166--1177,Association for Computational Linguistics,Neural Media Bias Detection Using Distant Supervision With {BABE} - Bias Annotations By Experts,https://aclanthology.org/2021.findings-emnlp.101,2021,,,,,
120,inproceedings,liu-etal-2021-sent2span-span,"The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening{---}the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in a human-machine pipeline for turning low-quality, crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews.","Punta Cana, Dominican Republic","Liu, Shifeng  and
Sun, Yifang  and
Li, Bing  and
Wang, Wei  and
Bourgeois, Florence T.  and
Dunn, Adam G.",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.147,November,1705--1715,Association for Computational Linguistics,{S}ent2{S}pan: Span Detection for {PICO} Extraction in the Biomedical Text without Span Annotations,https://aclanthology.org/2021.findings-emnlp.147,2021,,,,,
121,inproceedings,ham-kim-2021-semantic-alignment,"Measuring the similarity score between a pair of sentences in different languages is the essential requisite for multilingual sentence embedding methods. Predicting the similarity score consists of two sub-tasks, which are monolingual similarity evaluation and multilingual sentence retrieval. However, conventional methods have mainly tackled only one of the sub-tasks and therefore showed biased performances. In this paper, we suggest a novel and strong method for multilingual sentence embedding, which shows performance improvement on both sub-tasks, consequently resulting in robust predictions of multilingual similarity scores. The suggested method consists of two parts: to learn semantic similarity of sentences in the pivot language and then to extend the learned semantic structure to different languages. To align semantic structures across different languages, we introduce a teacher-student network. The teacher network distills the knowledge of the pivot language to different languages of the student network. During the distillation, the parameters of the teacher network are updated with the slow-moving average. Together with the distillation and the parameter updating, the semantic structure of the student network can be directly aligned across different languages while preserving the ability to measure the semantic similarity. Thus, the multilingual training method drives performance improvement on multilingual similarity evaluation. The suggested model achieves the state-of-the-art performance on extended STS 2017 multilingual similarity evaluation as well as two sub-tasks, which are extended STS 2017 monolingual similarity evaluation and Tatoeba multilingual retrieval in 14 languages.","Punta Cana, Dominican Republic","Ham, Jiyeon  and
Kim, Eun-Sol",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.153,November,1781--1791,Association for Computational Linguistics,Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding,https://aclanthology.org/2021.findings-emnlp.153,2021,,,,,
122,inproceedings,zhong-etal-2021-wikibias-detecting,"Biases continue to be prevalent in modern text and media, especially subjective bias {--} a special type of bias that introduces improper attitudes or presents a statement with the presupposition of truth. To tackle the problem of detecting and further mitigating subjective bias, we introduce a manually annotated parallel corpus WIKIBIAS with more than 4,000 sentence pairs from Wikipedia edits. This corpus contains annotations towards both sentence-level bias types and token-level biased segments. We present systematic analyses of our dataset and results achieved by a set of state-of-the-art baselines in terms of three tasks: bias classification, tagging biased segments, and neutralizing biased text. We find that current models still struggle with detecting multi-span biases despite their reasonable performances, suggesting that our dataset can serve as a useful research benchmark. We also demonstrate that models trained on our dataset can generalize well to multiple domains such as news and political speeches.","Punta Cana, Dominican Republic","Zhong, Yang  and
Yang, Jingfeng  and
Xu, Wei  and
Yang, Diyi",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.155,November,1799--1814,Association for Computational Linguistics,{WIKIBIAS}: Detecting Multi-Span Subjective Biases in Language,https://aclanthology.org/2021.findings-emnlp.155,2021,,,,,
123,inproceedings,du-ji-2021-sidecontrol-controlled,"Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods lead to high computation cost and can easily get overfitted on small training sets, while weighted-decoding methods are inherently constrained by the low-variance high-bias pre-trained model. In this work, we propose a novel approach to control the generation of Transformer-based pre-trained language models: the SideControl framework, which leverages a novel control attributes loss to incorporate useful control signals, and is shown to perform well with very limited training samples. We evaluate our proposed method on two benchmark open-domain dialogue datasets, and results show that the SideControl framework has better controllability, higher generation quality and better sample-efficiency than existing gradient-based and weighted-decoding baselines.","Punta Cana, Dominican Republic","Du, Wanyu  and
Ji, Yangfeng",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.188,November,2175--2194,Association for Computational Linguistics,{S}ide{C}ontrol: Controlled Open-domain Dialogue Generation via Additive Side Networks,https://aclanthology.org/2021.findings-emnlp.188,2021,,,,,
124,inproceedings,zhao-etal-2021-give-truth,"Abstractive dialogue summarization suffers from a lots of factual errors, which are due to scattered salient elements in the multi-speaker information interaction process. In this work, we design a heterogeneous semantic slot graph with a slot-level mask cross-attention to enhance the slot features for more correct summarization. We also propose a slot-driven beam search algorithm in the decoding process to give priority to generating salient elements in a limited length by {``}filling-in-the-blanks{''}. Besides, an adversarial contrastive learning assisting the training process is introduced to alleviate the exposure bias. Experimental performance on different types of factual errors shows the effectiveness of our methods and human evaluation further verifies the results..","Punta Cana, Dominican Republic","Zhao, Lulu  and
Zeng, Weihao  and
Xu, Weiran  and
Guo, Jun",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.209,November,2435--2446,Association for Computational Linguistics,Give the Truth: Incorporate Semantic Slot into Abstractive Dialogue Summarization,https://aclanthology.org/2021.findings-emnlp.209,2021,,,,,
125,inproceedings,welbl-etal-2021-challenges-detoxifying,"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions{---}highlighting further the nuances involved in careful evaluation of LM toxicity.","Punta Cana, Dominican Republic","Welbl, Johannes  and
Glaese, Amelia  and
Uesato, Jonathan  and
Dathathri, Sumanth  and
Mellor, John  and
Hendricks, Lisa Anne  and
Anderson, Kirsty  and
Kohli, Pushmeet  and
Coppin, Ben  and
Huang, Po-Sen",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.210,November,2447--2469,Association for Computational Linguistics,Challenges in Detoxifying Language Models,https://aclanthology.org/2021.findings-emnlp.210,2021,,,,,
126,inproceedings,levy-etal-2021-collecting-large,"Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at github.com/SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.","Punta Cana, Dominican Republic","Levy, Shahar  and
Lazar, Koren  and
Stanovsky, Gabriel",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.211,November,2470--2480,Association for Computational Linguistics,Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation,https://aclanthology.org/2021.findings-emnlp.211,2021,,,,,
127,inproceedings,haber-waks-2021-classification-geotemporal,"Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is both biased and time-consuming. Using this as motivation, we asked whether a text classification-based approach can automate the detection of four carefully defined, major quality-of-life issues: severe crime, noise nuisance, pest burden, and parking difficulties. We aggregate 5.5 million tenant reviews from five sources and use two-stage crowdsourced labeling on 0.1{\%} of the data to produce high-quality labels for subsequent text classification. Following fine-tuning of pretrained language models on millions of reviews, we train a multi-label reviews classifier that achieves a mean AUROC of 0.965 on these labels. We next use the model to reveal temporal and spatial patterns among tens of thousands of multifamily properties. Collectively, these results highlight the feasibility of automated analysis of housing trends and investment opportunities using tenant-perspective data.","Punta Cana, Dominican Republic","Haber, Adam  and
Waks, Zeev",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.217,November,2541--2553,Association for Computational Linguistics,Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews,https://aclanthology.org/2021.findings-emnlp.217,2021,,,,,
128,inproceedings,wang-etal-2021-eliminating-sentiment,"Aspect-level sentiment classification (ALSC) aims at identifying the sentiment polarity of a specified aspect in a sentence. ALSC is a practical setting in aspect-based sentiment analysis due to no opinion term labeling needed, but it fails to interpret why a sentiment polarity is derived for the aspect. To address this problem, recent works fine-tune pre-trained Transformer encoders for ALSC to extract an aspect-centric dependency tree that can locate the opinion words. However, the induced opinion words only provide an intuitive cue far below human-level interpretability. Besides, the pre-trained encoder tends to internalize an aspect{'}s intrinsic sentiment, causing sentiment bias and thus affecting model performance. In this paper, we propose a span-based anti-bias aspect representation learning framework. It first eliminates the sentiment bias in the aspect embedding by adversarial learning against aspects{'} prior sentiment. Then, it aligns the distilled opinion candidates with the aspect by span-based dependency modeling to highlight the interpretable opinion terms. Our method achieves new state-of-the-art performance on five benchmarks, with the capability of unsupervised opinion extraction.","Punta Cana, Dominican Republic","Wang, Bo  and
Shen, Tao  and
Long, Guodong  and
Zhou, Tianyi  and
Chang, Yi",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.258,November,3002--3012,Association for Computational Linguistics,Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction,https://aclanthology.org/2021.findings-emnlp.258,2021,,,,,
129,inproceedings,hassan-etal-2021-unpacking-interdependent,"Much of the world{'}s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically significant results demonstrate that people with disabilities can be disadvantaged. Findings also explore overlapping forms of discrimination related to interconnected gender and race identities.","Punta Cana, Dominican Republic","Hassan, Saad  and
Huenerfauth, Matt  and
Alm, Cecilia Ovesdotter",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.267,November,3116--3123,Association for Computational Linguistics,Unpacking the Interdependent Systems of Discrimination: Ableist Bias in {NLP} Systems through an Intersectional Lens,https://aclanthology.org/2021.findings-emnlp.267,2021,,,,,
130,inproceedings,storks-chai-2021-beyond-tip,"As large-scale, pre-trained language models achieve human-level and superhuman accuracy on existing language understanding tasks, statistical bias in benchmark data and probing studies have recently called into question their true capabilities. For a more informative evaluation than accuracy on text classification tasks can offer, we propose evaluating systems through a novel measure of prediction coherence. We apply our framework to two existing language understanding benchmarks with different properties to demonstrate its versatility. Our experimental results show that this evaluation framework, although simple in ideas and implementation, is a quick, effective, and versatile measure to provide insight into the coherence of machines{'} predictions.","Punta Cana, Dominican Republic","Storks, Shane  and
Chai, Joyce",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.272,November,3169--3177,Association for Computational Linguistics,Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers,https://aclanthology.org/2021.findings-emnlp.272,2021,,,,,
131,inproceedings,krishna-etal-2021-pretraining-summarization,"Pretraining techniques leveraging enormous datasets have driven recent advances in text summarization. While folk explanations suggest that knowledge transfer accounts for pretraining{'}s benefits, little is known about why it works or what makes a pretraining task or dataset suitable. In this paper, we challenge the knowledge transfer story, showing that pretraining on documents consisting of character n-grams selected at random, we can nearly match the performance of models pretrained on real corpora. This work holds the promise of eliminating upstream corpora, which may alleviate some concerns over offensive language, bias, and copyright issues. To see whether the small residual benefit of using real data could be accounted for by the structure of the pretraining task, we design several tasks motivated by a qualitative study of summarization corpora. However, these tasks confer no appreciable benefit, leaving open the possibility of a small role for knowledge transfer.","Punta Cana, Dominican Republic","Krishna, Kundan  and
Bigham, Jeffrey  and
Lipton, Zachary C.",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.273,November,3178--3189,Association for Computational Linguistics,Does Pretraining for Summarization Require Knowledge Transfer?,https://aclanthology.org/2021.findings-emnlp.273,2021,,,,,
132,inproceedings,faisal-etal-2021-sd-qa,"Question answering (QA) systems are now available through numerous commercial applications for a wide variety of domains, serving millions of users that interact with them via speech interfaces. However, current benchmarks in QA research do not account for the errors that speech recognition models might introduce, nor do they consider the language variations (dialects) of the users. To address this gap, we augment an existing QA dataset to construct a multi-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English, Kiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255 speakers. We provide baseline results showcasing the real-world performance of QA systems and analyze the effect of language variety and other sensitive speaker attributes on downstream performance. Last, we study the fairness of the ASR and QA models with respect to the underlying user populations.","Punta Cana, Dominican Republic","Faisal, Fahim  and
Keshava, Sharlina  and
Alam, Md Mahfuz Ibn  and
Anastasopoulos, Antonios",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.281,November,3296--3315,Association for Computational Linguistics,{SD}-{QA}: Spoken Dialectal Question Answering for the Real World,https://aclanthology.org/2021.findings-emnlp.281,2021,,,,,
133,inproceedings,keidar-etal-2021-towards-automatic,"With the recent surge in social applications relying on knowledge graphs, the need for techniques to ensure fairness in KG based methods is becoming increasingly evident. Previous works have demonstrated that KGs are prone to various social biases, and have proposed multiple methods for debiasing them. However, in such studies, the focus has been on debiasing techniques, while the relations to be debiased are specified manually by the user. As manual specification is itself susceptible to human cognitive bias, there is a need for a system capable of quantifying and exposing biases, that can support more informed decisions on what to debias. To address this gap in the literature, we describe a framework for identifying biases present in knowledge graph embeddings, based on numerical bias metrics. We illustrate the framework with three different bias measures on the task of profession prediction, and it can be flexibly extended to further bias definitions and applications. The relations flagged as biased can then be handed to decision makers for judgement upon subsequent debiasing.","Punta Cana, Dominican Republic","Keidar, Daphna  and
Zhong, Mian  and
Zhang, Ce  and
Shrestha, Yash Raj  and
Paudel, Bibek",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.321,November,3804--3811,Association for Computational Linguistics,Towards Automatic Bias Detection in Knowledge Graphs,https://aclanthology.org/2021.findings-emnlp.321,2021,,,,,
134,inproceedings,huang-etal-2021-uncovering-implicit,"Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly ({``}women can{'}t park{''}) or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the protagonist{'}s motivations, attributes, mental states, and implications on others. Our findings regarding implicit biases are in line with prior work that studied explicit biases, for example showing that female characters{'} portrayal is centered around appearance, while male figures{'} focus on intellect.","Punta Cana, Dominican Republic","Huang, Tenghao  and
Brahman, Faeze  and
Shwartz, Vered  and
Chaturvedi, Snigdha",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.326,November,3866--3873,Association for Computational Linguistics,Uncovering Implicit Gender Bias in Narratives through Commonsense Inference,https://aclanthology.org/2021.findings-emnlp.326,2021,,,,,
135,inproceedings,glushkova-etal-2021-uncertainty-aware,"Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting in unreliable quality predictions. In this paper, we introduce uncertainty-aware MT evaluation and analyze the trustworthiness of the predicted quality. We combine the COMET framework with two uncertainty estimation methods, Monte Carlo dropout and deep ensembles, to obtain quality scores along with confidence intervals. We compare the performance of our uncertainty-aware MT evaluation methods across multiple language pairs from the QT21 dataset and the WMT20 metrics task, augmented with MQM annotations. We experiment with varying numbers of references and further discuss the usefulness of uncertainty-aware quality estimation (without references) to flag possibly critical translation mistakes.","Punta Cana, Dominican Republic","Glushkova, Taisiya  and
Zerva, Chrysoula  and
Rei, Ricardo  and
Martins, Andr{\'e} F. T.",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.330,November,3920--3938,Association for Computational Linguistics,Uncertainty-Aware Machine Translation Evaluation,https://aclanthology.org/2021.findings-emnlp.330,2021,,,,,
136,inproceedings,garcia-ferrero-etal-2021-benchmarking-meta,"In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework, including both intrinsic and extrinsic tasks, for a fair and objective meta-embeddings evaluation. Furthermore, we present a new method to generate meta-embeddings, outperforming previous work on a large number of intrinsic evaluation benchmarks. Our evaluation framework also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been overestimated.","Punta Cana, Dominican Republic","Garc{\'\i}a-Ferrero, Iker  and
Agerri, Rodrigo  and
Rigau, German",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.333,November,3957--3972,Association for Computational Linguistics,Benchmarking Meta-embeddings: What Works and What Does Not,https://aclanthology.org/2021.findings-emnlp.333,2021,,,,,
137,inproceedings,dodge-etal-2021-expected-validation,"Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., accuracy) as a function of computational budget (e.g., number of hyperparameter tuning experiments). Where previous work analyzing such estimators focused on the bias, we also examine the variance and mean squared error (MSE). In both synthetic and realistic scenarios, we evaluate three estimators and find the unbiased estimator has the highest variance, and the estimator with the smallest variance has the largest bias; the estimator with the smallest MSE strikes a balance between bias and variance, displaying a classic bias-variance tradeoff. We use expected validation performance to compare between different models, and analyze how frequently each estimator leads to drawing incorrect conclusions about which of two models performs best. We find that the two biased estimators lead to the fewest incorrect conclusions, which hints at the importance of minimizing variance and MSE.","Punta Cana, Dominican Republic","Dodge, Jesse  and
Gururangan, Suchin  and
Card, Dallas  and
Schwartz, Roy  and
Smith, Noah A.",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.342,November,4066--4073,Association for Computational Linguistics,Expected Validation Performance and Estimation of a Random Variable{'}s Maximum,https://aclanthology.org/2021.findings-emnlp.342,2021,,,,,
138,inproceedings,he-etal-2021-detect-perturb,"Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to {`}neutralize{'} sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes.","Punta Cana, Dominican Republic","He, Zexue  and
Majumder, Bodhisattwa Prasad  and
McAuley, Julian",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.352,November,4173--4181,Association for Computational Linguistics,Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding,https://aclanthology.org/2021.findings-emnlp.352,2021,,,,,
139,inproceedings,chopra-etal-2021-switch-point,"Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence labeling tasks {--} POS and NER on 4 different language pairs with a suite of pretrained models to identify the problems and select the best performing char-BERT model among them (addressing (1)). We then propose a self training method to repurpose the existing pretrained models using a switch-point bias by leveraging unannotated data (addressing (2)). We finally demonstrate that our approach performs well on both tasks by reducing the gap between the switch point performance while retaining the overall performance on two distinct language pairs in both the tasks. We plan to release our models and the code for all our experiments.","Punta Cana, Dominican Republic","Chopra, Parul  and
Rallabandi, Sai Krishna  and
Black, Alan W  and
Chandu, Khyathi Raghavi",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.373,November,4389--4397,Association for Computational Linguistics,Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching,https://aclanthology.org/2021.findings-emnlp.373,2021,,,,,
140,inproceedings,amirkhani-pilehvar-2021-dont-discard,"Existing techniques for mitigating dataset bias often leverage a biased model to identify biased instances. The role of these biased instances is then reduced during the training of the main model to enhance its robustness to out-of-distribution data. A common core assumption of these techniques is that the main model handles biased instances similarly to the biased model, in that it will resort to biases whenever available. In this paper, we show that this assumption does not hold in general. We carry out a critical investigation on two well-known datasets in the domain, MNLI and FEVER, along with two biased instance detection methods, partial-input and limited-capacity models. Our experiments show that in around a third to a half of instances, the biased model is unable to predict the main model{'}s behavior, highlighted by the significantly different parts of the input on which they base their decisions. Based on a manual validation, we also show that this estimate is highly in line with human interpretation. Our findings suggest that down-weighting of instances detected by bias detection methods, which is a widely-practiced procedure, is an unnecessary waste of training data. We release our code to facilitate reproducibility and future research.","Punta Cana, Dominican Republic","Amirkhani, Hossein  and
Pilehvar, Mohammad Taher",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.405,November,4720--4728,Association for Computational Linguistics,Don{'}t Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques,https://aclanthology.org/2021.findings-emnlp.405,2021,,,,,
141,inproceedings,lauscher-etal-2021-sustainable-modular,"Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which {--} besides being computationally expensive {--} comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE, in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE, very effective in bias mitigation. We further show that {--} due to its modular nature {--} ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE, to six target languages.","Punta Cana, Dominican Republic","Lauscher, Anne  and
Lueken, Tobias  and
Glava{\v{s}}, Goran",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.411,November,4782--4797,Association for Computational Linguistics,Sustainable Modular Debiasing of Language Models,https://aclanthology.org/2021.findings-emnlp.411,2021,,,,,
142,inproceedings,wang-etal-2021-counterfactual-adversarial,"Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering.","Punta Cana, Dominican Republic","Wang, Wei  and
Wang, Boxin  and
Shi, Ning  and
Li, Jinfeng  and
Zhu, Bingyu  and
Liu, Xiangyu  and
Zhang, Rong",Findings of the Association for Computational Linguistics: EMNLP 2021,10.18653/v1/2021.findings-emnlp.413,November,4809--4820,Association for Computational Linguistics,Counterfactual Adversarial Learning with Representation Interpolation,https://aclanthology.org/2021.findings-emnlp.413,2021,,,,,
143,inproceedings,khurana-etal-2021-emotionally,"Despite their success, modern language models are fragile. Even small changes in their training pipeline can lead to unexpected results. We study this phenomenon by examining the robustness of ALBERT (Lan et al., 2020) in combination with Stochastic Weight Averaging (SWA){---}a cheap way of ensembling{---}on a sentiment analysis task (SST-2). In particular, we analyze SWA{'}s stability via CheckList criteria (Ribeiro et al., 2020), examining the agreement on errors made by models differing only in their random seed. We hypothesize that SWA is more stable because it ensembles model snapshots taken along the gradient descent trajectory. We quantify stability by comparing the models{'} mistakes with Fleiss{'} Kappa (Fleiss, 1971) and overlap ratio scores. We find that SWA reduces error rates in general; yet the models still suffer from their own distinct biases (according to CheckList).","Punta Cana, Dominican Republic","Khurana, Urja  and
Nalisnick, Eric  and
Fokkens, Antske",Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,10.18653/v1/2021.eval4nlp-1.3,November,16--31,Association for Computational Linguistics,How Emotionally Stable is {ALBERT}? Testing Robustness with Stochastic Weight Averaging on a Sentiment Analysis Task,https://aclanthology.org/2021.eval4nlp-1.3,2021,,,,,
144,inproceedings,wolfe-caliskan-2021-low,"We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman{'}s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman{'}s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman{'}s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.","Online and Punta Cana, Dominican Republic","Wolfe, Robert  and
Caliskan, Aylin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.41,November,518--532,Association for Computational Linguistics,Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models,https://aclanthology.org/2021.emnlp-main.41,2021,,,,,
145,inproceedings,ahn-oh-2021-mitigating,"In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.","Online and Punta Cana, Dominican Republic","Ahn, Jaimeen  and
Oh, Alice",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.42,November,533--549,Association for Computational Linguistics,Mitigating Language-Dependent Ethnic Bias in {BERT},https://aclanthology.org/2021.emnlp-main.42,2021,,,,,
146,inproceedings,basu-roy-chowdhury-etal-2021-adversarial,"Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target task. In this paper, we present an adversarial learning framework {``}Adversarial Scrubber{''} (AdS), to debias contextual representations. We perform theoretical analysis to show that our framework converges without leaking demographic information under certain conditions. We extend previous evaluation techniques by evaluating debiasing performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates representations with minimal information about demographic attributes while being maximally informative about the target task.","Online and Punta Cana, Dominican Republic","Basu Roy Chowdhury, Somnath  and
Ghosh, Sayan  and
Li, Yiyuan  and
Oliva, Junier  and
Srivastava, Shashank  and
Chaturvedi, Snigdha",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.43,November,550--562,Association for Computational Linguistics,Adversarial Scrubbing of Demographic Information for Text Classification,https://aclanthology.org/2021.emnlp-main.43,2021,,,,,
147,inproceedings,meister-etal-2021-conditional,"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019){'}s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.","Online and Punta Cana, Dominican Republic","Meister, Clara  and
Amini, Afra  and
Vieira, Tim  and
Cotterell, Ryan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.52,November,664--681,Association for Computational Linguistics,Conditional {P}oisson Stochastic Beams,https://aclanthology.org/2021.emnlp-main.52,2021,,,,,
148,inproceedings,ravishankar-sogaard-2021-impact,"In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is: sinusoidal encodings were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, compositionality becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the inductive bias to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.","Online and Punta Cana, Dominican Republic","Ravishankar, Vinit  and
S{\o}gaard, Anders",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.59,November,763--777,Association for Computational Linguistics,The Impact of Positional Encodings on Multilingual Compression,https://aclanthology.org/2021.emnlp-main.59,2021,,,,,
149,inproceedings,paik-etal-2021-world,"Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human{'}s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.","Online and Punta Cana, Dominican Republic","Paik, Cory  and
Aroca-Ouellette, St{\'e}phane  and
Roncone, Alessandro  and
Kann, Katharina",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.63,November,823--835,Association for Computational Linguistics,{T}he {W}orld of an {O}ctopus: {H}ow {R}eporting {B}ias {I}nfluences a {L}anguage {M}odel{'}s {P}erception of {C}olor,https://aclanthology.org/2021.emnlp-main.63,2021,,,,,
150,inproceedings,pujari-goldwasser-2021-understanding,"Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.","Online and Punta Cana, Dominican Republic","Pujari, Rajkumar  and
Goldwasser, Dan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.102,November,1353--1367,Association for Computational Linguistics,Understanding Politics via Contextualized Discourse Processing,https://aclanthology.org/2021.emnlp-main.102,2021,,,,,
151,inproceedings,mendelson-belinkov-2021-debiasing,"Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model{'}s inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model{'}s representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.","Online and Punta Cana, Dominican Republic","Mendelson, Michael  and
Belinkov, Yonatan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.116,November,1545--1557,Association for Computational Linguistics,Debiasing Methods in Natural Language Understanding Make Bias More Accessible,https://aclanthology.org/2021.emnlp-main.116,2021,,,,,
152,inproceedings,yuan-etal-2021-transferability,"Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a genetic algorithm to find an ensemble of models that can be used to induce adversarial examples to fool almost all existing models. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.","Online and Punta Cana, Dominican Republic","Yuan, Liping  and
Zheng, Xiaoqing  and
Zhou, Yi  and
Hsieh, Cho-Jui  and
Chang, Kai-Wei",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.121,November,1612--1625,Association for Computational Linguistics,On the Transferability of Adversarial Attacks against Neural Text Classifier,https://aclanthology.org/2021.emnlp-main.121,2021,,,,,
153,inproceedings,choubey-etal-2021-gfst,"Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.","Online and Punta Cana, Dominican Republic","Choubey, Prafulla Kumar  and
Currey, Anna  and
Mathur, Prashant  and
Dinu, Georgiana",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.123,November,1640--1654,Association for Computational Linguistics,{GFST}: {G}ender-Filtered Self-Training for More Accurate Gender in Translation,https://aclanthology.org/2021.emnlp-main.123,2021,,,,,
154,inproceedings,merrill-etal-2021-effects,"The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude ($\ell_2$ norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such {``}saturated{''} networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.","Online and Punta Cana, Dominican Republic","Merrill, William  and
Ramanujan, Vivek  and
Goldberg, Yoav  and
Schwartz, Roy  and
Smith, Noah A.",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.133,November,1766--1781,Association for Computational Linguistics,Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent,https://aclanthology.org/2021.emnlp-main.133,2021,,,,,
155,inproceedings,gardner-etal-2021-competency,"Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have {``}spurious{''} instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word {``}amazing{''} on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.","Online and Punta Cana, Dominican Republic","Gardner, Matt  and
Merrill, William  and
Dodge, Jesse  and
Peters, Matthew  and
Ross, Alexis  and
Singh, Sameer  and
Smith, Noah A.",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.135,November,1801--1813,Association for Computational Linguistics,Competency Problems: On Finding and Removing Artifacts in Language Data,https://aclanthology.org/2021.emnlp-main.135,2021,,,,,
156,inproceedings,dev-etal-2021-harms,"Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.","Online and Punta Cana, Dominican Republic","Dev, Sunipa  and
Monajatipoor, Masoud  and
Ovalle, Anaelia  and
Subramonian, Arjun  and
Phillips, Jeff  and
Chang, Kai-Wei",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.150,November,1968--1994,Association for Computational Linguistics,Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies,https://aclanthology.org/2021.emnlp-main.150,2021,,,,,
157,inproceedings,wang-etal-2021-gender,"Internet search affects people{'}s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.","Online and Punta Cana, Dominican Republic","Wang, Jialu  and
Liu, Yang  and
Wang, Xin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.151,November,1995--2008,Association for Computational Linguistics,Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search,https://aclanthology.org/2021.emnlp-main.151,2021,,,,,
158,inproceedings,mireshghallah-berg-kirkpatrick-2021-style,"Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in hiring decisions, regardless of whether hiring decisions are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style: (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling{'}s effect on fluency, semantic consistency, and attribute removal from text, in two and three domain style transfer.","Online and Punta Cana, Dominican Republic","Mireshghallah, Fatemehsadat  and
Berg-Kirkpatrick, Taylor",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.152,November,2009--2022,Association for Computational Linguistics,Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness,https://aclanthology.org/2021.emnlp-main.152,2021,,,,,
159,inproceedings,saxon-etal-2021-modeling,"Broader disclosive transparency{---}truth and clarity in communication regarding the function of AI systems{---}is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where {``}too much information{''} clouds a reader{'}s understanding of what a system description means. Disclosive transparency{'}s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions.","Online and Punta Cana, Dominican Republic","Saxon, Michael  and
Levy, Sharon  and
Wang, Xinyi  and
Albalak, Alon  and
Wang, William Yang",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.153,November,2023--2037,Association for Computational Linguistics,Modeling Disclosive Transparency in {NLP} Application Descriptions,https://aclanthology.org/2021.emnlp-main.153,2021,,,,,
160,inproceedings,subramanian-etal-2021-fairness,"Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.","Online and Punta Cana, Dominican Republic","Subramanian, Shivashankar  and
Rahimi, Afshin  and
Baldwin, Timothy  and
Cohn, Trevor  and
Frermann, Lea",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.155,November,2045--2051,Association for Computational Linguistics,Fairness-aware Class Imbalanced Learning,https://aclanthology.org/2021.emnlp-main.155,2021,,,,,
161,inproceedings,subramanian-etal-2021-evaluating,"Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider {`}gerrymandering{'} groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple identities.","Online and Punta Cana, Dominican Republic","Subramanian, Shivashankar  and
Han, Xudong  and
Baldwin, Timothy  and
Cohn, Trevor  and
Frermann, Lea",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.193,November,2492--2498,Association for Computational Linguistics,Evaluating Debiasing Techniques for Intersectional Biases,https://aclanthology.org/2021.emnlp-main.193,2021,,,,,
162,inproceedings,xu-etal-2021-adaptive,"Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario.In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task.What{'}s more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as {``}I don{'}t know{''} and {``}HaHa?{''} In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement.","Online and Punta Cana, Dominican Republic","Xu, Haoran  and
Zhang, Hainan  and
Zou, Yanyan  and
Chen, Hongshen  and
Ding, Zhuoye  and
Lan, Yanyan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.198,November,2541--2550,Association for Computational Linguistics,Adaptive Bridge between Training and Inference for Dialogue Generation,https://aclanthology.org/2021.emnlp-main.198,2021,,,,,
163,inproceedings,song-etal-2021-importance,"Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its syntactic accuracy, information saliency, and concept consistency simultaneously. However, most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results. In this paper, we propose a new approach to estimate the importance of keyphrase from multiple perspectives (called as \textit{KIEMP}) and further improve the performance of keyphrase extraction. Specifically, \textit{KIEMP} estimates the importance of phrase with three modules: a chunking module to measure its syntactic accuracy, a ranking module to check its information saliency, and a matching module to judge the concept (i.e., topic) consistency between phrase and the whole document. These three modules are seamlessly jointed together via an end-to-end multi-task learning model, which is helpful for three parts to enhance each other and balance the effects of three perspectives. Experimental results on six benchmark datasets show that \textit{KIEMP} outperforms the existing state-of-the-art keyphrase extraction approaches in most cases.","Online and Punta Cana, Dominican Republic","Song, Mingyang  and
Jing, Liping  and
Xiao, Lin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.215,November,2726--2736,Association for Computational Linguistics,{I}mportance {E}stimation from {M}ultiple {P}erspectives for {K}eyphrase {E}xtraction,https://aclanthology.org/2021.emnlp-main.215,2021,,,,,
164,inproceedings,hu-etal-2021-gradient,"Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to encourage pseudo label data to imitate the gradient descent direction on labeled data and bootstrap its optimization capability through trial and error. We also propose a framework called GradLRE, which handles two major scenarios in low-resource relation extraction. Besides the scenario where unlabeled data is sufficient, GradLRE handles the situation where no unlabeled data is available, by exploiting a contextualized augmentation method to generate data. Experimental results on two public datasets demonstrate the effectiveness of GradLRE on low resource relation extraction when comparing with baselines.","Online and Punta Cana, Dominican Republic","Hu, Xuming  and
Zhang, Chenwei  and
Yang, Yawen  and
Li, Xiaohe  and
Lin, Li  and
Wen, Lijie  and
Yu, Philip S.",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.216,November,2737--2746,Association for Computational Linguistics,Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction,https://aclanthology.org/2021.emnlp-main.216,2021,,,,,
165,inproceedings,wang-etal-2021-transprompt,"Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt embeddings, we propose TransPrompt, a transferable prompting framework for few-shot learning across similar tasks. In TransPrompt, we employ a multi-task meta-knowledge acquisition procedure to train a meta-learner that captures cross-task transferable knowledge. Two de-biasing techniques are further designed to make it more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to target tasks with high accuracy. Extensive experiments show that TransPrompt outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance on previously unseen tasks; and TransPrompt also outperforms strong fine-tuning baselines when learning with full training sets.","Online and Punta Cana, Dominican Republic","Wang, Chengyu  and
Wang, Jianing  and
Qiu, Minghui  and
Huang, Jun  and
Gao, Ming",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.221,November,2792--2802,Association for Computational Linguistics,{T}rans{P}rompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification,https://aclanthology.org/2021.emnlp-main.221,2021,,,,,
166,inproceedings,zhang-etal-2021-enlivening,"Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in the text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on widely adopted WMT14 and WMT16 English to German and English to Czech language machine translation validate the RHE effectiveness.","Online and Punta Cana, Dominican Republic","Zhang, Tianfu  and
Huang, Heyan  and
Feng, Chong  and
Cao, Longbing",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.260,November,3238--3248,Association for Computational Linguistics,Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation,https://aclanthology.org/2021.emnlp-main.260,2021,,,,,
167,inproceedings,liu-etal-2021-scheduled-sampling,"Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks.","Online and Punta Cana, Dominican Republic","Liu, Yijin  and
Meng, Fandong  and
Chen, Yufeng  and
Xu, Jinan  and
Zhou, Jie",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.264,November,3285--3296,Association for Computational Linguistics,Scheduled Sampling Based on Decoding Steps for Neural Machine Translation,https://aclanthology.org/2021.emnlp-main.264,2021,,,,,
168,inproceedings,liu-etal-2021-activeea,"Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods {--} neural EA models {--} rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In this paper, we devise a novel Active Learning (AL) framework for neural EA, aiming to create highly informative seed alignment to obtain more effective EA models with less annotation cost. Our framework tackles two main challenges encountered when applying AL to EA: (1) How to exploit dependencies between entities within the AL strategy. Most AL strategies assume that the data instances to sample are independent and identically distributed. However, entities in KGs are related. To address this challenge, we propose a structure-aware uncertainty sampling strategy that can measure the uncertainty of each entity as well as its impact on its neighbour entities in the KG. (2) How to recognise entities that appear in one KG but not in the other KG (i.e., bachelors). Identifying bachelors would likely save annotation budget. To address this challenge, we devise a bachelor recognizer paying attention to alleviate the effect of sampling bias. Empirical results show that our proposed AL strategy can significantly improve sampling quality with good generality across different datasets, EA models and amount of bachelors.","Online and Punta Cana, Dominican Republic","Liu, Bing  and
Scells, Harrisen  and
Zuccon, Guido  and
Hua, Wen  and
Zhao, Genghong",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.270,November,3364--3374,Association for Computational Linguistics,{A}ctive{EA}: Active Learning for Neural Entity Alignment,https://aclanthology.org/2021.emnlp-main.270,2021,,,,,
169,inproceedings,abbasi-etal-2021-constructing,"Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education - thereby affording opportunities for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict/categorize users{'} survey response labels - and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness.","Online and Punta Cana, Dominican Republic","Abbasi, Ahmed  and
Dobolyi, David  and
Lalor, John P.  and
Netemeyer, Richard G.  and
Smith, Kendall  and
Yang, Yi",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.304,November,3748--3758,Association for Computational Linguistics,Constructing a Psychometric Testbed for Fair Natural Language Processing,https://aclanthology.org/2021.emnlp-main.304,2021,,,,,
170,inproceedings,christiansen-etal-2021-effect,"Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47{\%} of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data.","Online and Punta Cana, Dominican Republic","Christiansen, Jonathan Gabel  and
Gammelgaard, Mathias  and
S{\o}gaard, Anders",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.363,November,4423--4428,Association for Computational Linguistics,The Effect of Round-Trip Translation on Fairness in Sentiment Analysis,https://aclanthology.org/2021.emnlp-main.363,2021,,,,,
171,inproceedings,zhang-etal-2021-sociolectal,"Using data from English cloze tests, in which subjects also self-reported their gender, age, education, and race, we examine performance differences of pretrained language models across demographic groups, defined by these (protected) attributes. We demonstrate wide performance gaps across demographic groups and show that pretrained language models systematically disfavor young non-white male speakers; i.e., not only do pretrained language models learn social biases (stereotypical associations) {--} pretrained language models also learn sociolectal biases, learning to speak more like some than like others. We show, however, that, with the exception of BERT models, larger pretrained language models reduce some the performance gaps between majority and minority groups.","Online and Punta Cana, Dominican Republic","Zhang, Sheng  and
Zhang, Xin  and
Zhang, Weiming  and
S{\o}gaard, Anders",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.375,November,4581--4588,Association for Computational Linguistics,Sociolectal Analysis of Pretrained Language Models,https://aclanthology.org/2021.emnlp-main.375,2021,,,,,
172,inproceedings,balashankar-etal-2021-improve,"Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model{'}s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7{\%} compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks.","Online and Punta Cana, Dominican Republic","Balashankar, Ananth  and
Wang, Xuezhi  and
Packer, Ben  and
Thain, Nithum  and
Chi, Ed  and
Beutel, Alex",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.386,November,4701--4712,Association for Computational Linguistics,Can We Improve Model Robustness through Secondary Attribute Counterfactuals?,https://aclanthology.org/2021.emnlp-main.386,2021,,,,,
173,inproceedings,mehrabi-etal-2021-lawyers,"Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms, overgeneralization of polarized perceptions and representation disparity across different demographic groups in both CSKBs. Next, we find similar representational harms for downstream models that use ConceptNet. Finally, we propose a filtering-based approach for mitigating such harms, and observe that our filtered-based approach can reduce the issues in both resources and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models.","Online and Punta Cana, Dominican Republic","Mehrabi, Ninareh  and
Zhou, Pei  and
Morstatter, Fred  and
Pujara, Jay  and
Ren, Xiang  and
Galstyan, Aram",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.410,November,5016--5033,Association for Computational Linguistics,Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources,https://aclanthology.org/2021.emnlp-main.410,2021,,,,,
174,inproceedings,dev-etal-2021-oscar,"Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.","Online and Punta Cana, Dominican Republic","Dev, Sunipa  and
Li, Tao  and
Phillips, Jeff M  and
Srikumar, Vivek",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.411,November,5034--5050,Association for Computational Linguistics,{OSC}a{R}: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings,https://aclanthology.org/2021.emnlp-main.411,2021,,,,,
175,inproceedings,he-etal-2021-exposure,"Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. In this work, we focus on the task of open-ended language generation, propose metrics to quantify the impact of exposure bias in the aspects of quality, diversity, and consistency. Our key intuition is that if we feed ground-truth data prefixes (instead of prefixes generated by the model itself) into the model and ask it to continue the generation, the performance should become much better because the training-generation discrepancy in the prefix is removed. Both automatic and human evaluations are conducted in our experiments. On the contrary to the popular belief in exposure bias, we find that the the distortion induced by the prefix discrepancy is limited, and does not seem to be incremental during the generation. Moreover, our analysis reveals an interesting self-recovery ability of the LM, which we hypothesize to be countering the harmful effects from exposure bias.","Online and Punta Cana, Dominican Republic","He, Tianxing  and
Zhang, Jingzhao  and
Zhou, Zhiming  and
Glass, James",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.415,November,5087--5102,Association for Computational Linguistics,Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?,https://aclanthology.org/2021.emnlp-main.415,2021,,,,,
176,inproceedings,zhou-etal-2021-generating,"Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.","Online and Punta Cana, Dominican Republic","Zhou, Li  and
Small, Kevin  and
Zhang, Yong  and
Atluri, Sandeep",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.416,November,5103--5135,Association for Computational Linguistics,Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning,https://aclanthology.org/2021.emnlp-main.416,2021,,,,,
177,inproceedings,lai-etal-2021-learning,"We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among classifiers across tasks to make the model more robust to outliers. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our model is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.","Online and Punta Cana, Dominican Republic","Lai, Viet  and
Dernoncourt, Franck  and
Nguyen, Thien Huu",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.427,November,5270--5277,Association for Computational Linguistics,Learning Prototype Representations Across Few-Shot Tasks for Event Detection,https://aclanthology.org/2021.emnlp-main.427,2021,,,,,
178,inproceedings,nguyen-etal-2021-crosslingual,"Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source language example of a different class. To address this issue, we propose a novel crosslingual alignment method that leverages class information of REE tasks for representation learning. In particular, we propose to learn two versions of representation vectors for each class in an REE task based on either source or target language examples. Representation vectors for corresponding classes will then be aligned to achieve class-aware alignment for crosslingual representations. In addition, we propose to further align representation vectors for language-universal word categories (i.e., parts of speech and dependency relations). As such, a novel filtering mechanism is presented to facilitate the learning of word category representations from contextualized representations on input texts based on adversarial learning. We conduct extensive crosslingual experiments with English, Chinese, and Arabic over REE tasks. The results demonstrate the benefits of the proposed method that significantly advances the state-of-the-art performance in these settings.","Online and Punta Cana, Dominican Republic","Nguyen, Minh Van  and
Nguyen, Tuan Ngo  and
Min, Bonan  and
Nguyen, Thien Huu",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.440,November,5414--5426,Association for Computational Linguistics,Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments,https://aclanthology.org/2021.emnlp-main.440,2021,,,,,
179,inproceedings,gor-etal-2021-toward,"The goal of question answering (QA) is to answer {\_}any{\_} question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, an analysis of model accuracy reveals little evidence that accuracy is lower for people based on gender or nationality; instead, there is more variation on professions (question topic) and question ambiguity. But QA{'}s lack of representation could itself hide evidence of bias, necessitating QA datasets that better represent global diversity.","Online and Punta Cana, Dominican Republic","Gor, Maharshi  and
Webster, Kellie  and
Boyd-Graber, Jordan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.444,November,5457--5473,Association for Computational Linguistics,Toward Deconfounding the Effect of Entity Demographics for Question Answering Accuracy,https://aclanthology.org/2021.emnlp-main.444,2021,,,,,
180,inproceedings,wang-etal-2021-controlled,"Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models{'} ability to learn grammatical dependencies in typologically different languages. Here we investigate this question in Mandarin Chinese, which has a logographic, largely syllable-based writing system; different word order; and sparser morphology than English. We train LSTMs, Recurrent Neural Network Grammars, Transformer language models, and Transformer-parameterized generative parsing models on two Mandarin Chinese datasets of different sizes. We evaluate the models{'} ability to learn different aspects of Mandarin grammar that assess syntactic and semantic relationships. We find suggestive evidence that structural supervision helps with representing syntactic state across intervening content and improves performance in low-data settings, suggesting that the benefits of hierarchical inductive biases in acquiring dependency relationships may extend beyond English.","Online and Punta Cana, Dominican Republic","Wang, Yiwen  and
Hu, Jennifer  and
Levy, Roger  and
Qian, Peng",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.454,November,5604--5620,Association for Computational Linguistics,Controlled Evaluation of Grammatical Knowledge in {M}andarin {C}hinese Language Models,https://aclanthology.org/2021.emnlp-main.454,2021,,,,,
181,inproceedings,wei-2021-good,"This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called {``}good-enough example extrapolation{''} (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than upsampling and other hidden-space data augmentation methods.","Online and Punta Cana, Dominican Republic","Wei, Jason",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.479,November,5923--5929,Association for Computational Linguistics,Good-Enough Example Extrapolation,https://aclanthology.org/2021.emnlp-main.479,2021,,,,,
182,inproceedings,yang-etal-2021-universal,"This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10{\%} improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.","Online and Punta Cana, Dominican Republic","Yang, Ziyi  and
Yang, Yinfei  and
Cer, Daniel  and
Law, Jax  and
Darve, Eric",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.502,November,6216--6228,Association for Computational Linguistics,Universal Sentence Representation Learning with Conditional Masked Language Model,https://aclanthology.org/2021.emnlp-main.502,2021,,,,,
183,inproceedings,kil-etal-2021-discovering,"Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples {---} there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the {``}unknowns{''} to the learned VQA model are indeed {``}known{''} in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the {``}how many{''} question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this {``}known{''} knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models{'} performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.","Online and Punta Cana, Dominican Republic","Kil, Jihyung  and
Zhang, Cheng  and
Xuan, Dong  and
Chao, Wei-Lun",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.512,November,6346--6361,Association for Computational Linguistics,Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering,https://aclanthology.org/2021.emnlp-main.512,2021,,,,,
184,inproceedings,luo-etal-2021-weakly,"Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models{'} performance. To address this issue, we collect a natural language knowledge base that can be used for any VQA system. Moreover, we propose a Visual Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever aims to retrieve relevant knowledge, and the visual reader seeks to predict answers based on given knowledge. We introduce various ways to retrieve knowledge using text and images and two reader styles: classification and extraction. Both the retriever and reader are trained with weak supervision. Our experimental results show that a good retriever can significantly improve the reader{'}s performance on the OK-VQA challenge. The code and corpus are provided in https://github.com/luomancs/retriever{\_}reader{\_}for{\_}okvqa.git.","Online and Punta Cana, Dominican Republic","Luo, Man  and
Zeng, Yankai  and
Banerjee, Pratyay  and
Baral, Chitta",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.517,November,6417--6431,Association for Computational Linguistics,Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering,https://aclanthology.org/2021.emnlp-main.517,2021,,,,,
185,inproceedings,yuan-2021-interactive,"Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good inductive biases, which can serve as an agent{'}s memory mechanism in iMRC tasks. We explore four different categories of graphs that can capture text information at various levels. We describe methods that dynamically build and update these graphs during information gathering, as well as neural models to encode graph representations in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.","Online and Punta Cana, Dominican Republic","Yuan, Xingdi",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.540,November,6734--6750,Association for Computational Linguistics,Interactive Machine Comprehension with Dynamic Knowledge Graphs,https://aclanthology.org/2021.emnlp-main.540,2021,,,,,
186,inproceedings,xu-etal-2021-videoclip,"We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.","Online and Punta Cana, Dominican Republic","Xu, Hu  and
Ghosh, Gargi  and
Huang, Po-Yao  and
Okhonko, Dmytro  and
Aghajanyan, Armen  and
Metze, Florian  and
Zettlemoyer, Luke  and
Feichtenhofer, Christoph",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.544,November,6787--6800,Association for Computational Linguistics,{V}ideo{CLIP}: Contrastive Pre-training for Zero-shot Video-Text Understanding,https://aclanthology.org/2021.emnlp-main.544,2021,,,,,
187,inproceedings,dua-etal-2021-generative,"Compositional reasoning tasks such as multi-hop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9{\%} higher than baseline) on adversarial held-out set which tests robustness of model{'}s multi-hop reasoning capabilities.","Online and Punta Cana, Dominican Republic","Dua, Dheeru  and
Nogueira dos Santos, Cicero  and
Ng, Patrick  and
Athiwaratkun, Ben  and
Xiang, Bing  and
Gardner, Matt  and
Singh, Sameer",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.561,November,7009--7015,Association for Computational Linguistics,Generative Context Pair Selection for Multi-hop Question Answering,https://aclanthology.org/2021.emnlp-main.561,2021,,,,,
188,inproceedings,oneill-etal-2021-wish,"Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of e-commerce reviews, and provides high quality professional annotations. We train CFD models using different text representation methods and classifiers. We find that these models are robust against the selectional biases introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying machine translation on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.","Online and Punta Cana, Dominican Republic","O{'}Neill, James  and
Rozenshtein, Polina  and
Kiryo, Ryuichi  and
Kubota, Motoko  and
Bollegala, Danushka",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.568,November,7092--7108,Association for Computational Linguistics,"{I} Wish {I} Would Have Loved This One, But {I} Didn{'}t {--} A Multilingual Dataset for Counterfactual Detection in Product Review",https://aclanthology.org/2021.emnlp-main.568,2021,,,,,
189,inproceedings,toney-caliskan-2021-valnorm,"Word embeddings learn implicit biases from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from social psychology. We apply ValNorm on static word embeddings from seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high accuracy in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a Pearson correlation of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in English. In contrast, we measure gender stereotypes using the same set of word embeddings and find that social biases vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.","Online and Punta Cana, Dominican Republic","Toney, Autumn  and
Caliskan, Aylin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.574,November,7203--7218,Association for Computational Linguistics,{V}al{N}orm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries,https://aclanthology.org/2021.emnlp-main.574,2021,,,,,
190,inproceedings,zhou-etal-2021-rica,"Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.","Online and Punta Cana, Dominican Republic","Zhou, Pei  and
Khanna, Rahul  and
Lee, Seyeon  and
Lin, Bill Yuchen  and
Ho, Daniel  and
Pujara, Jay  and
Ren, Xiang",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.598,November,7560--7579,Association for Computational Linguistics,{RICA}: Evaluating Robust Inference Capabilities Based on Commonsense Axioms,https://aclanthology.org/2021.emnlp-main.598,2021,,,,,
191,inproceedings,eisenschlos-etal-2021-mate,"This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20{\%} of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.","Online and Punta Cana, Dominican Republic","Eisenschlos, Julian  and
Gor, Maharshi  and
M{\""u}ller, Thomas  and
Cohen, William",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.600,November,7606--7619,Association for Computational Linguistics,{MATE}: Multi-view Attention for Table Transformer Efficiency,https://aclanthology.org/2021.emnlp-main.600,2021,,,,,
192,inproceedings,ahrens-etal-2021-bayesian,"Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.","Online and Punta Cana, Dominican Republic","Ahrens, Maximilian  and
Ashwin, Julian  and
Calliess, Jan-Peter  and
Nguyen, Vu",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.644,November,8162--8188,Association for Computational Linguistics,{B}ayesian Topic Regression for Causal Inference,https://aclanthology.org/2021.emnlp-main.644,2021,,,,,
193,inproceedings,sogaard-2021-lockes,"I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer \textit{What did Elizabeth want?} correctly in the context of {`}My kingdom for a cough drop, cried Queen Elizabeth.{'} Biased by co-occurrence statistics in the training data of pretrained language models, systems predict \textit{my kingdom}, rather than \textit{a cough drop}. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading.","Online and Punta Cana, Dominican Republic","S{\o}gaard, Anders",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.649,November,8240--8245,Association for Computational Linguistics,Locke{'}s Holiday: Belief Bias in Machine Reading,https://aclanthology.org/2021.emnlp-main.649,2021,,,,,
194,inproceedings,raganato-etal-2021-empirical,"Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs.","Online and Punta Cana, Dominican Republic","Raganato, Alessandro  and
V{\'a}zquez, Ra{\'u}l  and
Creutz, Mathias  and
Tiedemann, J{\""o}rg",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.664,November,8449--8456,Association for Computational Linguistics,An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation,https://aclanthology.org/2021.emnlp-main.664,2021,,,,,
195,inproceedings,emelin-sennrich-2021-wino,"Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established statistical methods and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.","Online and Punta Cana, Dominican Republic","Emelin, Denis  and
Sennrich, Rico",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.670,November,8517--8532,Association for Computational Linguistics,Wino-{X}: Multilingual {W}inograd Schemas for Commonsense Reasoning and Coreference Resolution,https://aclanthology.org/2021.emnlp-main.670,2021,,,,,
196,inproceedings,provilkov-malinin-2021-multi,"Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is dataset length-bias - NMT datasets are strongly biased towards short sentences. To mitigate this issue, we propose a new data augmentation technique {--} Multi-Sentence Resampling (MSR). This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets.","Online and Punta Cana, Dominican Republic","Provilkov, Ivan  and
Malinin, Andrey",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.677,November,8612--8621,Association for Computational Linguistics,Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation,https://aclanthology.org/2021.emnlp-main.677,2021,,,,,
197,inproceedings,buechel-etal-2021-towards,"Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068.","Online and Punta Cana, Dominican Republic","Buechel, Sven  and
Modersohn, Luise  and
Hahn, Udo",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.728,November,9231--9249,Association for Computational Linguistics,Towards Label-Agnostic Emotion Embeddings,https://aclanthology.org/2021.emnlp-main.728,2021,,,,,
198,inproceedings,hao-etal-2021-knowing,"Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out possible FN samples by heuristically leveraging the memory mechanism of deep neural networks. Then, it aligns those unlabeled data with the training data into a unified feature space by adversarial training to assign pseudo labels and further utilize the information contained in them. Experiments on two wildly-used benchmark datasets demonstrate the effectiveness of our approach.","Online and Punta Cana, Dominican Republic","Hao, Kailong  and
Yu, Botao  and
Hu, Wei",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.761,November,9661--9672,Association for Computational Linguistics,Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction,https://aclanthology.org/2021.emnlp-main.761,2021,,,,,
199,inproceedings,nan-etal-2021-uncovering,"Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues.","Online and Punta Cana, Dominican Republic","Nan, Guoshun  and
Zeng, Jiaqi  and
Qiao, Rui  and
Guo, Zhijiang  and
Lu, Wei",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.763,November,9683--9695,Association for Computational Linguistics,Uncovering Main Causalities for Long-tailed Information Extraction,https://aclanthology.org/2021.emnlp-main.763,2021,,,,,
200,inproceedings,zhao-etal-2021-relation,"The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the model to learn to cluster relational data, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our method reduces the error rate by 29.2{\%} and 15.7{\%}, on two datasets respectively, compared with current SOTA methods.","Online and Punta Cana, Dominican Republic","Zhao, Jun  and
Gui, Tao  and
Zhang, Qi  and
Zhou, Yaqian",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.765,November,9707--9718,Association for Computational Linguistics,A Relation-Oriented Clustering Method for Open Relation Extraction,https://aclanthology.org/2021.emnlp-main.765,2021,,,,,
201,inproceedings,wang-etal-2021-meta-distant,"With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.","Online and Punta Cana, Dominican Republic","Wang, Chengyu  and
Pan, Haojie  and
Qiu, Minghui  and
Huang, Jun  and
Yang, Fei  and
Zhang, Yin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.768,November,9742--9752,Association for Computational Linguistics,Meta Distant Transfer Learning for Pre-trained Language Models,https://aclanthology.org/2021.emnlp-main.768,2021,,,,,
202,inproceedings,du-etal-2021-assessing,"Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures{'} reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures","Online and Punta Cana, Dominican Republic","Du, Yupei  and
Fang, Qixiang  and
Nguyen, Dong",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.785,November,10012--10034,Association for Computational Linguistics,Assessing the Reliability of Word Embedding Gender Bias Measures,https://aclanthology.org/2021.emnlp-main.785,2021,,,,,
203,inproceedings,patel-pavlick-2021-stated,"People use language in subtle and nuanced ways to convey their beliefs. For instance, saying \textit{claimed} instead of \textit{said} casts doubt on the truthfulness of the underlying proposition, thus representing the author{'}s opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are sensitive to these linguistic framing effects. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find evidence that they lead to measurable style and topic differences in the generated text, leading to language that is, on average, more polarised and more skewed towards controversial entities and events.","Online and Punta Cana, Dominican Republic","Patel, Roma  and
Pavlick, Ellie",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.790,November,10080--10095,Association for Computational Linguistics,{``}Was it {``}stated{''} or was it {``}claimed{''}?: How linguistic bias affects generative language models,https://aclanthology.org/2021.emnlp-main.790,2021,,,,,
204,inproceedings,lian-etal-2021-effect,"Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during learning, instead of developing a more efficient or systematic language.","Online and Punta Cana, Dominican Republic","Lian, Yuchen  and
Bisazza, Arianna  and
Verhoef, Tessa",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.794,November,10121--10129,Association for Computational Linguistics,The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning,https://aclanthology.org/2021.emnlp-main.794,2021,,,,,
205,inproceedings,varab-schluter-2021-massivesumm,"Current research in automatic summarisation is unapologetically anglo-centered{--}a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance.","Online and Punta Cana, Dominican Republic","Varab, Daniel  and
Schluter, Natalie",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.797,November,10150--10161,Association for Computational Linguistics,"{M}assive{S}umm: a very large-scale, very multilingual, news summarisation dataset",https://aclanthology.org/2021.emnlp-main.797,2021,,,,,
206,inproceedings,liu-etal-2021-visually,"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.","Online and Punta Cana, Dominican Republic","Liu, Fangyu  and
Bugliarello, Emanuele  and
Ponti, Edoardo Maria  and
Reddy, Siva  and
Collier, Nigel  and
Elliott, Desmond",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.818,November,10467--10485,Association for Computational Linguistics,Visually Grounded Reasoning across Languages and Cultures,https://aclanthology.org/2021.emnlp-main.818,2021,,,,,
207,inproceedings,provatorova-etal-2021-robustness,"Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.","Online and Punta Cana, Dominican Republic","Provatorova, Vera  and
Bhargav, Samarth  and
Vakulenko, Svitlana  and
Kanoulas, Evangelos",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/2021.emnlp-main.820,November,10501--10510,Association for Computational Linguistics,Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing,https://aclanthology.org/2021.emnlp-main.820,2021,,,,,
208,inproceedings,vath-etal-2021-beyond,"On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise. Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the data sample level. As proof of concept, we perform a case study on four models. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they can not recognize text in images. Our metrics allow us to quantify which image and question embeddings provide most robustness to a model. All code s publicly available.","Online and Punta Cana, Dominican Republic","V{\""a}th, Dirk  and
Tilli, Pascal  and
Vu, Ngoc Thang",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/2021.emnlp-demo.14,November,114--123,Association for Computational Linguistics,Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking,https://aclanthology.org/2021.emnlp-demo.14,2021,,,,,
209,inproceedings,wang-etal-2021-fairseq,"This paper presents fairseq S{\^{}}2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, fairseq S{\^{}}2 also benefits from the scalability offered by fairseq and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models will be made available at https://github.com/pytorch/fairseq/tree/master/examples/speech{\_}synthesis.","Online and Punta Cana, Dominican Republic","Wang, Changhan  and
Hsu, Wei-Ning  and
Adi, Yossi  and
Polyak, Adam  and
Lee, Ann  and
Chen, Peng-Jen  and
Gu, Jiatao  and
Pino, Juan",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/2021.emnlp-demo.17,November,143--152,Association for Computational Linguistics,fairseq S{\^{}}2: A Scalable and Integrable Speech Synthesis Toolkit,https://aclanthology.org/2021.emnlp-demo.17,2021,,,,,
210,inproceedings,syed-etal-2021-summary,"This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The tool complements existing approaches for locally debugging summarization models and improves upon them. The tool is available at https://tldr.webis.de/","Online and Punta Cana, Dominican Republic","Syed, Shahbaz  and
Yousef, Tariq  and
Al Khatib, Khalid  and
J{\""a}nicke, Stefan  and
Potthast, Martin",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/2021.emnlp-demo.22,November,185--194,Association for Computational Linguistics,Summary Explorer: Visualizing the State of the Art in Text Summarization,https://aclanthology.org/2021.emnlp-demo.22,2021,,,,,
211,inproceedings,chheda-etal-2021-box,"A fundamental component to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with more geometric structure (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacity. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings. Fundamental geometric operations on boxes are implemented in a numerically stable way, as are modern approaches to training boxes which mitigate gradient sparsity. The library is fully open source, and compatible with both PyTorch and TensorFlow, which allows existing neural network layers to be replaced with or transformed into boxes easily. In this work, we present the implementation details of the fundamental components of the library, and the concepts required to use box representations alongside existing neural network architectures.","Online and Punta Cana, Dominican Republic","Chheda, Tejas  and
Goyal, Purujit  and
Tran, Trang  and
Patel, Dhruvesh  and
Boratko, Michael  and
Dasgupta, Shib Sankar  and
McCallum, Andrew",Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/2021.emnlp-demo.24,November,203--211,Association for Computational Linguistics,Box Embeddings: An open-source library for representation learning using geometric structures,https://aclanthology.org/2021.emnlp-demo.24,2021,,,,,
212,inproceedings,ghasemi-hiemstra-2021-bert,"Recently, various information retrieval models have been proposed based on pre-trained BERT models, achieving outstanding performance. The majority of such models have been tested on data collections with partial relevance labels, where various potentially relevant documents have not been exposed to the annotators. Therefore, evaluating BERT-based rankers may lead to biased and unfair evaluation results, simply because a relevant document has not been exposed to the annotators while creating the collection. In our work, we aim to better understand a BERT-based ranker{'}s strengths compared to a BERT-based re-ranker and the initial ranker. To this aim, we investigate BERT-based rankers performance on the Cranfield collection, which comes with full relevance judgment on all documents in the collection. Our results demonstrate the BERT-based full ranker{'}s effectiveness, as opposed to the BERT-based re-ranker and BM25. Also, analysis shows that there are documents that the BERT-based full-ranker finds that were not found by the initial ranker.",Online,"Ghasemi, Negin  and
Hiemstra, Djoerd",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/2021.eacl-srw.9,April,58--64,Association for Computational Linguistics,{BERT} meets Cranfield: Uncovering the Properties of Full Ranking on Fully Labeled Data,https://aclanthology.org/2021.eacl-srw.9,2021,,,,,
213,inproceedings,kedia-chinthakindi-2021-keep,"A common approach in many machine learning algorithms involves self-supervised learning on large unlabeled data before fine-tuning on downstream tasks to further improve performance. A new approach for language modelling, called dynamic evaluation, further fine-tunes a trained model during inference using trivially-present ground-truth labels, giving a large improvement in performance. However, this approach does not easily extend to classification tasks, where ground-truth labels are absent during inference. We propose to solve this issue by utilizing self-training and back-propagating the loss from the model{'}s own class-balanced predictions (pseudo-labels), adapting the Reptile algorithm from meta-learning, combined with an inductive bias towards pre-trained weights to improve generalization. Our method improves the performance of standard backbones such as BERT, Electra, and ResNet-50 on a wide variety of tasks, such as question answering on SQuAD and NewsQA, benchmark task SuperGLUE, conversation response selection on Ubuntu Dialog corpus v2.0, as well as image classification on MNIST and ImageNet without any changes to the underlying models. Our proposed method outperforms previous approaches, enables self-supervised fine-tuning during inference of any classifier model to better adapt to target domains, can be easily adapted to any model, and is also effective in online and transfer-learning settings.",Online,"Kedia, Akhil  and
Chinthakindi, Sai Chetan",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.6,April,63--77,Association for Computational Linguistics,Keep Learning: Self-supervised Meta-learning for Learning from Inference,https://aclanthology.org/2021.eacl-main.6,2021,,,,,
214,inproceedings,kaneko-bollegala-2021-dictionary,"Word embeddings trained on large corpora have shown to encode high levels of unfair discriminatory gender, racial, religious and ethnic biases. In contrast, human-written dictionaries describe the meanings of words in a concise, objective and an unbiased manner. We propose a method for debiasing pre-trained word embeddings using dictionaries, without requiring access to the original training resources or any knowledge regarding the word embedding algorithms used. Unlike prior work, our proposed method does not require the types of biases to be pre-defined in the form of word lists, and learns the constraints that must be satisfied by unbiased word embeddings automatically from dictionary definitions of the words. Specifically, we learn an encoder to generate a debiased version of an input word embedding such that it (a) retains the semantics of the pre-trained word embedding, (b) agrees with the unbiased definition of the word according to the dictionary, and (c) remains orthogonal to the vector space spanned by any biased basis vectors in the pre-trained word embedding space. Experimental results on standard benchmark datasets show that the proposed method can accurately remove unfair biases encoded in pre-trained word embeddings, while preserving useful semantics.",Online,"Kaneko, Masahiro  and
Bollegala, Danushka",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.16,April,212--223,Association for Computational Linguistics,Dictionary-based Debiasing of Pre-trained Word Embeddings,https://aclanthology.org/2021.eacl-main.16,2021,,,,,
215,inproceedings,wiegand-etal-2021-implicitly,We examine the task of detecting implicitly abusive comparisons (e.g. {``}Your hair looks like you have been electrocuted{''}). Implicitly abusive comparisons are abusive comparisons in which abusive words (e.g. {``}dumbass{''} or {``}scum{''}) are absent. We detail the process of creating a novel dataset for this task via crowdsourcing that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present classification experiments that include a range of linguistic features that help us better understand the mechanisms underlying abusive comparisons.,Online,"Wiegand, Michael  and
Geulig, Maja  and
Ruppenhofer, Josef",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.27,April,358--368,Association for Computational Linguistics,Implicitly Abusive Comparisons {--} A New Dataset and Linguistic Analysis,https://aclanthology.org/2021.eacl-main.27,2021,,,,,
216,inproceedings,lin-su-2021-fast,"This paper empirically studies whether BERT can really learn to conduct natural language inference (NLI) without utilizing hidden dataset bias; and how efficiently it can learn if it could. This is done via creating a simple entailment judgment case which involves only binary predicates in plain English. The results show that the learning process of BERT is very slow. However, the efficiency of learning can be greatly improved (data reduction by a factor of 1,500) if task-related features are added. This suggests that domain knowledge greatly helps when conducting NLI with neural networks.",Online,"Lin, Yi-Chung  and
Su, Keh-Yih",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.51,April,626--633,Association for Computational Linguistics,How Fast can {BERT} Learn Simple Natural Language Inference?,https://aclanthology.org/2021.eacl-main.51,2021,,,,,
217,inproceedings,thorne-vlachos-2021-elastic,"The biases present in training datasets have been shown to affect models for sentence pair classification tasks such as natural language inference (NLI) and fact verification. While fine-tuning models on additional data has been used to mitigate them, a common issue is that of catastrophic forgetting of the original training dataset. In this paper, we show that elastic weight consolidation (EWC) allows fine-tuning of models to mitigate biases while being less susceptible to catastrophic forgetting. In our evaluation on fact verification and NLI stress tests, we show that fine-tuning with EWC dominates standard fine-tuning, yielding models with lower levels of forgetting on the original (biased) dataset for equivalent gains in accuracy on the fine-tuning (unbiased) dataset.",Online,"Thorne, James  and
Vlachos, Andreas",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.82,April,957--964,Association for Computational Linguistics,Elastic weight consolidation for better bias inoculation,https://aclanthology.org/2021.eacl-main.82,2021,,,,,
218,inproceedings,kaneko-bollegala-2021-debiasing,"In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",Online,"Kaneko, Masahiro  and
Bollegala, Danushka",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.107,April,1256--1266,Association for Computational Linguistics,Debiasing Pre-trained Contextualised Embeddings,https://aclanthology.org/2021.eacl-main.107,2021,,,,,
219,inproceedings,ren-etal-2021-cross,"There has been much interest in rumor detection using deep learning models in recent years. A well-known limitation of deep learning models is that they tend to learn superficial patterns, which restricts their generalization ability. We find that this is also true for cross-topic rumor detection. In this paper, we propose a method inspired by the {``}mixture of experts{''} paradigm. We assume that the prediction of the rumor class label given an instance is dependent on the topic distribution of the instance. After deriving a vector representation for each topic, given an instance, we derive a {``}topic mixture{''} vector for the instance based on its topic distribution. This topic mixture is combined with the vector representation of the instance itself to make rumor predictions. Our experiments show that our proposed method can outperform two baseline debiasing methods in a cross-topic setting. In a synthetic setting when we removed topic-specific words, our method also works better than the baselines, showing that our method does not rely on superficial features.",Online,"Ren, Xiaoying  and
Jiang, Jing  and
Serena Khoo, Ling Min  and
Chieu, Hai Leong",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.131,April,1534--1538,Association for Computational Linguistics,Cross-Topic Rumor Detection using Topic-Mixtures,https://aclanthology.org/2021.eacl-main.131,2021,,,,,
220,inproceedings,patel-caragea-2021-exploiting,"Keyphrases associated with research papers provide an effective way to find useful information in the large and growing scholarly digital collections. In this paper, we present KPRank, an unsupervised graph-based algorithm for keyphrase extraction that exploits both positional information and contextual word embeddings into a biased PageRank. Our experimental results on five benchmark datasets show that KPRank that uses contextual word embeddings with additional position signal outperforms previous approaches and strong baselines for this task.",Online,"Patel, Krutarth  and
Caragea, Cornelia",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.136,April,1585--1591,Association for Computational Linguistics,Exploiting Position and Contextual Word Embeddings for Keyphrase Extraction from Scientific Papers,https://aclanthology.org/2021.eacl-main.136,2021,,,,,
221,inproceedings,sogaard-etal-2021-need,"(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.",Online,"S{\o}gaard, Anders  and
Ebert, Sebastian  and
Bastings, Jasmijn  and
Filippova, Katja",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.156,April,1823--1832,Association for Computational Linguistics,We Need To Talk About Random Splits,https://aclanthology.org/2021.eacl-main.156,2021,,,,,
222,inproceedings,ive-etal-2021-exploring,"Reinforcement Learning (RL) is a powerful framework to address the discrepancy between loss functions used during training and the final evaluation metrics to be used at test time. When applied to neural Machine Translation (MT), it minimises the mismatch between the cross-entropy loss and non-differentiable evaluation metrics like BLEU. However, the suitability of these metrics as reward function at training time is questionable: they tend to be sparse and biased towards the specific words used in the reference texts. We propose to address this problem by making models less reliant on such metrics in two ways: (a) with an entropy-regularised RL method that does not only maximise a reward function but also explore the action space to avoid peaky distributions; (b) with a novel RL method that explores a dynamic unsupervised reward function to balance between exploration and exploitation. We base our proposals on the Soft Actor-Critic (SAC) framework, adapting the off-policy maximum entropy model for language generation applications such as MT. We demonstrate that SAC with BLEU reward tends to overfit less to the training data and performs better on out-of-domain data. We also show that our dynamic unsupervised reward can lead to better translation of ambiguous words.",Online,"Ive, Julia  and
Wang, Zixu  and
Fomicheva, Marina  and
Specia, Lucia",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.164,April,1908--1920,Association for Computational Linguistics,Exploring Supervised and Unsupervised Rewards in Machine Translation,https://aclanthology.org/2021.eacl-main.164,2021,,,,,
223,inproceedings,hirmer-etal-2021-building,"Most well-established data collection methods currently adopted in NLP depend on the as- sumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.",Online,"Hirmer, Stephanie  and
Leonard, Alycia  and
Tumwesige, Josephine  and
Conforti, Costanza",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.186,April,2176--2189,Association for Computational Linguistics,Building Representative Corpora from Illiterate Communities: A Reviewof Challenges and Mitigation Strategies for Developing Countries,https://aclanthology.org/2021.eacl-main.186,2021,,,,,
224,inproceedings,vanmassenhove-etal-2021-machine,"Recent studies in the field of Machine Translation (MT) and Natural Language Processing (NLP) have shown that existing models amplify biases observed in the training data. The amplification of biases in language technology has mainly been examined with respect to specific phenomena, such as gender bias. In this work, we go beyond the study of gender in MT and investigate how bias amplification might affect language in a broader sense. We hypothesize that the {`}algorithmic bias{'}, i.e. an exacerbation of frequently observed patterns in combination with a loss of less frequent ones, not only exacerbates societal biases present in current datasets but could also lead to an artificially impoverished language: {`}machine translationese{'}. We assess the linguistic richness (on a lexical and morphological level) of translations created by different data-driven MT paradigms {--} phrase-based statistical (PB-SMT) and neural MT (NMT). Our experiments show that there is a loss of lexical and syntactic richness in the translations produced by all investigated MT paradigms for two language pairs (EN-FR and EN-ES).",Online,"Vanmassenhove, Eva  and
Shterionov, Dimitar  and
Gwilliam, Matthew",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.188,April,2203--2213,Association for Computational Linguistics,Machine Translationese: Effects of Algorithmic Bias on Linguistic Complexity in Machine Translation,https://aclanthology.org/2021.eacl-main.188,2021,,,,,
225,inproceedings,de-vassimon-manela-etal-2021-stereotype,"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.",Online,"de Vassimon Manela, Daniel  and
Errington, David  and
Fisher, Thomas  and
van Breugel, Boris  and
Minervini, Pasquale",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.190,April,2232--2242,Association for Computational Linguistics,Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models,https://aclanthology.org/2021.eacl-main.190,2021,,,,,
226,inproceedings,zhou-etal-2021-hidden,"Automatic unreliable news detection is a research problem with great potential impact. Recently, several papers have shown promising results on large-scale news datasets with models that only use the article itself without resorting to any fact-checking mechanism or retrieving any supporting evidence. In this work, we take a closer look at these datasets. While they all provide valuable resources for future research, we observe a number of problems that may lead to results that do not generalize in more realistic settings. Specifically, we show that selection bias during data collection leads to undesired artifacts in the datasets. In addition, while most systems train and predict at the level of individual articles, overlapping article sources in the training and evaluation data can provide a strong confounding factor that models can exploit. In the presence of this confounding factor, the models can achieve good performance by directly memorizing the site-label mapping instead of modeling the real task of unreliable news detection. We observed a significant drop ({\textgreater}10{\%}) in accuracy for all models tested in a clean split with no train/test source overlap. Using the observations and experimental results, we provide practical suggestions on how to create more reliable datasets for the unreliable news detection task. We suggest future dataset creation include a simple model as a difficulty/bias probe and future model development use a clean non-overlapping site and date split.",Online,"Zhou, Xiang  and
Elfardy, Heba  and
Christodoulopoulos, Christos  and
Butler, Thomas  and
Bansal, Mohit",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.211,April,2482--2492,Association for Computational Linguistics,Hidden Biases in Unreliable News Detection Datasets,https://aclanthology.org/2021.eacl-main.211,2021,,,,,
227,inproceedings,balachandran-etal-2021-structsum,"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.",Online,"Balachandran, Vidhisha  and
Pagnoni, Artidoro  and
Lee, Jay Yoon  and
Rajagopal, Dheeraj  and
Carbonell, Jaime  and
Tsvetkov, Yulia",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.220,April,2575--2585,Association for Computational Linguistics,{S}truct{S}um: Summarization via Structured Representations,https://aclanthology.org/2021.eacl-main.220,2021,,,,,
228,inproceedings,hede-etal-2021-toxicity,"The ability to quantify incivility online, in news and in congressional debates is of great interest to political scientists. Computational tools for detecting online incivility for English are now fairly accessible and potentially could be applied more broadly. We test the Jigsaw Perspective API for its ability to detect the degree of incivility on a corpus that we developed, consisting of manual annotations of civility in American news. We demonstrate that toxicity models, as exemplified by Perspective, are inadequate for the analysis of incivility in news. We carry out error analysis that points to the need to develop methods to remove spurious correlations between words often mentioned in the news, especially identity descriptors and incivility. Without such improvements, applying Perspective or similar models on news is likely to lead to wrong conclusions, that are not aligned with the human perception of incivility.",Online,"Hede, Anushree  and
Agarwal, Oshin  and
Lu, Linda  and
Mutz, Diana C.  and
Nenkova, Ani",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.225,April,2620--2630,Association for Computational Linguistics,From Toxicity in Online Comments to Incivility in {A}merican News: Proceed with Caution,https://aclanthology.org/2021.eacl-main.225,2021,,,,,
229,inproceedings,arthur-etal-2021-learning,"We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality quality while keeping the delay low.",Online,"Arthur, Philip  and
Cohn, Trevor  and
Haffari, Gholamreza",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.233,April,2709--2719,Association for Computational Linguistics,Learning Coupled Policies for Simultaneous Machine Translation using Imitation Learning,https://aclanthology.org/2021.eacl-main.233,2021,,,,,
230,inproceedings,han-etal-2021-diverse,"Adversarial learning can learn fairer and less biased models of language processing than standard training. However, current adversarial techniques only partially mitigate the problem of model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and stability of training.",Online,"Han, Xudong  and
Baldwin, Timothy  and
Cohn, Trevor",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.239,April,2760--2765,Association for Computational Linguistics,Diverse Adversaries for Mitigating Bias in Training,https://aclanthology.org/2021.eacl-main.239,2021,,,,,
231,inproceedings,jambor-etal-2021-exploring,"Real-world knowledge graphs are often characterized by low-frequency relations{---}a challenge that has prompted an increasing interest in few-shot link prediction methods. These methods perform link prediction for a set of new relations, unseen during training, given only a few example facts of each relation at test time. In this work, we perform a systematic study on a spectrum of models derived by generalizing the current state of the art for few-shot link prediction, with the goal of probing the limits of learning in this few-shot setting. We find that a simple, zero-shot baseline {---} which ignores any relation-specific information {---} achieves surprisingly strong performance. Moreover, experiments on carefully crafted synthetic datasets show that having only a few examples of a relation fundamentally limits models from using fine-grained structural information and only allows for exploiting the coarse-grained positional information of entities. Together, our findings challenge the implicit assumptions and inductive biases of prior work and highlight new directions for research in this area.",Online,"Jambor, Dora  and
Teru, Komal  and
Pineau, Joelle  and
Hamilton, William L.",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.245,April,2816--2822,Association for Computational Linguistics,Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs,https://aclanthology.org/2021.eacl-main.245,2021,,,,,
232,inproceedings,aguirre-etal-2021-gender,"Multiple studies have demonstrated that behaviors expressed on online social media platforms can indicate the mental health state of an individual. The widespread availability of such data has spurred interest in mental health research, using several datasets where individuals are labeled with mental health conditions. While previous research has raised concerns about possible biases in models produced from this data, no study has investigated how these biases manifest themselves with regards to demographic groups in data, such as gender and racial/ethnic groups. Here, we analyze the fairness of depression classifiers trained on Twitter data with respect to gender and racial demographic groups. We find that model performance differs for underrepresented groups, and we investigate sources of these biases beyond data representation. Our study results in recommendations on how to avoid these biases in future research.",Online,"Aguirre, Carlos  and
Harrigian, Keith  and
Dredze, Mark",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.256,April,2932--2949,Association for Computational Linguistics,Gender and Racial Fairness in Depression Research using Social Media,https://aclanthology.org/2021.eacl-main.256,2021,,,,,
233,inproceedings,dagan-etal-2021-co,"Referential games offer a grounded learning environment for neural agents which accounts for the fact that language is functionally used to communicate. However, they do not take into account a second constraint considered to be fundamental for the shape of human language: that it must be learnable by new language learners. Cogswell et al. (2019) introduced cultural transmission within referential games through a changing population of agents to constrain the emerging language to be learnable. However, the resulting languages remain inherently biased by the agents{'} underlying capabilities. In this work, we introduce Language Transmission Simulator to model both cultural and architectural evolution in a population of agents. As our core contribution, we empirically show that the optimal situation is to take into account also the learning biases of the language learners and thus let language and agents co-evolve. When we allow the agent population to evolve through architectural evolution, we achieve across the board improvements on all considered metrics and surpass the gains made with cultural transmission. These results stress the importance of studying the underlying agent architecture and pave the way to investigate the co-evolution of language and agent in language emergence studies.",Online,"Dagan, Gautier  and
Hupkes, Dieuwke  and
Bruni, Elia",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.260,April,2993--3004,Association for Computational Linguistics,Co-evolution of language and agents in referential games,https://aclanthology.org/2021.eacl-main.260,2021,,,,,
234,inproceedings,zhou-etal-2021-challenges,"Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.",Online,"Zhou, Xuhui  and
Sap, Maarten  and
Swayamdipta, Swabha  and
Choi, Yejin  and
Smith, Noah",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.274,April,3143--3155,Association for Computational Linguistics,Challenges in Automated Debiasing for Toxic Language Detection,https://aclanthology.org/2021.eacl-main.274,2021,,,,,
235,inproceedings,kassner-etal-2021-multilingual,"Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as {``}Paris is the capital of [MASK]{''} are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT{'}s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.",Online,"Kassner, Nora  and
Dufter, Philipp  and
Sch{\""u}tze, Hinrich",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.284,April,3250--3258,Association for Computational Linguistics,Multilingual {LAMA}: Investigating Knowledge in Multilingual Pretrained Language Models,https://aclanthology.org/2021.eacl-main.284,2021,,,,,
236,inproceedings,chalkidis-etal-2021-regulatory,"Major scandals in corporate history have urged the need for regulatory compliance, where organizations need to ensure that their controls (processes) comply with relevant laws, regulations, and policies. However, keeping track of the constantly changing legislation is difficult, thus organizations are increasingly adopting Regulatory Technology (RegTech) to facilitate the process. To this end, we introduce regulatory information retrieval (REG-IR), an application of document-to-document information retrieval (DOC2DOC IR), where the query is an entire document making the task more challenging than traditional IR where the queries are short. Furthermore, we compile and release two datasets based on the relationships between EU directives and UK legislation. We experiment on these datasets using a typical two-step pipeline approach comprising a pre-fetcher and a neural re-ranker. Experimenting with various pre-fetchers from BM25 to k nearest neighbors over representations from several BERT models, we show that fine-tuning a BERT model on an in-domain classification task produces the best representations for IR. We also show that neural re-rankers under-perform due to contradicting supervision, i.e., similar query-document pairs with opposite labels. Thus, they are biased towards the pre-fetcher{'}s score. Interestingly, applying a date filter further improves the performance, showcasing the importance of the time dimension.",Online,"Chalkidis, Ilias  and
Fergadiotis, Manos  and
Manginas, Nikolaos  and
Katakalou, Eva  and
Malakasiotis, Prodromos",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,10.18653/v1/2021.eacl-main.305,April,3498--3511,Association for Computational Linguistics,Regulatory Compliance through {D}oc2{D}oc Information Retrieval: A case study in {EU}/{UK} legislation where text similarity has limitations,https://aclanthology.org/2021.eacl-main.305,2021,,,,,
237,inproceedings,lu-etal-2021-sf,"Although open-domain question answering (QA) draws great attention in recent years, it requires large amounts of resources for building the full system and it is often difficult to reproduce previous results due to complex configurations. In this paper, we introduce SF-QA: simple and fair evaluation framework for open-domain QA. SF-QA framework modularizes the pipeline open-domain QA system, which makes the task itself easily accessible and reproducible to research groups without enough computing resources. The proposed evaluation framework is publicly available and anyone can contribute to the code and evaluations.",Online,"Lu, Xiaopeng  and
Lee, Kyusong  and
Zhao, Tiancheng",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,10.18653/v1/2021.eacl-demos.2,April,7--13,Association for Computational Linguistics,{SF}-{QA}: Simple and Fair Evaluation Library for Open-domain Question Answering,https://aclanthology.org/2021.eacl-demos.2,2021,,,,,
238,inproceedings,friedrich-etal-2021-debie,"Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE{'}s functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de.",Online,"Friedrich, Niklas  and
Lauscher, Anne  and
Ponzetto, Simone Paolo  and
Glava{\v{s}}, Goran",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,10.18653/v1/2021.eacl-demos.11,April,91--98,Association for Computational Linguistics,{D}eb{IE}: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces,https://aclanthology.org/2021.eacl-demos.11,2021,,,,,
239,inproceedings,terragni-etal-2021-octis,"In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: https://github.com/MIND-Lab/OCTIS.",Online,"Terragni, Silvia  and
Fersini, Elisabetta  and
Galuzzi, Bruno Giovanni  and
Tropeano, Pietro  and
Candelieri, Antonio",Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,10.18653/v1/2021.eacl-demos.31,April,263--270,Association for Computational Linguistics,{OCTIS}: Comparing and Optimizing Topic models is Simple!,https://aclanthology.org/2021.eacl-demos.31,2021,,,,,
240,inproceedings,srivastava-etal-2021-berts,"Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.",Online,"Srivastava, Vivek  and
Pilli, Stephen  and
Bhat, Savita  and
Pedanekar, Niranjan  and
Karande, Shirish",Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,10.18653/v1/2021.deelio-1.12,June,119--128,Association for Computational Linguistics,What {BERT}s and {GPT}s know about your brand? Probing contextual language models for affect associations,https://aclanthology.org/2021.deelio-1.12,2021,,,,,
241,inproceedings,zhao-etal-2021-pretrained,"Pretrained transformer-based language models achieve state-of-the-art performance in many NLP tasks, but it is an open question whether the knowledge acquired by the models during pretraining resembles the linguistic knowledge of humans. We present both humans and pretrained transformers with descriptions of events, and measure their preference for telic interpretations (the event has a natural endpoint) or atelic interpretations (the event does not have a natural endpoint). To measure these preferences and determine what factors influence them, we design an English test and a novel-word test that include a variety of linguistic cues (noun phrase quantity, resultative structure, contextual information, temporal units) that bias toward certain interpretations. We find that humans{'} choice of telicity interpretation is reliably influenced by theoretically-motivated cues, transformer models (BERT and RoBERTa) are influenced by some (though not all) of the cues, and transformer models often rely more heavily on temporal units than humans do.",Online,"Zhao, Yiyun  and
Ngui, Jian Gang  and
Hall Hartley, Lucy  and
Bethard, Steven",Proceedings of the 25th Conference on Computational Natural Language Learning,10.18653/v1/2021.conll-1.6,November,72--81,Association for Computational Linguistics,Do pretrained transformers infer telicity like humans?,https://aclanthology.org/2021.conll-1.6,2021,,,,,
242,inproceedings,portelance-etal-2021-emergence,"By the age of two, children tend to assume that new word categories are based on objects{'} shape, rather than their color or texture; this assumption is called the shape bias. They are thought to learn this bias by observing that their caregiver{'}s language is biased towards shape based categories. This presents a chicken and egg problem: if the shape bias must be present in the language in order for children to learn it, how did it arise in language in the first place? In this paper, we propose that communicative efficiency explains both how the shape bias emerged and why it persists across generations. We model this process with neural emergent language agents that learn to communicate about raw pixelated images. First, we show that the shape bias emerges as a result of efficient communication strategies employed by agents. Second, we show that pressure brought on by communicative need is also necessary for it to persist across generations; simply having a shape bias in an agent{'}s input language is insufficient. These results suggest that, over and above the operation of other learning strategies, the shape bias in human learners may emerge and be sustained by communicative pressures.",Online,"Portelance, Eva  and
Frank, Michael C.  and
Jurafsky, Dan  and
Sordoni, Alessandro  and
Laroche, Romain",Proceedings of the 25th Conference on Computational Natural Language Learning,10.18653/v1/2021.conll-1.48,November,607--623,Association for Computational Linguistics,The Emergence of the Shape Bias Results from Communicative Efficiency,https://aclanthology.org/2021.conll-1.48,2021,,,,,
243,inproceedings,li-etal-2021-investigating,"We investigate linguistic markers associated with schizophrenia in clinical conversations by detecting predictive features among French-speaking patients. Dealing with human-human dialogues makes for a realistic situation, but it calls for strategies to represent the context and face data sparsity. We compare different approaches for data representation {--} from individual speech turns to entire conversations {--}, and data modeling, using lexical, morphological, syntactic, and discourse features, dimensions presumed to be tightly connected to the language of schizophrenia. Previous English models were mostly lexical and reached high performance, here replicated (93.7{\%} acc.). However, our analysis reveals that these models are heavily biased, which probably concerns most datasets on this task. Our new delexicalized models are more general and robust, with the best accuracy score at 77.9{\%}.","Punta Cana, Dominican Republic and Online","Li, Chuyuan  and
Amblard, Maxime  and
Braud, Chlo{\'e}  and
Demily, Caroline  and
Franck, Nicolas  and
Musiol, Michel",Proceedings of the 2nd Workshop on Computational Approaches to Discourse,10.18653/v1/2021.codi-main.3,November,20--28,Association for Computational Linguistics,Investigating non lexical markers of the language of schizophrenia in spontaneous conversations,https://aclanthology.org/2021.codi-main.3,2021,,,,,
244,inproceedings,logacev-dokudan-2021-multinomial,"In the field of sentence processing, speakers{'} preferred interpretation of ambiguous sentences are often determined using a variant of a discrete choice task, in which participants are asked to indicate their preferred meaning of an ambiguous sentence. We discuss participants{'} degree of attentiveness as a potential source of bias and variability in such tasks. We show that it may distort the estimates of the preference of a particular interpretation obtained in such experiments and may thus complicate the interpretation of the results as well as the comparison of the results of several experiments. We propose an analysis method based on multinomial processing tree models (Batchelder and Riefer, 1999) which can correct for this bias and allows for a separation of parameters of theoretical importance from nuisance parameters. We test two variants of the MPT-based model on experimental data from English and Turkish and demonstrate that our method can provide deeper insight into the processes underlying participants{'} answering behavior and their interpretation preferences than an analysis based on raw percentages.",Online,"Logacev, Pavel  and
Dokudan, Noyan",Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,10.18653/v1/2021.cmcl-1.4,June,39--47,Association for Computational Linguistics,A Multinomial Processing Tree Model of {RC} Attachment,https://aclanthology.org/2021.cmcl-1.4,2021,,,,,
245,inproceedings,gamoran-etal-2021-using,"This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theory-driven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory-guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over-fitting. The models provided reasonable classification accuracy on unseen test data (0.68{\textless}=AUC{\textless}= 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.",Online,"Gamoran, Avi  and
Kaplan, Yonatan  and
Simchon, Almog  and
Gilead, Michael",Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,10.18653/v1/2021.clpsych-1.12,June,103--109,Association for Computational Linguistics,Using Psychologically-Informed Priors for Suicide Prediction in the {CLP}sych 2021 Shared Task,https://aclanthology.org/2021.clpsych-1.12,2021,,,,,
246,inproceedings,hitczenko-etal-2021-automated,"Thought disorder {--} linguistic disturbances including incoherence and derailment of topic {--} is seen in individuals both with and at risk for psychosis. Methods from computational linguistics have increasingly sought to quantify thought disorder to detect group differences between clinical populations and healthy controls. While previous work has been quite successful at these classification tasks, the lack of interpretability of the computational metrics has made it unclear whether they are in fact measuring thought disorder. In this paper, we dive into these measures to try to better understand what they reflect. While we find group differences between at-risk and healthy control populations, we also find that the measures mostly do not correlate with existing measures of thought disorder symptoms (what they are intended to measure), but rather correlate with surface properties of the speech (e.g., sentence length) and sociodemographic properties of the speaker (e.g., race). These results highlight the importance of considering interpretability and front and center as the field continues to grow. Ethical use of computational measures like those studied here {--} especially in the high-stakes context of clinical care {--} requires us to devote substantial attention to potential biases in our measures.",Online,"Hitczenko, Kasia  and
Cowan, Henry  and
Mittal, Vijay  and
Goldrick, Matthew",Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,10.18653/v1/2021.clpsych-1.16,June,129--150,Association for Computational Linguistics,Automated coherence measures fail to index thought disorder in individuals at risk for psychosis,https://aclanthology.org/2021.clpsych-1.16,2021,,,,,
247,inproceedings,aguirre-dredze-2021-qualitative,"Models for identifying depression using social media text exhibit biases towards different gender and racial/ethnic groups. Factors like representation and balance of groups within the dataset are contributory factors, but difference in content and social media use may further explain these biases. We present an analysis of the content of social media posts from different demographic groups. Our analysis shows that there are content differences between depression and control subgroups across demographic groups, and that temporal topics and demographic-specific topics are correlated with downstream depression model error. We discuss the implications of our work on creating future datasets, as well as designing and training models for mental health.",Online,"Aguirre, Carlos  and
Dredze, Mark",Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,10.18653/v1/2021.clpsych-1.19,June,169--180,Association for Computational Linguistics,Qualitative Analysis of Depression Models by Demographics,https://aclanthology.org/2021.clpsych-1.19,2021,,,,,
248,inproceedings,chandler-etal-2021-safeguarding,"A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: (1) the determination of cases in which models are unstable by means of attribute and class based outlier detection and (2) finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.",Online,"Chandler, Chelsea  and
Foltz, Peter  and
Cohen, Alex  and
Holmlund, Terje  and
Elvev{\aa}g, Brita",Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,10.18653/v1/2021.clpsych-1.20,June,181--191,Association for Computational Linguistics,Safeguarding against spurious {AI}-based predictions: The case of automated verbal memory assessment,https://aclanthology.org/2021.clpsych-1.20,2021,,,,,
249,inproceedings,sherman-etal-2021-towards,"Spurred by advances in machine learning and natural language processing, developing social media-based mental health surveillance models has received substantial recent attention. For these models to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of protected characteristics. In this paper we study the relationship between user demographics {--} focusing on gender {--} and depression. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domain-informed classifiers. We then study our models{'} parameters to gain qualitative insight into the differences in posting behavior across genders.",Online,"Sherman, Eli  and
Harrigian, Keith  and
Aguirre, Carlos  and
Dredze, Mark",Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,10.18653/v1/2021.clpsych-1.23,June,217--223,Association for Computational Linguistics,Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models,https://aclanthology.org/2021.clpsych-1.23,2021,,,,,
250,article,cao-daume-iii-2021-toward,"Abstract Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systematic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and investigate where in the machine learning pipeline such biases can enter a coreference resolution system. We inspect many existing data sets for trans-exclusionary biases, and develop two new data sets for interrogating bias in both crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we will build systems that fail for: quality of service, stereotyping, and over- or under-representation, especially for binary and non-binary trans users.","Cambridge, MA","Cao, Yang Trista  and
Daum{\'e} III, Hal",,10.1162/coli_a_00413,November,615--661,MIT Press,Toward Gender-Inclusive Coreference Resolution: An Analysis of Gender and Bias Throughout the Machine Learning Lifecycle*,https://aclanthology.org/2021.cl-3.19,2021,Computational Linguistics,47,3,,
251,article,feder-etal-2021-causalm,"Abstract Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning{--}based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1","Cambridge, MA","Feder, Amir  and
Oved, Nadav  and
Shalit, Uri  and
Reichart, Roi",,10.1162/coli_a_00404,June,333--386,MIT Press,{C}ausa{LM}: Causal Model Explanation Through Counterfactual Language Models,https://aclanthology.org/2021.cl-2.13,2021,Computational Linguistics,47,2,,
252,article,loureiro-etal-2021-analysis,"Abstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.","Cambridge, MA","Loureiro, Daniel  and
Rezaee, Kiamehr  and
Pilehvar, Mohammad Taher  and
Camacho-Collados, Jose",,10.1162/coli_a_00405,June,387--443,MIT Press,Analysis and Evaluation of Language Models for Word Sense Disambiguation,https://aclanthology.org/2021.cl-2.14,2021,Computational Linguistics,47,2,,
253,inproceedings,wang-etal-2021-enhancing,"Recent work has raised concerns on the risk of spurious correlations and unintended biases in statistical machine learning models that threaten model robustness and fairness. In this paper, we propose a simple and intuitive regularization approach to integrate causal knowledge during model training and build a robust and fair model by emphasizing causal features and de-emphasizing spurious features. Specifically, we first manually identify causal and spurious features with principles inspired from the counterfactual framework of causal inference. Then, we propose a regularization approach to penalize causal and spurious features separately. By adjusting the strength of the penalty for each type of feature, we build a predictive model that relies more on causal features and less on non-causal features. We conduct experiments to evaluate model robustness and fairness on three datasets with multiple metrics. Empirical results show that the new models built with causal awareness significantly improve model robustness with respect to counterfactual texts and model fairness with respect to sensitive attributes.","Punta Cana, Dominican Republic","Wang, Zhao  and
Shu, Kai  and
Culotta, Aron",Proceedings of the First Workshop on Causal Inference and NLP,10.18653/v1/2021.cinlp-1.3,November,33--43,Association for Computational Linguistics,Enhancing Model Robustness and Fairness with Causality: A Regularization Approach,https://aclanthology.org/2021.cinlp-1.3,2021,,,,,
254,inproceedings,xuewen-etal-2021-reducing,{``}Neural machine translation (NMT) usually employs beam search to expand the searching spaceand obtain more translation candidates. However the increase of the beam size often suffersfrom plenty of short translations resulting in dramatical decrease in translation quality. In this paper we handle the length bias problem through a perspective of causal inference. Specially we regard the model generated translation score S as a degraded true translation quality affectedby some noise and one of the confounders is the translation length. We apply a Half-Sibling Re-gression method to remove the length effect on S and then we can obtain a debiased translation score without length information. The proposed method is model agnostic and unsupervised which is adaptive to any NMT model and test dataset. We conduct the experiments on three translation tasks with different scales of datasets. Experimental results and further analyses showthat our approaches gain comparable performance with the empirical baseline methods.{''},"Huhhot, China","Xuewen, Shi  and
Heyan, Huang  and
Ping, Jian  and
Yi-Kun, Tang",Proceedings of the 20th Chinese National Conference on Computational Linguistics,,August,874--885,Chinese Information Processing Society of China,Reducing Length Bias in Scoring Neural Machine Translation via a Causal Inference Method,https://aclanthology.org/2021.ccl-1.78,2021,,,,English,
255,inproceedings,xiaoning-etal-2021-low,{``}Reinforcement learning has been proved to be effective in handling low resource machine trans-lation tasks and different sampling methods of reinforcement learning affect the performance ofthe model. The reward for generating translation is determined by the scalability and iteration ofthe sampling strategy so it is difficult for the model to achieve bias-variance trade-off. Therefore according to the poor ability of the model to analyze the structure of the sequence in low-resourcetasks this paper proposes a neural machine translation model parameter optimization method for asynchronous dynamic programming training strategies. In view of the experience priority situa-tion under the current strategy each selective sampling experience not only improves the value ofthe experience state but also avoids the high computational resource consumption inherent in tra-ditional valuation methods (such as dynamic programming). We verify the Mongolian-Chineseand Uyghur-Chinese tasks on CCMT2019. The result shows that our method has improved the quality of low-resource neural machine translation model compared with general reinforcement learning methods which fully demonstrates the effectiveness of our method.{''},"Huhhot, China","Xiaoning, Jia  and
Hongxu, Hou  and
Nier, Wu  and
Haoran, Li  and
Xin, Chang",Proceedings of the 20th Chinese National Conference on Computational Linguistics,,August,886--894,Chinese Information Processing Society of China,Low-Resource Machine Translation based on Asynchronous Dynamic Programming,https://aclanthology.org/2021.ccl-1.79,2021,,,,English,
256,inproceedings,feiyu-etal-2021-incorporating,{``}Exposure bias and poor translation diversity are two common problems in neural machine trans-lation (NMT) which are caused by the general of the teacher forcing strategy for training inthe NMT models. Moreover the NMT models usually require the large-scale and high-quality parallel corpus. However Korean is a low resource language and there is no large-scale parallel corpus between Chinese and Korean which is a challenging for the researchers. Therefore wepropose a method which is to incorporate translation quality estimation into the translation processand adopt reinforcement learning. The evaluation mechanism is used to guide the training of the model so that the prediction cannot converge completely to the ground truth word. When the model predicts a sequence different from the ground truth word the evaluation mechanism cangive an appropriate evaluation and reward to the model. In addition we alleviated the lack of Korean corpus resources by adding training data. In our experiment we introduce a monolingual corpus of a certain scale to construct pseudo-parallel data. At the same time we also preprocessed the Korean corpus with different granularities to overcome the data sparsity. Experimental results show that our work is superior to the baselines in Chinese-Korean and Korean-Chinese translation tasks which fully certificates the effectiveness of our method.{''},"Huhhot, China","Feiyu, Li  and
Yahui, Zhao  and
Feiyang, Yang  and
Rongyi, Cui",Proceedings of the 20th Chinese National Conference on Computational Linguistics,,August,906--915,Chinese Information Processing Society of China,Incorporating translation quality estimation into {C}hinese-{K}orean neural machine translation,https://aclanthology.org/2021.ccl-1.81,2021,,,,English,
257,inproceedings,zhuang-etal-2021-robustly,{``}In the paper we present a {`}pre-training{'}+{`}post-training{'}+{`}fine-tuning{'} three-stage paradigm which is a supplementary framework for the standard {`}pre-training{'}+{`}fine-tuning{'} languagemodel approach. Furthermore based on three-stage paradigm we present a language modelnamed PPBERT. Compared with original BERT architecture that is based on the standard two-stage paradigm we do not fine-tune pre-trained model directly but rather post-train it on the domain or task related dataset first which helps to better incorporate task-awareness knowl-edge and domain-awareness knowledge within pre-trained model also from the training datasetreduce bias. Extensive experimental results indicate that proposed model improves the perfor-mance of the baselines on 24 NLP tasks which includes eight GLUE benchmarks eight Su-perGLUE benchmarks six extractive question answering benchmarks. More remarkably our proposed model is a more flexible and pluggable model where post-training approach is able to be plugged into other PLMs that are based on BERT. Extensive ablations further validate the effectiveness and its state-of-the-art (SOTA) performance. The open source code pre-trained models and post-trained models are available publicly.{''},"Huhhot, China","Zhuang, Liu  and
Wayne, Lin  and
Ya, Shi  and
Jun, Zhao",Proceedings of the 20th Chinese National Conference on Computational Linguistics,,August,1218--1227,Chinese Information Processing Society of China,A Robustly Optimized {BERT} Pre-training Approach with Post-training,https://aclanthology.org/2021.ccl-1.108,2021,,,,English,
258,inproceedings,hurriyetoglu-etal-2021-challenges,"This workshop is the fourth issue of a series of workshops on automatic extraction of socio-political events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of socio-political events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the state-of-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i) multilingual protest news detection detection, ii) fine-grained classification of socio-political events, and iii) discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi- and cross-lingual machine learning in few- and zero-shot settings.",Online,"H{\""u}rriyeto{\u{g}}lu, Ali  and
Tanev, Hristo  and
Zavarella, Vanni  and
Piskorski, Jakub  and
Yeniterzi, Reyyan  and
Mutlu, Osman  and
Yuret, Deniz  and
Villavicencio, Aline",Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),10.18653/v1/2021.case-1.1,August,1--9,Association for Computational Linguistics,Challenges and Applications of Automated Extraction of Socio-political Events from Text ({CASE} 2021): Workshop and Shared Task Report,https://aclanthology.org/2021.case-1.1,2021,,,,,
259,inproceedings,bohn-etal-2021-semi,"Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.",Online (Virtual Mode),"Bohn, Jeremias  and
Fischbach, Jannik  and
Schmitt, Martin  and
Sch{\""u}tze, Hinrich  and
Vogelsang, Andreas",Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021),,September,40--45,INCOMA Ltd.,Semi-Automated Labeling of Requirement Datasets for Relation Extraction,https://aclanthology.org/2021.bucc-1.6,2021,,,,,
260,inproceedings,hansen-sogaard-2021-guideline,"NLP models struggle with generalization due to sampling and annotator bias. This paper focuses on a different kind of bias that has received very little attention: guideline bias, i.e., the bias introduced by how our annotator guidelines are formulated. We examine two recently introduced dialogue datasets, CCPE-M and Taskmaster-1, both collected by trained assistants in a Wizard-of-Oz set-up. For CCPE-M, we show how a simple lexical bias for the word like in the guidelines biases the data collection. This bias, in effect, leads to poor performance on data without this bias: a preference elicitation architecture based on BERT suffers a 5.3{\%} absolute drop in performance, when like is replaced with a synonymous phrase, and a 13.2{\%} drop in performance when evaluated on out-of-sample data. For Taskmaster-1, we show how the order in which instructions are resented, biases the data collection.",Online,"Hansen, Victor Petr{\'e}n Bach  and
S{\o}gaard, Anders","Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",10.18653/v1/2021.bppf-1.2,August,8--14,Association for Computational Linguistics,Guideline Bias in {W}izard-of-{O}z Dialogues,https://aclanthology.org/2021.bppf-1.2,2021,,,,,
261,inproceedings,chiang-chen-2021-relating,"This work focuses on relating two mysteries in neural-based text generation: exposure bias, and text degeneration. Despite the long time since exposure bias was mentioned and the numerous studies for its remedy, to our knowledge, its impact on text generation has not yet been verified. Text degeneration is a problem that the widely-used pre-trained language model GPT-2 was recently found to suffer from (Holtzman et al., 2020). Motivated by the unknown causation of the text degeneration, in this paper we attempt to relate these two mysteries. Specifically, we first qualitatively and quantitatively identify mistakes made before text degeneration occurs. Then we investigate the significance of the mistakes by inspecting the hidden states in GPT-2. Our results show that text degeneration is likely to be partly caused by exposure bias. We also study the self-reinforcing mechanism of text degeneration, explaining why the mistakes amplify. In sum, our study provides a more concrete foundation for further investigation on exposure bias and text degeneration problems.","Punta Cana, Dominican Republic","Chiang, Ting-Rui  and
Chen, Yun-Nung",Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/2021.blackboxnlp-1.16,November,228--239,Association for Computational Linguistics,Relating Neural Text Degeneration to Exposure Bias,https://aclanthology.org/2021.blackboxnlp-1.16,2021,,,,,
262,inproceedings,jiang-etal-2021-bert,"Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the end-to-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework{'}s veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.","Punta Cana, Dominican Republic","Jiang, Zhiying  and
Tang, Raphael  and
Xin, Ji  and
Lin, Jimmy",Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/2021.blackboxnlp-1.39,November,496--509,Association for Computational Linguistics,How Does {BERT} Rerank Passages? An Attribution Analysis with Information Bottlenecks,https://aclanthology.org/2021.blackboxnlp-1.39,2021,,,,,
263,inproceedings,nitschke-2021-restoring,"The historical comparative method has a long history in historical linguists. It describes a process by which historical linguists aim to reverse-engineer the historical developments of language families in order to reconstruct proto-forms and familial relations between languages. In recent years, there have been multiple attempts to replicate this process through machine learning, especially in the realm of cognate detection (List et al., 2016; Ciobanu and Dinu, 2014; Rama et al., 2018). So far, most of these experiments aimed at actual reconstruction have attempted the prediction of a proto-form from the forms of the daughter languages (Ciobanu and Dinu, 2018; Meloni et al., 2019).. Here, we propose a reimplementation that uses modern related languages, or sisters, instead, to reconstruct the vocabulary of a target language. In particular, we show that we can reconstruct vocabulary of a target language by using a fairly small data set of parallel cognates from different sister languages, using a neural machine translation (NMT) architecture with a standard encoder-decoder setup. This effort is directly in furtherance of the goal to use machine learning tools to help under-served language communities in their efforts at reclaiming, preserving, or reconstructing their own languages.",Online,"Nitschke, Remo",Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,10.18653/v1/2021.americasnlp-1.13,June,122--130,Association for Computational Linguistics,Restoring the Sister: Reconstructing a Lexicon from Sister Languages using Neural Machine Translation,https://aclanthology.org/2021.americasnlp-1.13,2021,,,,,
264,inproceedings,washington-etal-2021-towards,"This paper presents work towards a morphological transducer and orthography converter for Dizhsa, or San Lucas Quiavin{\'\i} Zapotec, an endangered Western Tlacolula Valley Zapotec language. The implementation of various aspects of the language{'}s morphology is presented, as well as the transducer{'}s ability to perform analysis in two orthographies and convert between them. Potential uses of the transducer for language maintenance and issues of licensing are also discussed. Evaluation of the transducer shows that it is fairly robust although incomplete, and evaluation of orthographic conversion shows that this method is strongly affected by the coverage of the transducer.",Online,"Washington, Jonathan  and
Lopez, Felipe  and
Lillehaugen, Brook",Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,10.18653/v1/2021.americasnlp-1.21,June,185--193,Association for Computational Linguistics,Towards a morphological transducer and orthography converter for {W}estern {T}lacolula {V}alley {Z}apotec,https://aclanthology.org/2021.americasnlp-1.21,2021,,,,,
265,inproceedings,zheng-etal-2021-low,"This paper describes UTokyo{'}s submission to the AmericasNLP 2021 Shared Task on machine translation systems for indigenous languages of the Americas. We present a low-resource machine translation system that improves translation accuracy using cross-lingual language model pretraining. Our system uses an mBART implementation of fairseq to pretrain on a large set of monolingual data from a diverse set of high-resource languages before finetuning on 10 low-resource indigenous American languages: Aymara, Bribri, Ash{\'a}ninka, Guaran{\'\i}, Wixarika, N{\'a}huatl, H{\~n}{\""a}h{\~n}u, Quechua, Shipibo-Konibo, and Rar{\'a}muri. On average, our system achieved BLEU scores that were 1.64 higher and chrF scores that were 0.0749 higher than the baseline.",Online,"Zheng, Francis  and
Reid, Machel  and
Marrese-Taylor, Edison  and
Matsuo, Yutaka",Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,10.18653/v1/2021.americasnlp-1.26,June,234--240,Association for Computational Linguistics,Low-Resource Machine Translation Using Cross-Lingual Language Model Pretraining,https://aclanthology.org/2021.americasnlp-1.26,2021,,,,,
266,inproceedings,suter-etal-2021-grounding,"Phrase grounding (PG) is a multimodal task that grounds language in images. PG systems are evaluated on well-known benchmarks, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel metric, c-IoU, based on a union box{'}s component boxes. We experimentally show that our new metric greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.",Online,"Suter, Julia  and
Parcalabescu, Letitia  and
Frank, Anette",Proceedings of the Second Workshop on Advances in Language and Vision Research,10.18653/v1/2021.alvr-1.4,June,22--28,Association for Computational Linguistics,Grounding Plural Phrases: Countering Evaluation Biases by Individuation,https://aclanthology.org/2021.alvr-1.4,2021,,,,,
267,inproceedings,sorato-etal-2021-using,"The current study provides a diachronic analysis of the stereotypical portrayals concerning seven of the most prominent foreign nationalities living in Spain in a Spanish news outlet. We use 12 years (2007-2018) of news articles to train word embedding models to quantify the association of such outgroups with drug use, prostitution, crimes, and poverty concepts. Then, we investigate the effects of sociopolitical variables on the computed bias series, such as the outgroup size in the host country and the rate of the population receiving unemployment benefits. Our findings indicate that the texts exhibit bias against foreign-born people, especially in the case of outgroups for which the country of origin has a lower Gross Domestic Product per capita (PPP) than Spain.",Online,"Sorato, Danielly  and
Zavala-Rojas, Diana  and
del Carme Colominas Ventura, Maria",Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association,,December,34--46,Australasian Language Technology Association,Using Word Embeddings to Quantify Ethnic Stereotypes in 12 years of {S}panish News,https://aclanthology.org/2021.alta-1.4,2021,,,,,
268,inproceedings,renduchintala-etal-2021-gender,"Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.",Online,"Renduchintala, Adithya  and
Diaz, Denise  and
Heafield, Kenneth  and
Li, Xian  and
Diab, Mona",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.15,August,99--109,Association for Computational Linguistics,Gender bias amplification during Speed-Quality optimization in Neural Machine Translation,https://aclanthology.org/2021.acl-short.15,2021,,,,,
269,inproceedings,han-wang-2021-good,"Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model{'}s generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.",Online,"Han, Mingyue  and
Wang, Yinglin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.20,August,151--157,Association for Computational Linguistics,Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models,https://aclanthology.org/2021.acl-short.20,2021,,,,,
270,inproceedings,gupta-etal-2021-enforcing,"The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model{'}s training signal) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs. Additionally, we study the importance of designing logical formalisms that facilitate this kind of consistency-based training. We find that a more consistent formalism leads to improved model performance even without consistency-based training. When combined together, these two insights lead to a 10{\%} absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset.",Online,"Gupta, Nitish  and
Singh, Sameer  and
Gardner, Matt",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.22,August,168--174,Association for Computational Linguistics,Enforcing Consistency in Weakly Supervised Semantic Parsing,https://aclanthology.org/2021.acl-short.22,2021,,,,,
271,inproceedings,sosea-caragea-2021-emlm,"BERT has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model. In this paper, we present Emotion Masked Language Modelling, a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks. Using the same pre-training corpora as the original model, Wikipedia and BookCorpus, our BERT variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2{\%} F-1. Moreover, our approach shows an increased performance in our task-specific robustness tests.",Online,"Sosea, Tiberiu  and
Caragea, Cornelia",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.38,August,286--293,Association for Computational Linguistics,e{MLM}: A New Pre-training Objective for Emotion Related Tasks,https://aclanthology.org/2021.acl-short.38,2021,,,,,
272,inproceedings,aithal-tan-2021-positivity,"Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation.",Online,"Aithal, Madhusudhan  and
Tan, Chenhao",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.39,August,294--304,Association for Computational Linguistics,On Positivity Bias in Negative Reviews,https://aclanthology.org/2021.acl-short.39,2021,,,,,
273,inproceedings,lin-etal-2021-using,"Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found that several pre-trained language models, including BERT, ALBERT, and RoBERTa, show consistent preference to some options, even when these options are irrelevant to the question. When interfered by these irrelevant options, the performance of MRC models can be reduced from human-level performance to the chance-level performance. Human readers, however, are not clearly affected by these irrelevant options. Finally, we propose an augmented training method that can greatly reduce models{'} statistical biases.",Online,"Lin, Jieyu  and
Zou, Jiajie  and
Ding, Nai",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.43,August,333--342,Association for Computational Linguistics,Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models,https://aclanthology.org/2021.acl-short.43,2021,,,,,
274,inproceedings,kummerfeld-2021-quantifying,"Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting high quality data.",Online,"Kummerfeld, Jonathan K.",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.44,August,343--349,Association for Computational Linguistics,Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing,https://aclanthology.org/2021.acl-short.44,2021,,,,,
275,inproceedings,sun-peng-2021-men,"Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level.",Online,"Sun, Jiao  and
Peng, Nanyun",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.45,August,350--360,Association for Computational Linguistics,"Men Are Elected, Women Are Married: Events Gender Bias on {W}ikipedia",https://aclanthology.org/2021.acl-short.45,2021,,,,,
276,inproceedings,norouzi-etal-2021-code,"Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03{\%} exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice.",Online,"Norouzi, Sajad  and
Tang, Keyi  and
Cao, Yanshuai",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.98,August,776--785,Association for Computational Linguistics,Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data,https://aclanthology.org/2021.acl-short.98,2021,,,,,
277,inproceedings,jauregi-unanue-etal-2021-berttune,"Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs show improvements of up to 0.58 pp (3.28{\%}) in BLEU score and up to 0.76 pp (0.98{\%}) in BERTScore (F{\_}BERT) when fine-tuning a strong baseline.",Online,"Jauregi Unanue, Inigo  and
Parnell, Jacob  and
Piccardi, Massimo",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.115,August,915--924,Association for Computational Linguistics,{BERTT}une: Fine-Tuning Neural Machine Translation with {BERTS}core,https://aclanthology.org/2021.acl-short.115,2021,,,,,
278,inproceedings,xing-etal-2021-demoting,"In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers, severely limiting their performance on data with different or even no bias. In this paper, we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model{'}s learned lead bias and improve its generality on out-of-distribution data, with little to no performance loss on in-distribution data.",Online,"Xing, Linzi  and
Xiao, Wen  and
Carenini, Giuseppe",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.119,August,948--954,Association for Computational Linguistics,Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning,https://aclanthology.org/2021.acl-short.119,2021,,,,,
279,inproceedings,liang-leung-2021-improving,"Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five datasets contain a significant proportion of entities that have been seen in the training sets. Such test data would therefore not be able to reflect the true generalization ability of a model. Secondly, all datasets are dominated by a few fat-head entities, i.e., entities appearing with particularly high frequency. As a result, a model might be able to produce high prediction accuracy simply by keyword memorization without leveraging context knowledge. To address these data biases, we first refine each test set by excluding seen entities from it, so as to better evaluate a model{'}s generalization ability. Then, we propose a simple yet effective entity resampling method to make entities within the same category distributed equally, encouraging a model to leverage both name and context knowledge in the training process. Experimental results demonstrate that the proposed entity resampling method significantly improves a model{'}s ability in detecting unseen entities, especially for company, organization and position categories.",Online,"Liang, Guanqing  and
Leung, Cane Wing-Ki",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.125,August,992--997,Association for Computational Linguistics,Improving Model Generalization: A {C}hinese Named Entity Recognition Case Study,https://aclanthology.org/2021.acl-short.125,2021,,,,,
280,inproceedings,siblini-etal-2021-towards,"With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. The new results that we bring come to reinforce methods of the current state of the art.",Online,"Siblini, Wissam  and
Sayil, Baris  and
Kessaci, Yacine",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),10.18653/v1/2021.acl-short.130,August,1028--1034,Association for Computational Linguistics,Towards a more Robust Evaluation for Conversational Question Answering,https://aclanthology.org/2021.acl-short.130,2021,,,,,
281,inproceedings,beck-etal-2021-investigating,"This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data. We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement {--} in terms of inter-annotator agreement (+.14 Fleiss{'} Îº) and annotation quality {--} compared to students that do not receive any label suggestions. We further find that label suggestions from interactively trained models do not lead to an improvement over suggestions from a static model. Nonetheless, our analysis of suggestion bias shows that annotators remain capable of reflecting upon the suggested label in general. Finally, we confirm the quality of the annotated data in transfer learning experiments between different annotator groups. To facilitate further research in opinion mining on social media data, we release our collected data consisting of 200 expert and 2,785 student annotations.",Online,"Beck, Tilman  and
Lee, Ji-Ung  and
Viehmann, Christina  and
Maurer, Marcus  and
Quiring, Oliver  and
Gurevych, Iryna",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.1,August,1--13,Association for Computational Linguistics,Investigating label suggestions for opinion mining in {G}erman Covid-19 social media,https://aclanthology.org/2021.acl-long.1,2021,,,,,
282,inproceedings,wu-etal-2021-unified,"Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. However, individuals{'} cognition of social things is not always able to truly reflect the objective. There may be one-sided or biased semantics in their opinions on a claim. The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance. In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification. From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence. From the view of individual cognition, we select the top-$k$ articles with high degree of difference and interact with the claim to explore the local key evidence fragments. To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both. Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance.",Online,"Wu, Lianwei  and
Rao, Yuan  and
Lan, Yuqian  and
Sun, Ling  and
Qi, Zhaoyin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.5,August,59--68,Association for Computational Linguistics,Unified Dual-view Cognitive Model for Interpretable Claim Verification,https://aclanthology.org/2021.acl-long.5,2021,,,,,
283,inproceedings,ao-etal-2021-pens,"In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user{'}s reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at https://msnews.github.io/pens.html.",Online,"Ao, Xiang  and
Wang, Xiting  and
Luo, Ling  and
Qiao, Ying  and
He, Qing  and
Xie, Xing",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.7,August,82--92,Association for Computational Linguistics,{PENS}: A Dataset and Generic Framework for Personalized News Headline Generation,https://aclanthology.org/2021.acl-long.7,2021,,,,,
284,inproceedings,muller-sennrich-2021-understanding,"Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search {--} the de facto standard inference algorithm in NMT {--} and Eikema {\&} Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR decoding on a number of previously reported biases and failure cases of beam search. We find that MBR still exhibits a length and token frequency bias, owing to the MT metrics used as utility functions, but that MBR also increases robustness against copy noise in the training data and domain shift.",Online,"M{\""u}ller, Mathias  and
Sennrich, Rico",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.22,August,259--272,Association for Computational Linguistics,Understanding the Properties of Minimum {B}ayes Risk Decoding in Neural Machine Translation,https://aclanthology.org/2021.acl-long.22,2021,,,,,
285,inproceedings,yang-etal-2021-exploring,"While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.",Online,"Yang, Linyi  and
Li, Jiazheng  and
Cunningham, Padraig  and
Zhang, Yue  and
Smyth, Barry  and
Dong, Ruihai",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.26,August,306--316,Association for Computational Linguistics,Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis,https://aclanthology.org/2021.acl-long.26,2021,,,,,
286,inproceedings,white-cotterell-2021-examining,"Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages.",Online,"White, Jennifer C.  and
Cotterell, Ryan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.38,August,454--463,Association for Computational Linguistics,Examining the Inductive Bias of Neural Language Models with Artificial Languages,https://aclanthology.org/2021.acl-long.38,2021,,,,,
287,inproceedings,fraser-etal-2021-understanding,"Stereotypical language expresses widely-held beliefs about different social categories. Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology. The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence. We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons. We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature. Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes. It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied. Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination.",Online,"Fraser, Kathleen C.  and
Nejadgholi, Isar  and
Kiritchenko, Svetlana",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.50,August,600--616,Association for Computational Linguistics,Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model,https://aclanthology.org/2021.acl-long.50,2021,,,,,
288,inproceedings,wang-etal-2021-discontinuous,"Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.",Online,"Wang, Yucheng  and
Yu, Bowen  and
Zhu, Hongsong  and
Liu, Tingwen  and
Yu, Nan  and
Sun, Limin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.63,August,764--774,Association for Computational Linguistics,Discontinuous Named Entity Recognition as Maximal Clique Discovery,https://aclanthology.org/2021.acl-long.63,2021,,,,,
289,inproceedings,jiang-etal-2021-lnn,"Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3{\%} increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.",Online,"Jiang, Hang  and
Gurajada, Sairam  and
Lu, Qiuhao  and
Neelam, Sumit  and
Popa, Lucian  and
Sen, Prithviraj  and
Li, Yunyao  and
Gray, Alexander",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.64,August,775--787,Association for Computational Linguistics,{LNN}-{EL}: A Neuro-Symbolic Approach to Short-text Entity Linking,https://aclanthology.org/2021.acl-long.64,2021,,,,,
290,inproceedings,shaw-etal-2021-compositional,"Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.",Online,"Shaw, Peter  and
Chang, Ming-Wei  and
Pasupat, Panupong  and
Toutanova, Kristina",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.75,August,922--938,Association for Computational Linguistics,Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?,https://aclanthology.org/2021.acl-long.75,2021,,,,,
291,inproceedings,wan-etal-2021-dqn,"Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large search space with lots of local optimums. Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences. In addition, to tackle the label bias on Q-values computed by DQN, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences. Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification.",Online,"Wan, Hai  and
Chen, Haicheng  and
Du, Jianfeng  and
Luo, Weilin  and
Ye, Rongzhen",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.83,August,1030--1039,Association for Computational Linguistics,A {DQN}-based Approach to Finding Precise Evidences for Fact Verification,https://aclanthology.org/2021.acl-long.83,2021,,,,,
292,inproceedings,davis-van-schijndel-2021-uncovering,"A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.",Online,"Davis, Forrest  and
van Schijndel, Marten",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.93,August,1159--1171,Association for Computational Linguistics,Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning,https://aclanthology.org/2021.acl-long.93,2021,,,,,
293,inproceedings,lin-etal-2021-common,"Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method {---} multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7{\%} accuracy for X-CSQA over XLM-R{\_}L).",Online,"Lin, Bill Yuchen  and
Lee, Seyeon  and
Qiao, Xiaoyang  and
Ren, Xiang",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.102,August,1274--1287,Association for Computational Linguistics,Common Sense Beyond {E}nglish: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,https://aclanthology.org/2021.acl-long.102,2021,,,,,
294,inproceedings,hou-sachan-2021-birds,"NLP has a rich history of representing our prior understanding of language in the form of graphs. Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon. However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem. In this paper, we propose a new information-theoretic probe, Bird{'}s Eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs. Instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations. Furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis. We call this probing setup Worm{'}s Eye. Using these probes, we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure, and find that these models encode to some degree both syntactic as well as semantic information; albeit syntactic information to a greater extent.",Online,"Hou, Yifan  and
Sachan, Mrinmaya",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.145,August,1844--1859,Association for Computational Linguistics,Bird{'}s Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach,https://aclanthology.org/2021.acl-long.145,2021,,,,,
295,inproceedings,cao-etal-2021-knowledgeable,"Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases.",Online,"Cao, Boxi  and
Lin, Hongyu  and
Han, Xianpei  and
Sun, Le  and
Yan, Lingyong  and
Liao, Meng  and
Xue, Tong  and
Xu, Jin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.146,August,1860--1874,Association for Computational Linguistics,Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases,https://aclanthology.org/2021.acl-long.146,2021,,,,,
296,inproceedings,antoniak-mimno-2021-bad,"A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.",Online,"Antoniak, Maria  and
Mimno, David",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.148,August,1889--1904,Association for Computational Linguistics,Bad Seeds: Evaluating Lexical Methods for Bias Measurement,https://aclanthology.org/2021.acl-long.148,2021,,,,,
297,inproceedings,field-etal-2021-survey,"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",Online,"Field, Anjalie  and
Blodgett, Su Lin  and
Waseem, Zeerak  and
Tsvetkov, Yulia",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.149,August,1905--1925,Association for Computational Linguistics,"A Survey of Race, Racism, and Anti-Racism in {NLP}",https://aclanthology.org/2021.acl-long.149,2021,,,,,
298,inproceedings,barikeri-etal-2021-redditbias,"Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.",Online,"Barikeri, Soumya  and
Lauscher, Anne  and
Vuli{\'c}, Ivan  and
Glava{\v{s}}, Goran",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.151,August,1941--1955,Association for Computational Linguistics,{R}eddit{B}ias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models,https://aclanthology.org/2021.acl-long.151,2021,,,,,
299,inproceedings,cheng-etal-2021-mitigating,"The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain demographic-identity terms (e.g., {``}gay{''} or {``}black{''}) is more likely to be classified as an instance of cyberbullying. In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram). We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances. Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.",Online,"Cheng, Lu  and
Mosallanezhad, Ahmadreza  and
Silva, Yasin  and
Hall, Deborah  and
Liu, Huan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.168,August,2158--2168,Association for Computational Linguistics,Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach,https://aclanthology.org/2021.acl-long.168,2021,,,,,
300,inproceedings,rogers-2021-changing,"NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process.",Online,"Rogers, Anna",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.170,August,2182--2194,Association for Computational Linguistics,Changing the World by Changing the Data,https://aclanthology.org/2021.acl-long.170,2021,,,,,
301,inproceedings,guo-etal-2021-chase,"The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS. It consists of 5,459 coherent question sequences (17,940 questions with their SQL queries annotated) over 280 databases, in which only 35{\%} of questions are context-independent, and 28{\%} of SQL queries are easy. We experiment on Chase with three state-of-the-art XDTS approaches. The best approach only achieves an exact match accuracy of 40{\%} over all questions and 16{\%} over all question sequences, indicating that Chase highlights the challenging problems of XDTS. We believe that XDTS can provide fertile soil for addressing the problems.",Online,"Guo, Jiaqi  and
Si, Ziliang  and
Wang, Yu  and
Liu, Qian  and
Fan, Ming  and
Lou, Jian-Guang  and
Yang, Zijiang  and
Liu, Ting",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.180,August,2316--2331,Association for Computational Linguistics,Chase: A Large-Scale and Pragmatic {C}hinese Dataset for Cross-Database Context-Dependent Text-to-{SQL},https://aclanthology.org/2021.acl-long.180,2021,,,,,
302,inproceedings,dopierre-etal-2021-protaugment,"Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes. In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task). ProtAugment is a novel extension of Prototypical Networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode. It relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode. The diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the Prototypical Network training objective as a consistency loss. ProtAugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain.",Online,"Dopierre, Thomas  and
Gravier, Christophe  and
Logerais, Wilfried",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.191,August,2454--2466,Association for Computational Linguistics,{PROTAUGMENT}: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning,https://aclanthology.org/2021.acl-long.191,2021,,,,,
303,inproceedings,liu-etal-2021-competence,"Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model{'}s performance. The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.",Online,"Liu, Fenglin  and
Ge, Shen  and
Wu, Xian",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.234,August,3001--3012,Association for Computational Linguistics,Competence-based Multimodal Curriculum Learning for Medical Report Generation,https://aclanthology.org/2021.acl-long.234,2021,,,,,
304,inproceedings,rust-etal-2021-good,"In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model{'}s vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.",Online,"Rust, Phillip  and
Pfeiffer, Jonas  and
Vuli{\'c}, Ivan  and
Ruder, Sebastian  and
Gurevych, Iryna",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.243,August,3118--3135,Association for Computational Linguistics,How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models,https://aclanthology.org/2021.acl-long.243,2021,,,,,
305,inproceedings,zeinert-etal-2021-annotating,"Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.",Online,"Zeinert, Philine  and
Inie, Nanna  and
Derczynski, Leon",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.247,August,3181--3197,Association for Computational Linguistics,Annotating Online Misogyny,https://aclanthology.org/2021.acl-long.247,2021,,,,,
306,inproceedings,yan-etal-2021-position,"The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses. We test the performance of existing models on such adversarial examples and observe a significant performance drop. To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause. Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.",Online,"Yan, Hanqi  and
Gui, Lin  and
Pergola, Gabriele  and
He, Yulan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.261,August,3364--3375,Association for Computational Linguistics,Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction,https://aclanthology.org/2021.acl-long.261,2021,,,,,
307,inproceedings,bao-etal-2021-g,"Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.",Online,"Bao, Guangsheng  and
Zhang, Yue  and
Teng, Zhiyang  and
Chen, Boxing  and
Luo, Weihua",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.267,August,3442--3455,Association for Computational Linguistics,{G}-Transformer for Document-Level Machine Translation,https://aclanthology.org/2021.acl-long.267,2021,,,,,
308,inproceedings,zhu-soricut-2021-h,"We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.",Online,"Zhu, Zhenhai  and
Soricut, Radu",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.294,August,3801--3815,Association for Computational Linguistics,{H}-Transformer-1{D}: Fast One-Dimensional Hierarchical Attention for Sequences,https://aclanthology.org/2021.acl-long.294,2021,,,,,
309,inproceedings,tan-etal-2021-reliability,"Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing {---} with an emphasis on interdisciplinary collaboration {---} will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.",Online,"Tan, Samson  and
Joty, Shafiq  and
Baxter, Kathy  and
Taeihagh, Araz  and
Bennett, Gregory A.  and
Kan, Min-Yen",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.321,August,4153--4169,Association for Computational Linguistics,Reliability Testing for Natural Language Processing Systems,https://aclanthology.org/2021.acl-long.321,2021,,,,,
310,inproceedings,ousidhoum-etal-2021-probing,"Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.",Online,"Ousidhoum, Nedjma  and
Zhao, Xinran  and
Fang, Tianqing  and
Song, Yangqiu  and
Yeung, Dit-Yan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.329,August,4262--4274,Association for Computational Linguistics,Probing Toxic Content in Large Pre-Trained Language Models,https://aclanthology.org/2021.acl-long.329,2021,,,,,
311,inproceedings,sheng-etal-2021-societal,"Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",Online,"Sheng, Emily  and
Chang, Kai-Wei  and
Natarajan, Prem  and
Peng, Nanyun",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.330,August,4275--4293,Association for Computational Linguistics,Societal Biases in Language Generation: Progress and Challenges,https://aclanthology.org/2021.acl-long.330,2021,,,,,
312,inproceedings,chen-etal-2021-evaluating,"Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous names, AmbER sets act as a challenging test of entity disambiguation. We create AmbER sets for three popular open-domain tasks: fact checking, slot filling, and question answering, and evaluate a diverse set of retrievers. We find that the retrievers exhibit popularity bias, significantly under-performing on rarer entities that share a name, e.g., they are twice as likely to retrieve erroneous documents on queries for the less popular entity under the same name. These experiments on AmbER sets show their utility as an evaluation tool and highlight the weaknesses of popular retrieval systems.",Online,"Chen, Anthony  and
Gudipati, Pallavi  and
Longpre, Shayne  and
Ling, Xiao  and
Singh, Sameer",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.345,August,4472--4485,Association for Computational Linguistics,Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based {NLP},https://aclanthology.org/2021.acl-long.345,2021,,,,,
313,inproceedings,ye-etal-2021-one2set,"Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.",Online,"Ye, Jiacheng  and
Gui, Tao  and
Luo, Yichao  and
Xu, Yige  and
Zhang, Qi",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.354,August,4598--4608,Association for Computational Linguistics,{O}ne2{S}et: {G}enerating Diverse Keyphrases as a Set,https://aclanthology.org/2021.acl-long.354,2021,,,,,
314,inproceedings,logan-iv-etal-2021-benchmarking,"Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison. In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models. We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance. Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches. We also find that performance is minimally impacted by limiting the number of tracked mentions.",Online,"Logan IV, Robert L  and
McCallum, Andrew  and
Singh, Sameer  and
Bikel, Dan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.364,August,4717--4731,Association for Computational Linguistics,Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference,https://aclanthology.org/2021.acl-long.364,2021,,,,,
315,inproceedings,zhang-etal-2021-de,"Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.",Online,"Zhang, Wenkai  and
Lin, Hongyu  and
Han, Xianpei  and
Sun, Le",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.371,August,4803--4813,Association for Computational Linguistics,De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention,https://aclanthology.org/2021.acl-long.371,2021,,,,,
316,inproceedings,li-etal-2021-semi-supervised,"Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods. Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce.",Online,"Li, Changchun  and
Li, Ximing  and
Ouyang, Jihong",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.391,August,5044--5053,Association for Computational Linguistics,Semi-Supervised Text Classification with Balanced Deep Representation Distributions,https://aclanthology.org/2021.acl-long.391,2021,,,,,
317,inproceedings,wei-etal-2021-cognitive,"The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.",Online,"Wei, Jason  and
Meister, Clara  and
Cotterell, Ryan",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.404,August,5191--5202,Association for Computational Linguistics,A Cognitive Regularizer for Language Modeling,https://aclanthology.org/2021.acl-long.404,2021,,,,,
318,inproceedings,gupta-jaggi-2021-obtaining,"The advent of contextual word embeddings {---} representations of words which incorporate semantic and syntactic information from their context{---}has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.",Online,"Gupta, Prakhar  and
Jaggi, Martin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.408,August,5241--5253,Association for Computational Linguistics,Obtaining Better Static Word Embeddings Using Contextual Embedding Models,https://aclanthology.org/2021.acl-long.408,2021,,,,,
319,inproceedings,nadeem-etal-2021-stereoset,"A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.",Online,"Nadeem, Moin  and
Bethke, Anna  and
Reddy, Siva",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.416,August,5356--5371,Association for Computational Linguistics,{S}tereo{S}et: Measuring stereotypical bias in pretrained language models,https://aclanthology.org/2021.acl-long.416,2021,,,,,
320,inproceedings,qian-etal-2021-counterfactual,"Today{'}s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models{'} generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework {--} Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases {`}poison{'} the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair{'}s effectiveness, generalizability and fairness.",Online,"Qian, Chen  and
Feng, Fuli  and
Wen, Lijie  and
Ma, Chunping  and
Xie, Pengjun",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.422,August,5434--5445,Association for Computational Linguistics,Counterfactual Inference for Text Classification Debiasing,https://aclanthology.org/2021.acl-long.422,2021,,,,,
321,inproceedings,qi-etal-2021-pp,"Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests. Besides, they are usually diverse in content and topic. Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation. In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score. The former is used to capture the personalized user interest in news. The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework. Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.",Online,"Qi, Tao  and
Wu, Fangzhao  and
Wu, Chuhan  and
Huang, Yongfeng",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.424,August,5457--5467,Association for Computational Linguistics,{PP}-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity,https://aclanthology.org/2021.acl-long.424,2021,,,,,
322,inproceedings,kamigaito-hayashi-2021-unified,"In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",Online,"Kamigaito, Hidetaka  and
Hayashi, Katsuhiko",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.429,August,5517--5531,Association for Computational Linguistics,Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding,https://aclanthology.org/2021.acl-long.429,2021,,,,,
323,inproceedings,le-etal-2021-dvd,"A video-grounded dialogue system is required to understand both dialogue, which contains semantic dependencies from turn to turn, and video, which contains visual cues of spatial and temporal scene variations. Building such dialogue systems is a challenging problem, involving various reasoning types on both visual and language inputs. Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation. These benchmarks are also not explicitly designed to minimise biases that models can exploit without actual reasoning. To address these limitations, in this paper, we present DVD, a Diagnostic Dataset for Video-grounded Dialogue. The dataset is designed to contain minimal biases and has detailed annotations for the different types of reasoning over the spatio-temporal space of video. Dialogues are synthesized over multiple question turns, each of which is injected with a set of cross-turn semantic relationships. We use DVD to analyze existing approaches, providing interesting insights into their abilities and limitations. In total, DVD is built from 11k CATER synthetic videos and contains 10 instances of 10-round dialogues for each video, resulting in more than 100k dialogues and 1M question-answer pairs. Our code and dataset are publicly available.",Online,"Le, Hung  and
Sankar, Chinnadhurai  and
Moon, Seungwhan  and
Beirami, Ahmad  and
Geramifard, Alborz  and
Kottur, Satwik",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.439,August,5651--5665,Association for Computational Linguistics,{DVD}: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue,https://aclanthology.org/2021.acl-long.439,2021,,,,,
324,inproceedings,lee-etal-2021-robustifying,"This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate {``}pseudo-evidentiality{''} annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.",Online,"Lee, Kyungjae  and
Hwang, Seung-won  and
Han, Sang-eun  and
Lee, Dohyeon",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.476,August,6110--6119,Association for Computational Linguistics,Robustifying Multi-hop {QA} through Pseudo-Evidentiality Training,https://aclanthology.org/2021.acl-long.476,2021,,,,,
325,inproceedings,guan-etal-2021-openmeva,"Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research.",Online,"Guan, Jian  and
Zhang, Zhexin  and
Feng, Zhuoer  and
Liu, Zitao  and
Ding, Wenbiao  and
Mao, Xiaoxi  and
Fan, Changjie  and
Huang, Minlie",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.500,August,6394--6407,Association for Computational Linguistics,{O}pen{MEVA}: A Benchmark for Evaluating Open-ended Story Generation Metrics,https://aclanthology.org/2021.acl-long.500,2021,,,,,
326,inproceedings,mitzalis-etal-2021-bertgen,"We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen{'}s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.",Online,"Mitzalis, Faidon  and
Caglayan, Ozan  and
Madhyastha, Pranava  and
Specia, Lucia",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.503,August,6440--6455,Association for Computational Linguistics,{BERTG}en: Multi-task Generation through {BERT},https://aclanthology.org/2021.acl-long.503,2021,,,,,
327,inproceedings,colombo-etal-2021-novel,"Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement. In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains. This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. Our bound aims at controlling the approximation error via the Renyi{'}s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios. We show the superiority of this method on fair classification and on textual style transfer tasks. Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.",Online,"Colombo, Pierre  and
Piantanida, Pablo  and
Clavel, Chlo{\'e}",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.511,August,6539--6550,Association for Computational Linguistics,A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations,https://aclanthology.org/2021.acl-long.511,2021,,,,,
328,inproceedings,lan-etal-2021-neural,"Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.",Online,"Lan, Wuwei  and
Jiang, Chao  and
Xu, Wei",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.531,August,6815--6828,Association for Computational Linguistics,Neural semi-{M}arkov {CRF} for Monolingual Word Alignment,https://aclanthology.org/2021.acl-long.531,2021,,,,,
329,inproceedings,wei-jia-2021-statistical,"Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated against noisy, human predicted labels instead of the ground truth, and metric predictions fluctuate based on the test sets they were calculated on. By applying a bias-variance-noise decomposition, we adjust this error to a noise-free, infinite test set setting. Our analysis compares the adjusted error of metrics to humans and a derived, perfect segment-level annotator, both of which are unbiased estimators dependent on the number of judgments collected. In MT, we identify two settings where metrics outperform humans due to a statistical advantage in variance: when the number of human judgments used is small, and when the quality difference between compared systems is small.",Online,"Wei, Johnny  and
Jia, Robin",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.533,August,6840--6854,Association for Computational Linguistics,The statistical advantage of automatic {NLG} metrics at the system level,https://aclanthology.org/2021.acl-long.533,2021,,,,,
330,inproceedings,fu-etal-2021-spanner,"Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model{'}s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems{'} outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all codes and datasets available: \url{https://github.com/neulab/spanner}, as well as an online system demo: \url{http://spanner.sh}. Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: \url{http://explainaboard.nlpedia.ai/leaderboard/task-ner/}.",Online,"Fu, Jinlan  and
Huang, Xuanjing  and
Liu, Pengfei",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),10.18653/v1/2021.acl-long.558,August,7183--7195,Association for Computational Linguistics,{S}pan{NER}: Named Entity Re-/Recognition as Span Prediction,https://aclanthology.org/2021.acl-long.558,2021,,,,,
331,inproceedings,imanigooghari-etal-2021-parcoure,"With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.",Online,"ImaniGooghari, Ayyoob  and
Jalili Sabet, Masoud  and
Dufter, Philipp  and
Cysou, Michael  and
Sch{\""u}tze, Hinrich",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,10.18653/v1/2021.acl-demo.8,August,63--72,Association for Computational Linguistics,{P}ar{C}our{E}: A Parallel Corpus Explorer for a Massively Multilingual Corpus,https://aclanthology.org/2021.acl-demo.8,2021,,,,,
332,inproceedings,zeng-etal-2021-openattack,"Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack.",Online,"Zeng, Guoyang  and
Qi, Fanchao  and
Zhou, Qianrui  and
Zhang, Tingji  and
Ma, Zixian  and
Hou, Bairu  and
Zang, Yuan  and
Liu, Zhiyuan  and
Sun, Maosong",Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,10.18653/v1/2021.acl-demo.43,August,363--371,Association for Computational Linguistics,{O}pen{A}ttack: An Open-source Textual Adversarial Attack Toolkit,https://aclanthology.org/2021.acl-demo.43,2021,,,,,
333,inproceedings,radhakrishnan-etal-2020-little,"The rise in the usage of social media has placed it in a central position for news dissemination and consumption. This greatly increases the potential for proliferation of rumours and misinformation. In an effort to mitigate the spread of rumours, we tackle the related task of identifying the stance (Support, Deny, Query, Comment) of a social media post. Unlike previous works, we impose inductive biases that capture platform specific user behavior. These biases, coupled with social media fine-tuning of BERT allow for better language understanding, thus yielding an F1 score of 58.7 on the SemEval 2019 task on rumour stance detection.",Online,"Radhakrishnan, Karthik  and
Kanakagiri, Tushar  and
Chakravarthy, Sharanya  and
Balachandran, Vidhisha",Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),10.18653/v1/2020.wnut-1.31,November,244--248,Association for Computational Linguistics,{``}A Little Birdie Told Me ... {''} - Inductive Biases for Rumour Stance Detection on Social Media,https://aclanthology.org/2020.wnut-1.31,2020,,,,,
334,inproceedings,kocmi-etal-2020-gender,"Gender bias in machine translation can manifest when choosing gender inflections based on spurious gender correlations. For example, always translating doctors as men and nurses as women. This can be particularly harmful as models become more popular and deployed within commercial systems. Our work presents the largest evidence for the phenomenon in more than 19 systems submitted to the WMT over four diverse target languages: Czech, German, Polish, and Russian. To achieve this, we use WinoMT, a recent automatic test suite which examines gender coreference and bias when translating from English to languages with grammatical gender. We extend WinoMT to handle two new languages tested in WMT: Polish and Czech. We find that all systems consistently use spurious correlations in the data rather than meaningful contextual information.",Online,"Kocmi, Tom  and
Limisiewicz, Tomasz  and
Stanovsky, Gabriel",Proceedings of the Fifth Conference on Machine Translation,,November,357--364,Association for Computational Linguistics,Gender Coreference and Bias Evaluation at {WMT} 2020,https://aclanthology.org/2020.wmt-1.39,2020,,,,,
335,inproceedings,scherrer-etal-2020-mucow,"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -{\textgreater} Czech, English -{\textgreater} German, and English -{\textgreater} Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year.",Online,"Scherrer, Yves  and
Raganato, Alessandro  and
Tiedemann, J{\""o}rg",Proceedings of the Fifth Conference on Machine Translation,,November,365--370,Association for Computational Linguistics,The {MUCOW} word sense disambiguation test suite at {WMT} 2020,https://aclanthology.org/2020.wmt-1.40,2020,,,,,
336,inproceedings,rios-etal-2020-subword,"Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN{\textless}-{\textgreater}FR,CS,DE,FI system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.",Online,"Rios, Annette  and
M{\""u}ller, Mathias  and
Sennrich, Rico",Proceedings of the Fifth Conference on Machine Translation,,November,528--537,Association for Computational Linguistics,Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation,https://aclanthology.org/2020.wmt-1.64,2020,,,,,
337,inproceedings,saunders-byrne-2020-addressing,"The 2020 WMT Biomedical translation task evaluated Medline abstract translations. This is a small-domain translation task, meaning limited relevant training data with very distinct style and vocabulary. Models trained on such data are susceptible to exposure bias effects, particularly when training sentence pairs are imperfect translations of each other. This can result in poor behaviour during inference if the model learns to neglect the source sentence. The UNICAM entry addresses this problem during fine-tuning using a robust variant on Minimum Risk Training. We contrast this approach with data-filtering to remove {`}problem{'} training examples. Under MRT fine-tuning we obtain good results for both directions of English-German and English-Spanish biomedical translation. In particular we achieve the best English-to-Spanish translation result and second-best Spanish-to-English result, despite using only single models with no ensembling.",Online,"Saunders, Danielle  and
Byrne, Bill",Proceedings of the Fifth Conference on Machine Translation,,November,862--869,Association for Computational Linguistics,Addressing Exposure Bias With Document Minimum Risk Training: {C}ambridge at the {WMT}20 Biomedical Translation Task,https://aclanthology.org/2020.wmt-1.94,2020,,,,,
338,inproceedings,basta-etal-2020-towards,"Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5{\%} accuracy).","Seattle, USA","Basta, Christine  and
Costa-juss{\`a}, Marta R.  and
Fonollosa, Jos{\'e} A. R.",Proceedings of the The Fourth Widening Natural Language Processing Workshop,10.18653/v1/2020.winlp-1.25,July,99--102,Association for Computational Linguistics,Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information,https://aclanthology.org/2020.winlp-1.25,2020,,,,,
339,inproceedings,yeo-chen-2020-defining,"Our work focuses on the biases that emerge in the natural language generation (NLG) task of sentence completion. In this paper, we introduce a mathematical framework of fairness for NLG followed by an evaluation of gender biases in two state-of-the-art language models. Our analysis provides a theoretical formulation for biases in NLG and empirical evidence that existing language generation models embed gender bias.","Seattle, USA","Yeo, Catherine  and
Chen, Alyssa",Proceedings of the The Fourth Widening Natural Language Processing Workshop,10.18653/v1/2020.winlp-1.27,July,107--109,Association for Computational Linguistics,Defining and Evaluating Fair Natural Language Generation,https://aclanthology.org/2020.winlp-1.27,2020,,,,,
340,inproceedings,lauscher-etal-2020-araweat,"Recent work has shown that distributional word vector spaces often encode human biases like sexism or racism. In this work, we conduct an extensive analysis of biases in Arabic word embeddings by applying a range of recently introduced bias tests on a variety of embedding spaces induced from corpora in Arabic. We measure the presence of biases across several dimensions, namely: embedding models (Skip-Gram, CBOW, and FastText) and vector sizes, types of text (encyclopedic text, and news vs. user-generated content), dialects (Egyptian Arabic vs. Modern Standard Arabic), and time (diachronic analyses over corpora from different time periods). Our analysis yields several interesting findings, e.g., that implicit gender bias in embeddings trained on Arabic news corpora steadily increases over time (between 2007 and 2017). We make the Arabic bias specifications (AraWEAT) publicly available.","Barcelona, Spain (Online)","Lauscher, Anne  and
Takieddin, Rafik  and
Ponzetto, Simone Paolo  and
Glava{\v{s}}, Goran",Proceedings of the Fifth Arabic Natural Language Processing Workshop,,December,192--199,Association for Computational Linguistics,{A}ra{WEAT}: Multidimensional Analysis of Biases in {A}rabic Word Embeddings,https://aclanthology.org/2020.wanlp-1.17,2020,,,,,
341,inproceedings,sinnemaki-haakana-2020-variation,"In this paper we present a method for identifying and analyzing adnominal possessive constructions in 66 Universal Dependencies treebanks. We classify adpossessive constructions in terms of their morphological type (locus of marking) and present a workflow for detecting and analyzing them typologically. Based on a preliminary evaluation, the algorithm works fairly reliably in adpossessive constructions that are morphologically marked. However, it performs rather poorly in adpossessive constructions that are not marked morphologically, so-called zero-marked constructions, because of difficulties in identifying these constructions with the current annotation. We also discuss different types of variation in annotation in different treebanks for the same language and for treebanks of closely related languages. The research focuses on one well-circumscribed and universal construction in the hope of generating more interest in using UD for cross-linguistic comparison and for contributing towards developing yet more consistent annotation of constructions in the UD annotation scheme.","Barcelona, Spain (Online)","Sinnem{\""a}ki, Kaius  and
Haakana, Viljami",Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),,December,158--167,Association for Computational Linguistics,Variation in {U}niversal {D}ependencies annotation: A token-based typological case study on adpossessive constructions,https://aclanthology.org/2020.udw-1.18,2020,,,,,
342,inproceedings,risch-krestel-2020-bagging,"Modern transformer-based models with hundreds of millions of parameters, such as BERT, achieve impressive results at text classification tasks. This also holds for aggression identification and offensive language detection, where deep learning approaches consistently outperform less complex models, such as decision trees. While the complex models fit training data well (low bias), they also come with an unwanted high variance. Especially when fine-tuning them on small datasets, the classification performance varies significantly for slightly different training data. To overcome the high variance and provide more robust predictions, we propose an ensemble of multiple fine-tuned BERT models based on bootstrap aggregating (bagging). In this paper, we describe such an ensemble system and present our submission to the shared tasks on aggression identification 2020 (team name: Julian). Our submission is the best-performing system for five out of six subtasks. For example, we achieve a weighted F1-score of 80.3{\%} for task A on the test dataset of English social media posts. In our experiments, we compare different model configurations and vary the number of models used in the ensemble. We find that the F1-score drastically increases when ensembling up to 15 models, but the returns diminish for more models.","Marseille, France","Risch, Julian  and
Krestel, Ralf","Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",,May,55--61,European Language Resources Association (ELRA),Bagging {BERT} Models for Robust Aggression Identification,https://aclanthology.org/2020.trac-1.9,2020,,,,English,979-10-95546-56-6
343,inproceedings,al-ghezi-kurimo-2020-graph,"We propose a simple and efficient framework to learn syntactic embeddings based on information derived from constituency parse trees. Using biased random walk methods, our embeddings not only encode syntactic information about words, but they also capture contextual information. We also propose a method to train the embeddings on multiple constituency parse trees to ensure the encoding of global syntactic representation. Quantitative evaluation of the embeddings show a competitive performance on POS tagging task when compared to other types of embeddings, and qualitative evaluation reveals interesting facts about the syntactic typology learned by these embeddings.","Barcelona, Spain (Online)","Al-Ghezi, Ragheb  and
Kurimo, Mikko",Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),10.18653/v1/2020.textgraphs-1.8,December,72--78,Association for Computational Linguistics,Graph-based Syntactic Word Embeddings,https://aclanthology.org/2020.textgraphs-1.8,2020,,,,,
344,article,mccoy-etal-2020-syntax,"Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.","Cambridge, MA","McCoy, R. Thomas  and
Frank, Robert  and
Linzen, Tal",,10.1162/tacl_a_00304,,125--140,MIT Press,Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks,https://aclanthology.org/2020.tacl-1.9,2020,Transactions of the Association for Computational Linguistics,8,,,
345,article,azpiazu-pera-2020-hierarchical,"The alignment of word embedding spaces in different languages into a common crosslingual space has recently been in vogue. Strategies that do so compute pairwise alignments and then map multiple languages to a single pivot language (most often English). These strategies, however, are biased towards the choice of the pivot language, given that language proximity and the linguistic characteristics of the target language can strongly impact the resultant crosslingual space in detriment of topologically distant languages. We present a strategy that eliminates the need for a pivot language by learning the mappings across languages in a hierarchical way. Experiments demonstrate that our strategy significantly improves vocabulary induction scores in all existing benchmarks, as well as in a new non-English{--}centered benchmark we built, which we make publicly available.","Cambridge, MA","Azpiazu, Ion Madrazo  and
Pera, Maria Soledad",,10.1162/tacl_a_00320,,361--376,MIT Press,Hierarchical Mapping for Crosslingual Word Embedding Alignment,https://aclanthology.org/2020.tacl-1.24,2020,Transactions of the Association for Computational Linguistics,8,,,
346,article,kumar-etal-2020-nurse,"Word embeddings are the standard model for semantic and syntactic representations of words. Unfortunately, these models have been shown to exhibit undesirable word associations resulting from gender, racial, and religious biases. Existing post-processing methods for debiasing word embeddings are unable to mitigate gender bias hidden in the spatial arrangement of word vectors. In this paper, we propose RAN-Debias, a novel gender debiasing methodology that not only eliminates the bias present in a word vector but also alters the spatial distribution of its neighboring vectors, achieving a bias-free setting while maintaining minimal semantic offset. We also propose a new bias evaluation metric, Gender-based Illicit Proximity Estimate (GIPE), which measures the extent of undue proximity in word vectors resulting from the presence of gender-based predilections. Experiments based on a suite of evaluation metrics show that RAN-Debias significantly outperforms the state-of-the-art in reducing proximity bias (GIPE) by at least 42.02{\%}. It also reduces direct bias, adding minimal semantic disturbance, and achieves the best performance in a downstream application task (coreference resolution).","Cambridge, MA","Kumar, Vaibhav  and
Bhotia, Tenzin Singhay  and
Kumar, Vaibhav  and
Chakraborty, Tanmoy",,10.1162/tacl_a_00327,,486--503,MIT Press,Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings,https://aclanthology.org/2020.tacl-1.32,2020,Transactions of the Association for Computational Linguistics,8,,,
347,article,opitz-etal-2020-amr,"Different metrics have been proposed to compare Abstract Meaning Representation (AMR) graphs. The canonical Smatch metric (Cai and Knight, 2013) aligns the variables of two graphs and assesses triple matches. The recent SemBleu metric (Song and Gildea, 2019) is based on the machine-translation metric Bleu (Papineni et al., 2002) and increases computational efficiency by ablating the variable-alignment. In this paper, i) we establish criteria that enable researchers to perform a principled assessment of metrics comparing meaning representations like AMR; ii) we undertake a thorough analysis of Smatch and SemBleu where we show that the latter exhibits some undesirable properties. For example, it does not conform to the identity of indiscernibles rule and introduces biases that are hard to control; and iii) we propose a novel metric S2 match that is more benevolent to only very slight meaning deviations and targets the fulfilment of all established criteria. We assess its suitability and show its advantages over Smatch and SemBleu.","Cambridge, MA","Opitz, Juri  and
Parcalabescu, Letitia  and
Frank, Anette",,10.1162/tacl_a_00329,,522--538,MIT Press,{AMR} Similarity Metrics from Principles,https://aclanthology.org/2020.tacl-1.34,2020,Transactions of the Association for Computational Linguistics,8,,,
348,article,kuncoro-etal-2020-syntactic,"Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open question whether scalable learners like BERT can become fully proficient in the syntax of natural language by virtue of data scale alone, or whether they still benefit from more explicit syntactic biases. To answer this question, we introduce a knowledge distillation strategy for injecting syntactic biases into BERT pretraining, by distilling the syntactically informative predictions of a hierarchical{---}albeit harder to scale{---}syntactic language model. Since BERT models masked words in bidirectional context, we propose to distill the approximate marginal distribution over words in context from the syntactic LM. Our approach reduces relative error by 2{--}21{\%} on a diverse set of structured prediction tasks, although we obtain mixed results on the GLUE benchmark. Our findings demonstrate the benefits of syntactic biases, even for representation learners that exploit large amounts of data, and contribute to a better understanding of where syntactic biases are helpful in benchmarks of natural language understanding.","Cambridge, MA","Kuncoro, Adhiguna  and
Kong, Lingpeng  and
Fried, Daniel  and
Yogatama, Dani  and
Rimell, Laura  and
Dyer, Chris  and
Blunsom, Phil",,10.1162/tacl_a_00345,,776--794,MIT Press,Syntactic Structure Distillation Pretraining for Bidirectional Encoders,https://aclanthology.org/2020.tacl-1.50,2020,Transactions of the Association for Computational Linguistics,8,,,
349,article,meister-etal-2020-best,"Decoding for many NLP tasks requires an effective heuristic algorithm for approximating exact search because the problem of searching the full output space is often intractable, or impractical in many settings. The default algorithm for this job is beam search{---}a pruned version of breadth-first search. Quite surprisingly, beam search often returns better results than exact inference due to beneficial search bias for NLP tasks. In this work, we show that the standard implementation of beam search can be made up to 10x faster in practice. Our method assumes that the scoring function is monotonic in the sequence length, which allows us to safely prune hypotheses that cannot be in the final set of hypotheses early on. We devise effective monotonic approximations to popular nonmonontic scoring functions, including length normalization and mutual information decoding. Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar beneficial search bias in terms of downstream performance, but runs in a fraction of the time.","Cambridge, MA","Meister, Clara  and
Vieira, Tim  and
Cotterell, Ryan",,10.1162/tacl_a_00346,,795--809,MIT Press,Best-First Beam Search,https://aclanthology.org/2020.tacl-1.51,2020,Transactions of the Association for Computational Linguistics,8,,,
350,inproceedings,gantt-etal-2020-natural,"There is growing evidence that the prevalence of disagreement in the raw annotations used to construct natural language inference datasets makes the common practice of aggregating those annotations to a single label problematic. We propose a generic method that allows one to skip the aggregation step and train on the raw annotations directly without subjecting the model to unwanted noise that can arise from annotator response biases. We demonstrate that this method, which generalizes the notion of a mixed effects model by incorporating annotator random effects into any existing neural model, improves performance over models that do not incorporate such effects.","Barcelona, Spain (Online)","Gantt, William  and
Kane, Benjamin  and
White, Aaron Steven",Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,,December,81--87,Association for Computational Linguistics,Natural Language Inference with Mixed Effects,https://aclanthology.org/2020.starsem-1.9,2020,,,,,
351,inproceedings,casas-etal-2020-syntax,"The dominant language modeling paradigm handles text as a sequence of discrete tokens. While that approach can capture the latent structure of the text, it is inherently constrained to sequential dynamics for text generation. We propose a new paradigm for introducing a syntactic inductive bias into neural text generation, where the dependency parse tree is used to drive the Transformer model to generate sentences iteratively. Our experiments show that this paradigm is effective at text generation, with quality between LSTMs and Transformers, and comparable diversity, requiring less than half their decoding steps, and its generation process allows direct control over the syntactic constructions of the generated text, enabling the induction of stylistic variations.",Online,"Casas, Noe  and
Fonollosa, Jos{\'e} A. R.  and
Costa-juss{\`a}, Marta R.",Proceedings of the Fourth Workshop on Structured Prediction for NLP,10.18653/v1/2020.spnlp-1.1,November,1--10,Association for Computational Linguistics,Syntax-driven Iterative Expansion Language Models for Controllable Text Generation,https://aclanthology.org/2020.spnlp-1.1,2020,,,,,
352,inproceedings,mitra-etal-2020-deeply,"Successful application of Knowledge Representation and Reasoning (KR) in Natural Language Understanding (NLU) is largely limited by the availability of a robust and general purpose natural language parser. Even though several projects have been launched in the pursuit of developing a universal meaning representation language, the existence of an accurate universal parser is far from reality. This has severely limited the application of knowledge representation and reasoning (KR) in the field of NLP and also prevented a proper evaluation of KR based NLU systems. Our goal is to build KR based systems for Natural Language Understanding without relying on a parser. Towards this we propose a method named Deeply Embedded Knowledge Representation {\&} Reasoning (DeepEKR) where we replace the parser by a neural network, soften the symbolic representation so that a deterministic mapping exists between the parser neural network and the interpretable logical form, and finally replace the symbolic solver by an equivalent neural network, so the model can be trained end-to-end. We evaluate our method with respect to the task of Qualitative Word Problem Solving on the two available datasets (QuaRTz and QuaRel). Our system achieves same accuracy as that of the state-of-the-art accuracy on QuaRTz, outperforms the state-of-the-art on QuaRel and severely outperforms a traditional KR based system. The results show that the bias introduced by a KR solution does not prevent it from doing a better job at the end task. Moreover, our method is interpretable due to the bias introduced by the KR approach.",Online,"Mitra, Arindam  and
Narayana, Sanjay  and
Baral, Chitta",Proceedings of the Fourth Workshop on Structured Prediction for NLP,10.18653/v1/2020.spnlp-1.12,November,102--111,Association for Computational Linguistics,Deeply Embedded Knowledge Representation {\&} Reasoning For Natural Language Question Answering: A Practitioner{'}s Perspective,https://aclanthology.org/2020.spnlp-1.12,2020,,,,,
353,inproceedings,kameswari-etal-2020-enhancing,"Usage of presuppositions in social media and news discourse can be a powerful way to influence the readers as they usually tend to not examine the truth value of the hidden or indirectly expressed information. Fairclough and Wodak (1997) discuss presupposition at a discourse level where some implicit claims are taken for granted in the explicit meaning of a text or utterance. From the Gricean perspective, the presuppositions of a sentence determine the class of contexts in which the sentence could be felicitously uttered. This paper aims to correlate the type of knowledge presupposed in a news article to the bias present in it. We propose a set of guidelines to identify various kinds of presuppositions in news articles and present a dataset consisting of 1050 articles which are annotated for bias (positive, negative or neutral) and the magnitude of presupposition. We introduce a supervised classification approach for detecting bias in political news which significantly outperforms the existing systems.",Online,"Kameswari, Lalitha  and
Sravani, Dama  and
Mamidi, Radhika",Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,10.18653/v1/2020.socialnlp-1.1,July,1--6,Association for Computational Linguistics,Enhancing Bias Detection in Political News Using Pragmatic Presupposition,https://aclanthology.org/2020.socialnlp-1.1,2020,,,,,
354,inproceedings,jain-etal-2020-identifying,"In this work, we study collaborative online conversations. Such conversations are rich in content, constructive and motivated by a shared goal. Automatically identifying such conversations requires modeling complex discourse behaviors, which characterize the flow of information, sentiment and community structure within discussions. To help capture these behaviors, we define a hybrid relational model in which relevant discourse behaviors are formulated as discrete latent variables and scored using neural networks. These variables provide the information needed for predicting the overall collaborative characterization of the entire conversational thread. We show that adding inductive bias in the form of latent variables results in performance improvement, while providing a natural way to explain the decision.",1st virtual meeting,"Jain, Ayush  and
Pacheco, Maria Leonor  and
Lancette, Steven  and
Goindani, Mahak  and
Goldwasser, Dan",Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,74--78,Association for Computational Linguistics,Identifying Collaborative Conversations using Latent Discourse Behaviors,https://aclanthology.org/2020.sigdial-1.10,2020,,,,,
355,inproceedings,hewitt-beaver-2020-case,"We investigate differences in user communication with live chat agents versus a commercial Intelligent Virtual Agent (IVA). This case study compares the two types of interactions in the same domain for the same company filling the same purposes. We compared 16,794 human-to-human conversations and 27,674 conversations with the IVA. Of those IVA conversations, 8,324 escalated to human live chat agents. We then investigated how human-to-human communication strategies change when users first communicate with an IVA in the same conversation thread. We measured quantity, quality, and diversity of language, and analyzed complexity using numerous features. We find that while the complexity of language did not significantly change between modes, the quantity and some quality metrics did vary significantly. This fair comparison provides unique insight into how humans interact with commercial IVAs and how IVA and chatbot designers might better curate training data when automating customer service tasks.",1st virtual meeting,"Hewitt, Timothy  and
Beaver, Ian",Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,79--85,Association for Computational Linguistics,A Case Study of User Communication Styles with Customer Service Agents versus Intelligent Virtual Agents,https://aclanthology.org/2020.sigdial-1.11,2020,,,,,
356,inproceedings,finch-choi-2020-towards,"As conversational AI-based dialogue management has increasingly become a trending topic, the need for a standardized and reliable evaluation procedure grows even more pressing. The current state of affairs suggests various evaluation protocols to assess chat-oriented dialogue management systems, rendering it difficult to conduct fair comparative studies across different approaches and gain an insightful understanding of their values. To foster this research, a more robust evaluation protocol must be set in place. This paper presents a comprehensive synthesis of both automated and human evaluation methods on dialogue systems, identifying their shortcomings while accumulating evidence towards the most effective evaluation dimensions. A total of 20 papers from the last two years are surveyed to analyze three types of evaluation protocols: automated, static, and interactive. Finally, the evaluation dimensions used in these papers are compared against our expert evaluation on the system-user dialogue data collected from the Alexa Prize 2020.",1st virtual meeting,"Finch, Sarah E.  and
Choi, Jinho D.",Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,,July,236--245,Association for Computational Linguistics,Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of Current Evaluation Protocols,https://aclanthology.org/2020.sigdial-1.29,2020,,,,,
357,inproceedings,kong-etal-2020-hpcc,"It is fairly common to use code-mixing on a social media platform to express opinions and emotions in multilingual societies. The purpose of this task is to detect the sentiment of code-mixed social media text. Code-mixed text poses a great challenge for the traditional NLP system, which currently uses monolingual resources to deal with the problem of multilingual mixing. This task has been solved in the past using lexicon lookup in respective sentiment dictionaries and using a long short-term memory (LSTM) neural network for monolingual resources. In this paper, we present a system that uses a bilingual vector gating mechanism for bilingual resources to complete the task. The model consists of two main parts: the vector gating mechanism, which combines the character and word levels, and the attention mechanism, which extracts the important emotional parts of the text. The results show that the proposed system outperforms the baseline algorithm. We achieved fifth place in Spanglish and 19th place in Hinglish.",Barcelona (online),"Kong, Jun  and
Wang, Jin  and
Zhang, Xuejie",Proceedings of the Fourteenth Workshop on Semantic Evaluation,10.18653/v1/2020.semeval-1.120,December,940--945,International Committee for Computational Linguistics,{HPCC}-{YNU} at {S}em{E}val-2020 Task 9: A Bilingual Vector Gating Mechanism for Sentiment Analysis of Code-Mixed Text,https://aclanthology.org/2020.semeval-1.120,2020,,,,,
358,inproceedings,gupta-etal-2020-compositionality,"Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.",Online,"Gupta, Abhinav  and
Resnick, Cinjon  and
Foerster, Jakob  and
Dai, Andrew  and
Cho, Kyunghyun",Proceedings of the 5th Workshop on Representation Learning for NLP,10.18653/v1/2020.repl4nlp-1.5,July,34--38,Association for Computational Linguistics,Compositionality and Capacity in Emergent Languages,https://aclanthology.org/2020.repl4nlp-1.5,2020,,,,,
359,inproceedings,cengiz-yuret-2020-joint,"End-to-end models trained on natural language inference (NLI) datasets show low generalization on out-of-distribution evaluation sets. The models tend to learn shallow heuristics due to dataset biases. The performance decreases dramatically on diagnostic sets measuring compositionality or robustness against simple heuristics. Existing solutions for this problem employ dataset augmentation which has the drawbacks of being applicable to only a limited set of adversaries and at worst hurting the model performance on other adversaries not included in the augmentation set. Instead, our proposed solution is to improve sentence understanding (hence out-of-distribution generalization) with joint learning of explicit semantics. We show that a BERT based model trained jointly on English semantic role labeling (SRL) and NLI achieves significantly higher performance on external evaluation sets measuring generalization performance.",Online,"Cengiz, Cemil  and
Yuret, Deniz",Proceedings of the 5th Workshop on Representation Learning for NLP,10.18653/v1/2020.repl4nlp-1.11,July,78--88,Association for Computational Linguistics,Joint Training with Semantic Role Labeling for Better Generalization in Natural Language Inference,https://aclanthology.org/2020.repl4nlp-1.11,2020,,,,,
360,inproceedings,toshniwal-etal-2020-cross,"Many natural language processing (NLP) tasks involve reasoning with textual spans, including question answering, entity recognition, and coreference resolution. While extensive research has focused on functional architectures for representing words and sentences, there is less work on representing arbitrary spans of text within sentences. In this paper, we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks, including two tasks that we introduce. We find that, although some simple span representations are fairly reliable across tasks, in general the optimal span representation varies by task, and can also vary within different facets of individual tasks. We also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.",Online,"Toshniwal, Shubham  and
Shi, Haoyue  and
Shi, Bowen  and
Gao, Lingyu  and
Livescu, Karen  and
Gimpel, Kevin",Proceedings of the 5th Workshop on Representation Learning for NLP,10.18653/v1/2020.repl4nlp-1.20,July,166--176,Association for Computational Linguistics,A Cross-Task Analysis of Text Span Representations,https://aclanthology.org/2020.repl4nlp-1.20,2020,,,,,
361,inproceedings,vincze-szabo-2020-automatic,"Online news do not always come from reliable sources and they are not always even realistic. The constantly growing number of online textual data has raised the need for detecting deception and bias in texts from different domains recently. In this paper, we identify different types of unrealistic news (clickbait and fake news written for entertainment purposes) written in Hungarian on the basis of a rich feature set and with the help of machine learning methods. Our tool achieves competitive scores: it is able to classify clickbait, fake news written for entertainment purposes and real news with an accuracy of over 80{\%}. It is also highlighted that morphological features perform the best in this classification task.","Barcelona, Spain (Online)","Vincze, Veronika  and
Szab{\'o}, Martina Katalin",Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM),,December,58--69,Association for Computational Linguistics,Automatic Detection of {H}ungarian Clickbait and Entertaining Fake News,https://aclanthology.org/2020.rdsm-1.6,2020,,,,,
362,inproceedings,johannssen-biemann-2020-social,"The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research.","Barcelona, Spain (Online)","Johann{\ss}en, Dirk  and
Biemann, Chris","Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",,December,74--86,Association for Computational Linguistics,Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises,https://aclanthology.org/2020.peoples-1.8,2020,,,,,
363,inproceedings,oberlander-etal-2020-experiencers,"Emotion recognition is predominantly formulated as text classification in which textual units are assigned to an emotion from a predefined inventory (e.g., fear, joy, anger, disgust, sadness, surprise, trust, anticipation). More recently, semantic role labeling approaches have been developed to extract structures from the text to answer questions like: {``}who is described to feel the emotion?{''} (experiencer), {``}what causes this emotion?{''} (stimulus), and at which entity is it directed?{''} (target). Though it has been shown that jointly modeling stimulus and emotion category prediction is beneficial for both subtasks, it remains unclear which of these semantic roles enables a classifier to infer the emotion. Is it the experiencer, because the identity of a person is biased towards a particular emotion (X is always happy)? Is it a particular target (everybody loves X) or a stimulus (doing X makes everybody sad)? We answer these questions by training emotion classification models on five available datasets annotated with at least one semantic role by masking the fillers of these roles in the text in a controlled manner and find that across multiple corpora, stimuli and targets carry emotion information, while the experiencer might be considered a confounder. Further, we analyze if informing the model about the position of the role improves the classification decision. Particularly on literature corpora we find that the role information improves the emotion classification.","Barcelona, Spain (Online)","Oberl{\""a}nder, Laura Ana Maria  and
Reich, Kevin  and
Klinger, Roman","Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",,December,119--128,Association for Computational Linguistics,"Experiencers, Stimuli, or Targets: Which Semantic Roles Enable Machine Learning to Infer the Emotions?",https://aclanthology.org/2020.peoples-1.12,2020,,,,,
364,inproceedings,rodven-eide-2020-anforanden,"The Swedish parliamentary debates have been available since 2010 through the parliament{'}s open data web site Riksdagens {\""o}ppna data. While fairly comprehensive, the structure of the data can be hard to understand and its content is somewhat noisy for use as a quality language resource. In order to make them easier to use and process {--} in particular for language technology research, but also for political science and other fields with an interest in parliamentary data {--} we have published a large selection of the debates in a cleaned and structured format, annotated with linguistic information and augmented with semantic links. Especially prevalent in the parliament{'}s data were end-line hyphenations {--} something that tokenisers generally are not equipped for {--} and a lot of the effort went into resolving these. In this paper, we provide detailed descriptions of the structure and contents of the resource, and explain how it differs from the parliament{'}s own version.","Marseille, France","R{\o}dven Eide, Stian",Proceedings of the Second ParlaCLARIN Workshop,,May,5--10,European Language Resources Association,"{A}nf{\""o}randen: Annotated and Augmented Parliamentary Debates from {S}weden",https://aclanthology.org/2020.parlaclarin-1.2,2020,,,,English,979-10-95546-47-4
365,inproceedings,blaette-etal-2020-europeanization,"Corpora of plenary debates in national parliaments are available for many European states. For comparative research on political discourse, a persisting problem is that the periods covered by corpora differ and that a lack of standardization of data formats inhibits the integration of corpora into a single analytical framework. The solution we pursue is a {`}Framework for Parsing Plenary Protocols{'} (frappp), which has been used to prepare corpora of the Assembl{\'e}e Nationale ({`}{`}ParisParl{''}), the German Bundestag ({`}{`}GermaParl{''}), the Tweede Kamer of the Netherlands ({`}{`}TweedeTwee{''}), and the Austrian Nationalrat ({`}{`}AustroParl{''}) for the first two decades of the 21st century (2000-2019). To demonstrate the usefulness of the data gained, we investigate the Europeanization of migration debates in these Western European countries of immigration, i.e. references to a European dimension of policy-making in speeches on migration and integration. Based on a segmentation of the corpora into speeches, the method we use is topic modeling, and the analysis of joint occurrences of topics indicating migration and European affairs, respectively. A major finding is that after 2015, we see an increasing Europeanization of migration debates in the small EU member states in our sample (Austria and the Netherlands), and a regression of respective Europeanization in France and {--} more notably {--} in Germany.","Marseille, France","Blaette, Andreas  and
Gehlhar, Simon  and
Leonhardt, Christoph",Proceedings of the Second ParlaCLARIN Workshop,,May,66--74,European Language Resources Association,"The {E}uropeanization of Parliamentary Debates on Migration in {A}ustria, {F}rance, {G}ermany, and the {N}etherlands",https://aclanthology.org/2020.parlaclarin-1.12,2020,,,,English,979-10-95546-47-4
366,inproceedings,noble-etal-2020-personae,"In this paper, we propose a probabilistic model of social signalling which adopts a persona-based account of social meaning. We use this model to develop a socio-semantic theory of conventionalised reasoning patterns, known as topoi. On this account the social meaning of a topos, as conveyed in a argument, is based on the set of idealogically-related topoi it indicates in context. We draw a connection between the role of personae in social meaning and the category adjustment effect, a well-known psychological phenomenon in which the representation of a stimulus is biased in the direction of the category in which it falls. Finally, we situate the interpretation of social signals as an update to the information state of an agent in a formal TTR model of dialogue.",Gothenburg,"Noble, Bill  and
Breitholtz, Ellen  and
Cooper, Robin",Proceedings of the Probability and Meaning Conference (PaM 2020),,June,8--16,Association for Computational Linguistics,Personae under uncertainty: The case of topoi,https://aclanthology.org/2020.pam-1.2,2020,,,,,
367,inproceedings,abu-farha-magdy-2020-arabic,"Sarcasm is one of the main challenges for sentiment analysis systems. Its complexity comes from the expression of opinion using implicit indirect phrasing. In this paper, we present ArSarcasm, an Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets. The dataset contains 10,547 tweets, 16{\%} of which are sarcastic. In addition to sarcasm the data was annotated for sentiment and dialects. Our analysis shows the highly subjective nature of these tasks, which is demonstrated by the shift in sentiment labels based on annotators{'} biases. Experiments show the degradation of state-of-the-art sentiment analysers when faced with sarcastic content. Finally, we train a deep learning model for sarcasm detection using BiLSTM. The model achieves an F1 score of 0.46, which shows the challenging nature of the task, and should act as a basic baseline for future research on our dataset.","Marseille, France","Abu Farha, Ibrahim  and
Magdy, Walid","Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",,May,32--39,European Language Resource Association,From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset,https://aclanthology.org/2020.osact-1.5,2020,,,,English,979-10-95546-51-1
368,inproceedings,zad-finlayson-2020-systematic,"Identifying emotions as expressed in text (a.k.a. text emotion recognition) has received a lot of attention over the past decade. Narratives often involve a great deal of emotional expression, and so emotion recognition on narrative text is of great interest to computational approaches to narrative understanding. Prior work by Kim et al. 2010 was the work with the highest reported emotion detection performance, on a corpus of fairy tales texts. Close inspection of that work, however, revealed significant reproducibility problems, and we were unable to reimplement Kim{'}s approach as described. As a consequence, we implemented a framework inspired by Kim{'}s approach, where we carefully evaluated the major design choices. We identify the highest-performing combination, which outperforms Kim{'}s reported performance by 7.6 $F_1$ points on average. Close inspection of the annotated data revealed numerous missing and incorrect emotion terms in the relevant lexicon, WordNetAffect (WNA; Strapparava and Valitutti, 2004), which allowed us to augment it in a useful way. More generally, this showed that numerous clearly emotive words and phrases are missing from WNA, which suggests that effort invested in augmenting or refining emotion ontologies could be useful for improving the performance of emotion recognition systems. We release our code and data to definitely enable future reproducibility of this work.",Online,"Zad, Samira  and
Finlayson, Mark","Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events",10.18653/v1/2020.nuse-1.4,July,26--37,Association for Computational Linguistics,Systematic Evaluation of a Framework for Unsupervised Emotion Recognition for Narrative Text,https://aclanthology.org/2020.nuse-1.4,2020,,,,,
369,inproceedings,belyy-van-durme-2020-script,"We show that the count-based Script Induction models of Chambers and Jurafsky (2008) and Jans et al. (2012) can be unified in a general framework of narrative chain likelihood maximization. We provide efficient algorithms based on Association Rule Mining (ARM) and weighted set cover that can discover interesting patterns in the training data and combine them in a reliable and explainable way to predict the missing event. The proposed method, unlike the prior work, does not assume full conditional independence and makes use of higher-order count statistics. We perform the ablation study and conclude that the inductive biases introduced by ARM are conducive to better performance on the narrative cloze test.",Online,"Belyy, Anton  and
Van Durme, Benjamin","Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events",10.18653/v1/2020.nuse-1.7,July,55--62,Association for Computational Linguistics,Script Induction as Association Rule Mining,https://aclanthology.org/2020.nuse-1.7,2020,,,,,
370,inproceedings,shahid-etal-2020-detecting,"We describe work in progress on detecting and understanding the moral biases of news sources by combining framing theory with natural language processing. First we draw connections between issue-specific frames and moral frames that apply to all issues. Then we analyze the connection between moral frame presence and news source political leaning. We develop and test a simple classification model for detecting the presence of a moral frame, highlighting the need for more sophisticated models. We also discuss some of the annotation and frame detection challenges that can inform future research in this area.",Online,"Shahid, Usman  and
Di Eugenio, Barbara  and
Rojecki, Andrew  and
Zheleva, Elena","Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events",10.18653/v1/2020.nuse-1.15,July,120--125,Association for Computational Linguistics,Detecting and understanding moral biases in news,https://aclanthology.org/2020.nuse-1.15,2020,,,,,
371,inproceedings,lee-lee-2020-lxper,"Developing a text readability assessment model specifically for texts in a foreign English Language Training (ELT) curriculum has never had much attention in the field of Natural Language Processing. Hence, most developed models show extremely low accuracy for L2 English texts, up to the point where not many even serve as a fair comparison. In this paper, we investigate a text readability assessment model for L2 English learners in Korea. In accordance, we improve and expand the Text Corpus of the Korean ELT curriculum (CoKEC-text). Each text is labeled with its target grade level. We train our model with CoKEC-text and significantly improve the accuracy of readability assessment for texts in the Korean ELT curriculum.","Suzhou, China","Lee, Bruce W.  and
Lee, Jason",Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,,December,20--24,Association for Computational Linguistics,{LXPER} Index 2.0: Improving Text Readability Assessment Model for {L}2 {E}nglish Students in {K}orea,https://aclanthology.org/2020.nlptea-1.3,2020,,,,,
372,inproceedings,kumar-etal-2020-fair,"Non-contextual word embedding models have been shown to inherit human-like stereotypical biases of gender, race and religion from the training corpora. To counter this issue, a large body of research has emerged which aims to mitigate these biases while keeping the syntactic and semantic utility of embeddings intact. This paper describes Fair Embedding Engine (FEE), a library for analysing and mitigating gender bias in word embeddings. FEE combines various state of the art techniques for quantifying, visualising and mitigating gender bias in word embeddings under a standard abstraction. FEE will aid practitioners in fast track analysis of existing debiasing methods on their embedding models. Further, it will allow rapid prototyping of new methods by evaluating their performance on a suite of standard metrics.",Online,"Kumar, Vaibhav  and
Bhotia, Tenzin  and
Kumar, Vaibhav",Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),10.18653/v1/2020.nlposs-1.5,November,26--31,Association for Computational Linguistics,Fair Embedding Engine: A Library for Analyzing and Mitigating Gender Bias in Word Embeddings,https://aclanthology.org/2020.nlposs-1.5,2020,,,,,
373,inproceedings,lester-2020-iobes,"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each token is given a label and these labels are prefixed with special tokens such as B- or I-. After a model assigns labels to each token, these prefixes are used to group the tokens into spans. Properly parsing these annotations is critical for producing fair and comparable metrics; however, despite its importance, there is not an easy-to-use, standardized, programmatically integratable library to help work with span labeling. To remedy this, we introduce our open-source library, iobes. iobes is used for parsing, converting, and processing spans represented as token level decisions.",Online,"Lester, Brian",Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),10.18653/v1/2020.nlposs-1.16,November,115--119,Association for Computational Linguistics,iobes: Library for Span Level Processing,https://aclanthology.org/2020.nlposs-1.16,2020,,,,,
374,inproceedings,dunn-etal-2020-measuring,"Computational measures of linguistic diversity help us understand the linguistic landscape using digital language data. The contribution of this paper is to calibrate measures of linguistic diversity using restrictions on international travel resulting from the COVID-19 pandemic. Previous work has mapped the distribution of languages using geo-referenced social media and web data. The goal, however, has been to describe these corpora themselves rather than to make inferences about underlying populations. This paper shows that a difference-in-differences method based on the Herfindahl-Hirschman Index can identify the bias in digital corpora that is introduced by non-local populations. These methods tell us where significant changes have taken place and whether this leads to increased or decreased diversity. This is an important step in aligning digital corpora like social media with the real-world populations that have produced them.",Online,"Dunn, Jonathan  and
Coupe, Tom  and
Adams, Benjamin",Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/2020.nlpcss-1.1,November,1--10,Association for Computational Linguistics,Measuring Linguistic Diversity During {COVID}-19,https://aclanthology.org/2020.nlpcss-1.1,2020,,,,,
375,inproceedings,gupta-etal-2020-viable,"Recent advancements in natural language generation has raised serious concerns. High-performance language models are widely used for language generation tasks because they are able to produce fluent and meaningful sentences. These models are already being used to create fake news. They can also be exploited to generate biased news, which can then be used to attack news aggregators to change their reader{'}s behavior and influence their bias. In this paper, we use a threat model to demonstrate that the publicly available language models can reliably generate biased news content based on an input original news. We also show that a large number of high-quality biased news articles can be generated using controllable text generation. A subjective evaluation with 80 participants demonstrated that the generated biased news is generally fluent, and a bias evaluation with 24 participants demonstrated that the bias (left or right) is usually evident in the generated articles and can be easily identified.",Online,"Gupta, Saurabh  and
Nguyen, Hong Huy  and
Yamagishi, Junichi  and
Echizen, Isao",Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/2020.nlpcss-1.7,November,55--65,Association for Computational Linguistics,Viable Threat on News Reading: Generating Biased News Using Natural Language Models,https://aclanthology.org/2020.nlpcss-1.7,2020,,,,,
376,inproceedings,schmahl-etal-2020-wikipedia,"Large text corpora used for creating word embeddings (vectors which represent word meanings) often contain stereotypical gender biases. As a result, such unwanted biases will typically also be present in word embeddings derived from such corpora and downstream applications in the field of natural language processing (NLP). To minimize the effect of gender bias in these settings, more insight is needed when it comes to where and how biases manifest themselves in the text corpora employed. This paper contributes by showing how gender bias in word embeddings from Wikipedia has developed over time. Quantifying the gender bias over time shows that art related words have become more female biased. Family and science words have stereotypical biases towards respectively female and male words. These biases seem to have decreased since 2006, but these changes are not more extreme than those seen in random sets of words. Career related words are more strongly associated with male than with female, this difference has only become smaller in recently written articles. These developments provide additional understanding of what can be done to make Wikipedia more gender neutral and how important time of writing can be when considering biases in word embeddings trained from Wikipedia or from other text corpora.",Online,"Schmahl, Katja Geertruida  and
Viering, Tom Julian  and
Makrodimitris, Stavros  and
Naseri Jahfari, Arman  and
Tax, David  and
Loog, Marco",Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/2020.nlpcss-1.11,November,94--103,Association for Computational Linguistics,Is {W}ikipedia succeeding in reducing gender bias? Assessing changes in gender bias in {W}ikipedia using word embeddings,https://aclanthology.org/2020.nlpcss-1.11,2020,,,,,
377,inproceedings,chen-etal-2020-analyzing,"Media is an indispensable source of information and opinion, shaping the beliefs and attitudes of our society. Obviously, media portals can also provide overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such a form of unfair news coverage can be exposed. This paper addresses the automatic detection of bias, but it goes one step further in that it explores how political bias and unfairness are manifested linguistically. We utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com to develop a neural model for bias assessment. Analyzing the model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.",Online,"Chen, Wei-Fan  and
Al Khatib, Khalid  and
Wachsmuth, Henning  and
Stein, Benno",Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/2020.nlpcss-1.16,November,149--154,Association for Computational Linguistics,Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity,https://aclanthology.org/2020.nlpcss-1.16,2020,,,,,
378,inproceedings,gala-etal-2020-analyzing,"Popular media reflects and reinforces societal biases through the use of tropes, which are narrative elements, such as archetypal characters and plot arcs, that occur frequently across media. In this paper, we specifically investigate gender bias within a large collection of tropes. To enable our study, we crawl tvtropes.org, an online user-created repository that contains 30K tropes associated with 1.9M examples of their occurrences across film, television, and literature. We automatically score the {``}genderedness{''} of each trope in our TVTROPES dataset, which enables an analysis of (1) highly-gendered topics within tropes, (2) the relationship between gender bias and popular reception, and (3) how the gender of a work{'}s creator correlates with the types of tropes that they use.",Online,"Gala, Dhruvil  and
Khursheed, Mohammad Omar  and
Lerner, Hannah  and
O{'}Connor, Brendan  and
Iyyer, Mohit",Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/2020.nlpcss-1.23,November,212--217,Association for Computational Linguistics,Analyzing Gender Bias within Narrative Tropes,https://aclanthology.org/2020.nlpcss-1.23,2020,,,,,
379,inproceedings,chapman-etal-2020-natural,"Timely and accurate accounting of positive cases has been an important part of the response to the COVID-19 pandemic. While most positive cases within Veterans Affairs (VA) are identified through structured laboratory results, some patients are tested or diagnosed outside VA so their clinical status is documented only in free-text narratives. We developed a Natural Language Processing pipeline for identifying positively diagnosed COVID19 patients and deployed this system to accelerate chart review. As part of the VA national response to COVID-19, this process identified 6,360 positive cases which did not have corresponding laboratory data. These cases accounted for 36.1{\%} of total confirmed positive cases in VA to date. With available data, performance of the system is estimated as 82.4{\%} precision and 94.2{\%} recall. A public-facing implementation is released as open source and available to the community.",Online,"Chapman, Alec  and
Peterson, Kelly  and
Turano, Augie  and
Box, Tam{\'a}ra  and
Wallace, Katherine  and
Jones, Makoto",Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,,July,,Association for Computational Linguistics,A Natural Language Processing System for National {COVID-19} Surveillance in the {US Department of Veterans Affairs},https://aclanthology.org/2020.nlpcovid19-acl.10,2020,,,,,
380,inproceedings,niven-kao-2020-measuring,"We introduce what is to the best of our knowledge a new task in natural language processing: measuring alignment to authoritarian state media. We operationalize alignment in terms of sociological definitions of media bias. We take as a case study the alignment of four Taiwanese media outlets to the Chinese Communist Party state media. We present the results of an initial investigation using the frequency of words in psychologically meaningful categories. Our findings suggest that the chosen word categories correlate with framing choices. We develop a calculation method that yields reasonable results for measuring alignment, agreeing well with the known labels. We confirm that our method does capture event selection bias, but whether it captures framing bias requires further investigation.","Barcelona, Spain (Online)","Niven, Timothy  and
Kao, Hung-Yu","Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",,December,11--21,International Committee on Computational Linguistics (ICCL),Measuring Alignment to Authoritarian State Media as Framing Bias,https://aclanthology.org/2020.nlp4if-1.2,2020,,,,,
381,inproceedings,ntoutsi-2020-bias,"Algorithmic-based decision making powered via AI and (big) data has already penetrated into almost all spheres of human life, from content recommendation and healthcare to predictive policing and autonomous driving, deeply affecting everyone, anywhere, anytime. While technology allows previously unthinkable optimizations in the automation of expensive human decision making, the risks that the technology can pose are also high, leading to an ever increasing public concern about the impact of the technology in our lives. The area of responsible AI has recently emerged in an attempt to put humans at the center of AI-based systems by considering aspects, such as fairness, reliability and privacy of decision-making systems. In this talk, we will focus on the fairness aspect. We will start with understanding the many sources of bias and how biases can enter at each step of the learning process and even get propagated/amplified from previous steps. We will continue with methods for mitigating bias which typically focus on some step of the pipeline (data, algorithms or results) and why it is important to target bias in each step and collectively, in the whole (machine) learning pipeline. We will conclude this talk by discussing accountability issues in connection to bias and in particular, proactive consideration via bias-aware data collection, processing and algorithmic selection and retroactive consideration via explanations.","Dublin, Ireland","Ntoutsi, Eirini",2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,,November,3--4,Association for Computational Linguistics,Bias in {AI}-systems: A multi-step approach,https://aclanthology.org/2020.nl4xai-1.2,2020,,,,,
382,inproceedings,rieger-etal-2020-toward,"Cognitive biases in the context of consuming online information filtered by recommender systems may lead to sub-optimal choices. One approach to mitigate such biases is through interface and interaction design. This survey reviews studies focused on cognitive bias mitigation of recommender system users during two processes: 1) item selection and 2) preference elicitation. It highlights a number of promising directions for Natural Language Generation research for mitigating cognitive bias including: the need for personalization, as well as for transparency and control.","Dublin, Ireland","Rieger, Alisa  and
Theune, Mari{\""e}t  and
Tintarev, Nava",2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,,November,50--54,Association for Computational Linguistics,Toward Natural Language Mitigation Strategies for Cognitive Biases in Recommender Systems,https://aclanthology.org/2020.nl4xai-1.11,2020,,,,,
383,inproceedings,aji-heafield-2020-compressing,"Neural Machine Translation (NMT) is resource-intensive. We design a quantization procedure to compress fit NMT models better for devices with limited hardware capability. We use logarithmic quantization, instead of the more commonly used fixed-point quantization, based on the empirical fact that parameters distribution is not uniform. We find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. We also propose to use an error-feedback mechanism during retraining, to preserve the compressed model as a stale gradient. We empirically show that NMT models based on Transformer or RNN architecture can be compressed up to 4-bit precision without any noticeable quality degradation. Models can be compressed up to binary precision, albeit with lower quality. RNN architecture seems to be more robust towards compression, compared to the Transformer.",Online,"Aji, Alham Fikri  and
Heafield, Kenneth",Proceedings of the Fourth Workshop on Neural Generation and Translation,10.18653/v1/2020.ngt-1.4,July,35--42,Association for Computational Linguistics,Compressing Neural Machine Translation Models with 4-bit Precision,https://aclanthology.org/2020.ngt-1.4,2020,,,,,
384,inproceedings,bernard-han-2020-mandarinograd,"This article introduces Mandarinograd, a corpus of Winograd Schemas in Mandarin Chinese. Winograd Schemas are particularly challenging anaphora resolution problems, designed to involve common sense reasoning and to limit the biases and artefacts commonly found in natural language understanding datasets. Mandarinograd contains the schemas in their traditional form, but also as natural language inference instances (ENTAILMENT or NO ENTAILMENT pairs) as well as in their fully disambiguated candidate forms. These two alternative representations are often used by modern solvers but existing datasets present automatically converted items that sometimes contain syntactic or semantic anomalies. We detail the difficulties faced when building this corpus and explain how weavoided the anomalies just mentioned. We also show that Mandarinograd is resistant to a statistical method based on a measure of word association.","Marseille, France","Bernard, Timoth{\'e}e  and
Han, Ting",Proceedings of the 12th Language Resources and Evaluation Conference,,May,21--26,European Language Resources Association,{M}andarinograd: A {C}hinese Collection of {W}inograd Schemas,https://aclanthology.org/2020.lrec-1.3,2020,,,,English,979-10-95546-34-4
385,inproceedings,sato-miyazawa-2020-quality,"The quality estimation of artifacts generated by creators via crowdsourcing has great significance for the construction of a large-scale data resource. A common approach to this problem is to ask multiple reviewers to evaluate the same artifacts. However, the commonly used majority voting method to aggregate reviewers{'} evaluations does not work effectively for partially subjective or purely subjective tasks because reviewers{'} sensitivity and bias of evaluation tend to have a wide variety. To overcome this difficulty, we propose a probabilistic model for subjective classification tasks that incorporates the qualities of artifacts as well as the abilities and biases of creators and reviewers as latent variables to be jointly inferred. We applied this method to the partially subjective task of speech classification into the following four attitudes: agreement, disagreement, stalling, and question. The result shows that the proposed method estimates the quality of speech more effectively than a vote aggregation, measured by correlation with a fine-grained classification by experts.","Marseille, France","Sato, Yoshinao  and
Miyazawa, Kouki",Proceedings of the 12th Language Resources and Evaluation Conference,,May,229--235,European Language Resources Association,Quality Estimation for Partially Subjective Classification Tasks via Crowdsourcing,https://aclanthology.org/2020.lrec-1.29,2020,,,,English,979-10-95546-34-4
386,inproceedings,gomes-etal-2020-effort,"Named Entity Recognition (NER) is an essential component of many Natural Language Processing pipelines. However, building these language dependent models requires large amounts of annotated data. Crowdsourcing emerged as a scalable solution to collect and enrich data in a more time-efficient manner. To manage these annotations at scale, it is important to predict completion timelines and compute fair pricing for workers in advance. To achieve these goals, we need to know how much effort will be taken to complete each task. In this paper, we investigate which variables influence the time spent on a named entity annotation task by a human. Our results are two-fold: first, the understanding of the effort-impacting factors which we divided into cognitive load and input length; and second, the performance of the prediction itself. On the latter, through model adaptation and feature engineering, we attained a Root Mean Squared Error (RMSE) of 25.68 words per minute with a Nearest Neighbors model.","Marseille, France","Gomes, In{\^e}s  and
Correia, Rui  and
Ribeiro, Jorge  and
Freitas, Jo{\~a}o",Proceedings of the 12th Language Resources and Evaluation Conference,,May,298--306,European Language Resources Association,Effort Estimation in Named Entity Tagging Tasks,https://aclanthology.org/2020.lrec-1.37,2020,,,,English,979-10-95546-34-4
387,inproceedings,kontogiorgos-etal-2020-chinese,"In this paper, we introduce a multimodal dataset in which subjects are instructing each other how to assemble IKEA furniture. Using the concept of {`}Chinese Whispers{'}, an old children{'}s game, we employ a novel method to avoid implicit experimenter biases. We let subjects instruct each other on the nature of the task: the process of the furniture assembly. Uncertainty, hesitations, repairs and self-corrections are naturally introduced in the incremental process of establishing common ground. The corpus consists of 34 interactions, where each subject first assembles and then instructs. We collected speech, eye-gaze, pointing gestures, and object movements, as well as subjective interpretations of mutual understanding, collaboration and task recall. The corpus is of particular interest to researchers who are interested in multimodal signals in situated dialogue, especially in referential communication and the process of language grounding.","Marseille, France","Kontogiorgos, Dimosthenis  and
Sibirtseva, Elena  and
Gustafson, Joakim",Proceedings of the 12th Language Resources and Evaluation Conference,,May,743--749,European Language Resources Association,{C}hinese Whispers: A Multimodal Dataset for Embodied Language Grounding,https://aclanthology.org/2020.lrec-1.93,2020,,,,English,979-10-95546-34-4
388,inproceedings,rodier-carter-2020-online,"Near-duplicate documents are particularly common in news media corpora. Editors often update wirefeed articles to address space constraints in print editions or to add local context; journalists often lightly modify previous articles with new information or minor corrections. Near-duplicate documents have potentially significant costs, including bloating corpora with redundant information (biasing techniques built upon such corpora) and requiring additional human and computational analytic resources for marginal benefit. Filtering near-duplicates out of a collection is thus important, and is particularly challenging in applications that require them to be filtered out in real-time with high precision. Previous near-duplicate detection methods typically work offline to identify all near-duplicate pairs in a set of documents. We propose an online system which flags a near-duplicate document by finding its most likely original. This system adapts the shingling algorithm proposed by Broder (1997), and we test it on a challenging dataset of web-based news articles. Our online system presents state-of-the-art F1-scores, and can be tuned to trade precision for recall and vice-versa. Given its performance and online nature, our method can be used in many real-world applications. We present one such application, filtering near-duplicates to improve productivity of human analysts in a situational awareness tool.","Marseille, France","Rodier, Simon  and
Carter, Dave",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1242--1249,European Language Resources Association,Online Near-Duplicate Detection of News Articles,https://aclanthology.org/2020.lrec-1.156,2020,,,,English,979-10-95546-34-4
389,inproceedings,lazaridou-etal-2020-discovering,"Unbiased and fair reporting is an integral part of ethical journalism. Yet, political propaganda and one-sided views can be found in the news and can cause distrust in media. Both accidental and deliberate political bias affect the readers and shape their views. We contribute to a trustworthy media ecosystem by automatically identifying politically biased news articles. We introduce novel corpora annotated by two communities, i.e., domain experts and crowd workers, and we also consider automatic article labels inferred by the newspapers{'} ideologies. Our goal is to compare domain experts to crowd workers and also to prove that media bias can be detected automatically. We classify news articles with a neural network and we also improve our performance in a self-supervised manner.","Marseille, France","Lazaridou, Konstantina  and
L{\""o}ser, Alexander  and
Mestre, Maria  and
Naumann, Felix",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1268--1277,European Language Resources Association,Discovering Biased News Articles Leveraging Multiple Human Annotations,https://aclanthology.org/2020.lrec-1.159,2020,,,,English,979-10-95546-34-4
390,inproceedings,van-der-meulen-reijnierse-2020-factcorp,"Fact-checking information before publication has long been a core task for journalists, but recent times have seen the emergence of dedicated news items specifically aimed at fact-checking after publication. This relatively new form of fact-checking receives a fair amount of attention from academics, with current research focusing mostly on journalists{'} motivations for publishing post-hoc fact-checks, the effects of fact-checking on the perceived accuracy of false claims, and the creation of computational tools for automatic fact-checking. In this paper, we propose to study fact-checks from a corpus linguistic perspective. This will enable us to gain insight in the scope and contents of fact-checks, to investigate what fact-checks can teach us about the way in which science appears (incorrectly) in the news, and to see how fact-checks behave in the science communication landscape. We report on the creation of FactCorp, a 1,16 million-word corpus containing 1,974 fact-checks from three major Dutch newspapers. We also present results of several exploratory analyses, including a rhetorical moves analysis, a qualitative content elements analysis, and keyword analyses. Through these analyses, we aim to demonstrate the wealth of possible applications that FactCorp allows, thereby stressing the importance of creating such resources.","Marseille, France","van der Meulen, Marten  and
Reijnierse, W. Gudrun",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1286--1292,European Language Resources Association,{F}act{C}orp: A Corpus of {D}utch Fact-checks and its Multiple Usages,https://aclanthology.org/2020.lrec-1.161,2020,,,,English,979-10-95546-34-4
391,inproceedings,zotova-etal-2020-multilingual,"Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The TW-10 referendum Dataset released at IberEval 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the ndependence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the TW-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the TW-10 dataset, both for Catalan and Spanish.","Marseille, France","Zotova, Elena  and
Agerri, Rodrigo  and
Nu{\~n}ez, Manuel  and
Rigau, German",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1368--1375,European Language Resources Association,Multilingual Stance Detection in Tweets: The {C}atalonia Independence Corpus,https://aclanthology.org/2020.lrec-1.171,2020,,,,English,979-10-95546-34-4
392,inproceedings,cecillon-etal-2020-wac,"With the spread of online social networks, it is more and more difficult to monitor all the user-generated content. Automating the moderation process of the inappropriate exchange content on Internet has thus become a priority task. Methods have been proposed for this purpose, but it can be challenging to find a suitable dataset to train and develop them. This issue is especially true for approaches based on information derived from the structure and the dynamic of the conversation. In this work, we propose an original framework, based on the the Wikipedia Comment corpus, with comment-level abuse annotations of different types. The major contribution concerns the reconstruction of conversations, by comparison to existing corpora, which focus only on isolated messages (i.e. taken out of their conversational context). This large corpus of more than 380k annotated messages opens perspectives for online abuse detection and especially for context-based approaches. We also propose, in addition to this corpus, a complete benchmarking platform to stimulate and fairly compare scientific works around the problem of content abuse detection, trying to avoid the recurring problem of result replication. Finally, we apply two classification methods to our dataset to demonstrate its potential.","Marseille, France","C{\'e}cillon, No{\'e}  and
Labatut, Vincent  and
Dufour, Richard  and
Linar{\`e}s, Georges",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1382--1390,European Language Resources Association,{WAC}: A Corpus of {W}ikipedia Conversations for Online Abuse Detection,https://aclanthology.org/2020.lrec-1.173,2020,,,,English,979-10-95546-34-4
393,inproceedings,lim-etal-2020-annotating,"The spread of biased news and its consumption by the readers has become a considerable issue. Researchers from multiple domains including social science and media studies have made efforts to mitigate this media bias issue. Specifically, various techniques ranging from natural language processing to machine learning have been used to help determine news bias automatically. However, due to the lack of publicly available datasets in this field, especially ones containing labels concerning bias on a fine-grained level (e.g., on sentence level), it is still challenging to develop methods for effectively identifying bias embedded in new articles. In this paper, we propose a novel news bias dataset which facilitates the development and evaluation of approaches for detecting subtle bias in news articles and for understanding the characteristics of biased sentences. Our dataset consists of 966 sentences from 46 English-language news articles covering 4 different events and contains labels concerning bias on the sentence level. For scalability reasons, the labels were obtained based on crowd-sourcing. Our dataset can be used for analyzing news bias, as well as for developing and evaluating methods for news bias detection. It can also serve as resource for related researches including ones focusing on fake news detection.","Marseille, France","Lim, Sora  and
Jatowt, Adam  and
F{\""a}rber, Michael  and
Yoshikawa, Masatoshi",Proceedings of the 12th Language Resources and Evaluation Conference,,May,1478--1484,European Language Resources Association,Annotating and Analyzing Biased Sentences in News Articles using Crowdsourcing,https://aclanthology.org/2020.lrec-1.184,2020,,,,English,979-10-95546-34-4
394,inproceedings,dunn-adams-2020-geographically,"While text corpora have been steadily increasing in overall size, even very large corpora are not designed to represent global population demographics. For example, recent work has shown that existing English gigaword corpora over-represent inner-circle varieties from the US and the UK. To correct implicit geographic and demographic biases, this paper uses country-level population demographics to guide the construction of gigaword web corpora. The resulting corpora explicitly match the ground-truth geographic distribution of each language, thus equally representing language users from around the world. This is important because it ensures that speakers of under-resourced language varieties (i.e., Indian English or Algerian French) are represented, both in the corpora themselves but also in derivative resources like word embeddings.","Marseille, France","Dunn, Jonathan  and
Adams, Ben",Proceedings of the 12th Language Resources and Evaluation Conference,,May,2528--2536,European Language Resources Association,Geographically-Balanced {G}igaword Corpora for 50 Language Varieties,https://aclanthology.org/2020.lrec-1.308,2020,,,,English,979-10-95546-34-4
395,inproceedings,jungmaier-etal-2020-dirichlet,"Nowadays, classical count-based word embeddings using positive pointwise mutual information (PPMI) weighted co-occurrence matrices have been widely superseded by machine-learning-based methods like word2vec and GloVe. But these methods are usually applied using very large amounts of text data. In many cases, however, there is not much text data available, for example for specific domains or low-resource languages. This paper revisits PPMI by adding Dirichlet smoothing to correct its bias towards rare words. We evaluate on standard word similarity data sets and compare to word2vec and the recent state of the art for low-resource settings: Positive and Unlabeled (PU) Learning for word embeddings. The proposed method outperforms PU-Learning for low-resource settings and obtains competitive results for Maltese and Luxembourgish.","Marseille, France","Jungmaier, Jakob  and
Kassner, Nora  and
Roth, Benjamin",Proceedings of the 12th Language Resources and Evaluation Conference,,May,3560--3565,European Language Resources Association,{D}irichlet-Smoothed Word Embeddings for Low-Resource Settings,https://aclanthology.org/2020.lrec-1.437,2020,,,,English,979-10-95546-34-4
396,inproceedings,raganato-etal-2020-evaluation,"Lexical ambiguity is one of the many challenging linguistic phenomena involved in translation, i.e., translating an ambiguous word with its correct sense. In this respect, previous work has shown that the translation quality of neural machine translation systems can be improved by explicitly modeling the senses of ambiguous words. Recently, several evaluation test sets have been proposed to measure the word sense disambiguation (WSD) capability of machine translation systems. However, to date, these evaluation test sets do not include any training data that would provide a fair setup measuring the sense distributions present within the training data itself. In this paper, we present an evaluation benchmark on WSD for machine translation for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW.","Marseille, France","Raganato, Alessandro  and
Scherrer, Yves  and
Tiedemann, J{\""o}rg",Proceedings of the 12th Language Resources and Evaluation Conference,,May,3668--3675,European Language Resources Association,An Evaluation Benchmark for Testing the Word Sense Disambiguation Capabilities of Machine Translation Systems,https://aclanthology.org/2020.lrec-1.452,2020,,,,English,979-10-95546-34-4
397,inproceedings,fonteyne-etal-2020-literary,"Several studies (covering many language pairs and translation tasks) have demonstrated that translation quality has improved enormously since the emergence of neural machine translation systems. This raises the question whether such systems are able to produce high-quality translations for more creative text types such as literature and whether they are able to generate coherent translations on document level. Our study aimed to investigate these two questions by carrying out a document-level evaluation of the raw NMT output of an entire novel. We translated Agatha Christie{'}s novel The Mysterious Affair at Styles with Google{'}s NMT system from English into Dutch and annotated it in two steps: first all fluency errors, then all accuracy errors. We report on the overall quality, determine the remaining issues, compare the most frequent error types to those in general-domain MT, and investigate whether any accuracy and fluency errors co-occur regularly. Additionally, we assess the inter-annotator agreement on the first chapter of the novel.","Marseille, France","Fonteyne, Margot  and
Tezcan, Arda  and
Macken, Lieve",Proceedings of the 12th Language Resources and Evaluation Conference,,May,3790--3798,European Language Resources Association,Literary Machine Translation under the Magnifying Glass: Assessing the Quality of an {NMT}-Translated Detective Novel on Document Level,https://aclanthology.org/2020.lrec-1.468,2020,,,,English,979-10-95546-34-4
398,inproceedings,sahala-etal-2020-babyfst,"Akkadian is a fairly well resourced extinct language that does not yet have a comprehensive morphological analyzer available. In this paper we describe a general finite-state based morphological model for Babylonian, a southern dialect of the Akkadian language, that can achieve a coverage up to 97.3{\%} and recall up to 93.7{\%} on lemmatization and POS-tagging task on token level from a transcribed input. Since Akkadian word forms exhibit a high degree of morphological ambiguity, in that only 20.1{\%} of running word tokens receive a single unambiguous analysis, we attempt a first pass at weighting our finite-state transducer, using existing extensive Akkadian corpora which have been partially validated for their lemmas and parts-of-speech but not the entire morphological analyses. The resultant weighted finite-state transducer yields a moderate improvement so that for 57.4{\%} of the word tokens the highest ranked analysis is the correct one. We conclude with a short discussion on how morphological ambiguity in the analysis of Akkadian could be further reduced with improvements in the training data used in weighting the finite-state transducer as well as through other, context-based techniques.","Marseille, France","Sahala, Aleksi  and
Silfverberg, Miikka  and
Arppe, Antti  and
Lind{\'e}n, Krister",Proceedings of the 12th Language Resources and Evaluation Conference,,May,3886--3894,European Language Resources Association,{B}aby{FST} - Towards a Finite-State Based Computational Model of Ancient Babylonian,https://aclanthology.org/2020.lrec-1.479,2020,,,,English,979-10-95546-34-4
399,inproceedings,kanayama-iwamoto-2020-universal,"This paper investigates clause-level sentiment detection in a multilingual scenario. Aiming at a high-precision, fine-grained, configurable, and non-biased system for practical use cases, we have designed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, avoiding machine-learning approaches that may cause obstacles to our purposes. We achieved high precision in sentiment detection for 17 languages and identified the advantages of common syntactic structures as well as issues stemming from structural differences on Universal Dependencies. In addition to reusable tips for handling multilingual syntax, we provide a parallel benchmarking data set for further research.","Marseille, France","Kanayama, Hiroshi  and
Iwamoto, Ran",Proceedings of the 12th Language Resources and Evaluation Conference,,May,4063--4073,European Language Resources Association,How Universal are {U}niversal {D}ependencies? Exploiting Syntax for Multilingual Clause-level Sentiment Detection,https://aclanthology.org/2020.lrec-1.500,2020,,,,English,979-10-95546-34-4
400,inproceedings,kameswari-mamidi-2020-manovaad,"In today{'}s era of globalisation, the increased outreach for every event across the world has been leading to conflicting opinions, arguments and disagreements, often reflected in print media and online social platforms. It is necessary to distinguish factual observations from personal judgements in news, as subjectivity in reporting can influence the audience{'}s perception of reality. Several studies conducted on the different styles of reporting in journalism are essential in understanding phenomena such as media bias and multiple interpretations of the same event. This domain finds applications in fields such as Media Studies, Discourse Analysis, Information Extraction, Sentiment Analysis, and Opinion Mining. We present an event corpus Manovaad-v1.0 consisting of 1035 news articles corresponding to 65 events from 3 levels of newspapers viz., Local, National, and International levels. Using this novel format, we correlate the trends in the degree of subjectivity with the geographical closeness of reporting using a Bi-RNN model. We also analyse the role of background and focus in event reporting and capture the focus shift patterns within a global discourse structure for an event. We do this across different levels of reporting and compare the results with the existing work on discourse processing.","Marseille, France","Kameswari, Lalitha  and
Mamidi, Radhika",Proceedings of the 12th Language Resources and Evaluation Conference,,May,4948--4954,European Language Resources Association,{M}anovaad: A Novel Approach to Event Oriented Corpus Creation Capturing Subjectivity and Focus,https://aclanthology.org/2020.lrec-1.609,2020,,,,English,979-10-95546-34-4
401,inproceedings,anderson-gomez-rodriguez-2020-inherent,"A wide variety of transition-based algorithms are currently used for dependency parsers. Empirical studies have shown that performance varies across different treebanks in such a way that one algorithm outperforms another on one treebank and the reverse is true for a different treebank. There is often no discernible reason for what causes one algorithm to be more suitable for a certain treebank and less so for another. In this paper we shed some light on this by introducing the concept of an algorithm{'}s inherent dependency displacement distribution. This characterises the bias of the algorithm in terms of dependency displacement, which quantify both distance and direction of syntactic relations. We show that the similarity of an algorithm{'}s inherent distribution to a treebank{'}s displacement distribution is clearly correlated to the algorithm{'}s parsing performance on that treebank, specificially with highly significant and substantial correlations for the predominant sentence lengths in Universal Dependency treebanks. We also obtain results which show a more discrete analysis of dependency displacement does not result in any meaningful correlations.","Marseille, France","Anderson, Mark  and
G{\'o}mez-Rodr{\'\i}guez, Carlos",Proceedings of the 12th Language Resources and Evaluation Conference,,May,5147--5155,European Language Resources Association,Inherent Dependency Displacement Bias of Transition-Based Algorithms,https://aclanthology.org/2020.lrec-1.633,2020,,,,English,979-10-95546-34-4
402,inproceedings,oshikawa-etal-2020-survey,"Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Thus, the effect of fake news has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of fake news. In this paper, we describe the challenges involved in fake news detection and also describe related tasks. We systematically review and compare the task formulations, datasets and NLP solutions that have been developed for this task, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between fake news detection and other related tasks, and the importance of NLP solutions for fake news detection.","Marseille, France","Oshikawa, Ray  and
Qian, Jing  and
Wang, William Yang",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6086--6093,European Language Resources Association,A Survey on Natural Language Processing for Fake News Detection,https://aclanthology.org/2020.lrec-1.747,2020,,,,English,979-10-95546-34-4
403,inproceedings,gorisch-etal-2020-using,"The newest generation of speech technology caused a huge increase of audio-visual data nowadays being enhanced with orthographic transcripts such as in automatic subtitling in online platforms. Research data centers and archives contain a range of new and historical data, which are currently only partially transcribed and therefore only partially accessible for systematic querying. Automatic Speech Recognition (ASR) is one option of making that data accessible. This paper tests the usability of a state-of-the-art ASR-System on a historical (from the 1960s), but regionally balanced corpus of spoken German, and a relatively new corpus (from 2012) recorded in a narrow area. We observed a regional bias of the ASR-System with higher recognition scores for the north of Germany vs. lower scores for the south. A detailed analysis of the narrow region data revealed {--} despite relatively high ASR-confidence {--} some specific word errors due to a lack of regional adaptation. These findings need to be considered in decisions on further data processing and the curation of corpora, e.g. correcting transcripts or transcribing from scratch. Such geography-dependent analyses can also have the potential for ASR-development to make targeted data selection for training/adaptation and to increase the sensitivity towards varieties of pluricentric languages.","Marseille, France","Gorisch, Jan  and
Gref, Michael  and
Schmidt, Thomas",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6423--6428,European Language Resources Association,Using Automatic Speech Recognition in Spoken Corpus Curation,https://aclanthology.org/2020.lrec-1.790,2020,,,,English,979-10-95546-34-4
404,inproceedings,meyer-etal-2020-artie,"We describe the creation of the Artie Bias Corpus, an English dataset of expert-validated {\textless}audio, transcript{\textgreater} pairs with demographic tags for age, gender, accent. We also release open software which may be used with the Artie Bias Corpus to detect demographic bias in Automatic Speech Recognition systems, and can be extended to other speech technologies. The Artie Bias Corpus is a curated subset of the Mozilla Common Voice corpus, which we release under a Creative Commons CC0 license {--} the most open and permissive license for data. This article contains information on the criteria used to select and annotate the Artie Bias Corpus in addition to experiments in which we detect and attempt to mitigate bias in end-to-end speech recognition models. We we observe a significant accent bias in our baseline DeepSpeech model, with more accurate transcriptions of US English compared to Indian English. We do not, however, find evidence for a significant gender bias. We then show significant improvements on individual demographic groups from fine-tuning.","Marseille, France","Meyer, Josh  and
Rauchenstein, Lindy  and
Eisenberg, Joshua D.  and
Howell, Nicholas",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6462--6468,European Language Resources Association,Artie Bias Corpus: An Open Dataset for Detecting Demographic Bias in Speech Applications,https://aclanthology.org/2020.lrec-1.796,2020,,,,English,979-10-95546-34-4
405,inproceedings,garnerin-etal-2020-gender,"With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora.","Marseille, France","Garnerin, Mahault  and
Rossato, Solange  and
Besacier, Laurent",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6599--6605,European Language Resources Association,Gender Representation in Open Source Speech Resources,https://aclanthology.org/2020.lrec-1.813,2020,,,,English,979-10-95546-34-4
406,inproceedings,hayashibe-2020-japanese,"We perform the textual entailment (TE) corpus construction for the Japanese Language with the following three characteristics: First, the corpus consists of realistic sentences; that is, all sentences are spontaneous or almost equivalent. It does not need manual writing which causes hidden biases. Second, the corpus contains adversarial examples. We collect challenging examples that can not be solved by a recent pre-trained language model. Third, the corpus contains explanations for a part of non-entailment labels. We perform the reasoning annotation where annotators are asked to check which tokens in hypotheses are the reason why the relations are labeled. It makes easy to validate the annotation and analyze system errors. The resulting corpus consists of 48,000 realistic Japanese examples. It is the largest among publicly available Japanese TE corpora. Additionally, it is the first Japanese TE corpus that includes reasons for the annotation as we know. We are planning to distribute this corpus to the NLP community at the time of publication.","Marseille, France","Hayashibe, Yuta",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6827--6834,European Language Resources Association,{J}apanese Realistic Textual Entailment Corpus,https://aclanthology.org/2020.lrec-1.843,2020,,,,English,979-10-95546-34-4
407,inproceedings,liu-etal-2020-hyponli,"Many recent studies have shown that for models trained on datasets for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such bias. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out {`}hard{'} and {`}easy{'} instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (BERT, RoBerta, XLNet) and competitive non-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias: down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in NLI debiasing tasks.","Marseille, France","Liu, Tianyu  and
Xin, Zheng  and
Chang, Baobao  and
Sui, Zhifang",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6852--6860,European Language Resources Association,{H}ypo{NLI}: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference,https://aclanthology.org/2020.lrec-1.846,2020,,,,English,979-10-95546-34-4
408,inproceedings,paul-panenghat-etal-2020-towards,"Modeling natural language inference is a challenging task. With large annotated data sets available it has now become feasible to train complex neural network based inference methods which achieve state of the art performance. However, it has been shown that these models also learn from the subtle biases inherent in these datasets (CITATION). In this work we explore two techniques for delexicalization that modify the datasets in such a way that we can control the importance that neural-network based methods place on lexical entities. We demonstrate that the proposed methods not only maintain the performance in-domain but also improve performance in some out-of-domain settings. For example, when using the delexicalized version of the FEVER dataset, the in-domain performance of a state of the art neural network method dropped only by 1.12{\%} while its out-of-domain performance on the FNC dataset improved by 4.63{\%}. We release the delexicalized versions of three common datasets used in natural language inference. These datasets are delexicalized using two methods: one which replaces the lexical entities in an overlap-aware manner, and a second, which additionally incorporates semantic lifting of nouns and verbs to their WordNet hypernym synsets","Marseille, France","Paul Panenghat, Mithun  and
Suntwal, Sandeep  and
Rafique, Faiz  and
Sharp, Rebecca  and
Surdeanu, Mihai",Proceedings of the 12th Language Resources and Evaluation Conference,,May,6883--6888,European Language Resources Association,Towards the Necessity for Debiasing Natural Language Inference Datasets,https://aclanthology.org/2020.lrec-1.850,2020,,,,English,979-10-95546-34-4
409,inproceedings,di-donato-etal-2020-social,"The paper presents a journey, which starts from various social sciences and humanities (SSH) Research Infrastructures in Europe and arrives at the comprehensive {``}ecosystem of infrastructures{''}, namely the European Open Science Cloud (EOSC). We will highlight how the SSH Open Science infrastructures contribute to the goal of establishing the EOSC. First, through the example of OPERAS, the European Research Infrastructure for Open Scholarly Communication in the SSH, to see how its services are conceived to be part of the EOSC and to address the communities{'} needs. The next two sections highlight collaboration practices between partners in Europe to build the SSH component of the EOSC and a SSH discovery platform, as a service of OPERAS and the EOSC. The last two sections will focus on an implementation network dedicated to SSH data fairification.","Marseille, France","Di Donato, Francesca  and
Monachini, Monica  and
Eskevich, Maria  and
Pohle, Stefanie  and
Moranville, Yoann  and
Dumouchel, Suzanne",Proceedings of the Workshop about Language Resources for the SSH Cloud,,May,5--9,European Language Resources Association,Social Sciences and Humanities Pathway Towards the {E}uropean Open Science Cloud,https://aclanthology.org/2020.lr4sshoc-1.2,2020,,,,English,979-10-95546-43-6
410,inproceedings,kim-2020-deep,"Recently, several studies have investigated active learning (AL) for natural language processing tasks to alleviate data dependency. However, for query selection, most of these studies mainly rely on uncertainty-based sampling, which generally does not exploit the structural information of the unlabeled data. This leads to a sampling bias in the batch active learning setting, which selects several samples at once. In this work, we demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task. We examined the effects of our sequence-based approach by selecting weighted diverse in the gradient embedding approach across multiple tasks, datasets, models, and consistently outperform classic uncertainty-based sampling and diversity-based sampling.","Suzhou, China","Kim, Yekyung",Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems,,December,1--8,Association for Computational Linguistics,Deep Active Learning for Sequence Labeling Based on Diversity and Uncertainty in Gradient,https://aclanthology.org/2020.lifelongnlp-1.1,2020,,,,,
411,inproceedings,beck-etal-2020-representation,"The development of linguistic corpora is fraught with various problems of annotation and representation. These constitute a very real challenge for the development and use of annotated corpora, but as yet not much literature exists on how to address the underlying problems. In this paper, we identify and discuss five sources of representation problems, which are independent though interrelated: ambiguity, variation, uncertainty, error and bias. We outline and characterize these sources, discussing how their improper treatment can have stark consequences for research outcomes. Finally, we discuss how an adequate treatment can inform corpus-related linguistic research, both computational and theoretical, improving the reliability of research results and NLP models, as well as informing the more general reproducibility issue.","Barcelona, Spain","Beck, Christin  and
Booth, Hannah  and
El-Assady, Mennatallah  and
Butt, Miriam",Proceedings of the 14th Linguistic Annotation Workshop,,December,60--73,Association for Computational Linguistics,"Representation Problems in Linguistic Annotations: Ambiguity, Variation, Uncertainty, Error and Bias",https://aclanthology.org/2020.law-1.6,2020,,,,,
412,inproceedings,bagga-piper-2020-measuring,"Downstream effects of biased training data have become a major concern of the NLP community. How this may impact the automated curation and annotation of cultural heritage material is currently not well known. In this work, we create an experimental framework to measure the effects of different types of stylistic and social bias within training data for the purposes of literary classification, as one important subclass of cultural material. Because historical collections are often sparsely annotated, much like our knowledge of history is incomplete, researchers often cannot know the underlying distributions of different document types and their various sub-classes. This means that bias is likely to be an intrinsic feature of training data when it comes to cultural heritage material. Our aim in this study is to investigate which classification methods may help mitigate the effects of different types of bias within curated samples of training data. We find that machine learning techniques such as BERT or SVM are robust against reproducing the different kinds of bias within our test data, except in the most extreme cases. We hope that this work will spur further research into the potential effects of bias within training data for other cultural heritage material beyond the study of literature.",Online,"Bagga, Sunyam  and
Piper, Andrew","Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",,December,74--84,International Committee on Computational Linguistics,Measuring the Effects of Bias in Training Data for Literary Classification,https://aclanthology.org/2020.latechclfl-1.9,2020,,,,,
413,inproceedings,dennis-henderson-etal-2020-life,"An increasing amount of historic data is now available in digital (text) formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional close reading, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such data set that has recently been transcribed is a collection of over 500 Australian World War I (WW1) diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this corpus to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that data, topic modelling and sentiment analysis can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues.",Online,"Dennis-Henderson, Ashley  and
Roughan, Matthew  and
Mitchell, Lewis  and
Tuke, Jonathan","Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",,December,90--104,International Committee on Computational Linguistics,Life still goes on: Analysing {A}ustralian {WW}1 Diaries through Distant Reading,https://aclanthology.org/2020.latechclfl-1.11,2020,,,,,
414,inproceedings,brate-etal-2020-towards,"Environmental factors determine the smells we perceive, but societal factors factors shape the importance, sentiment and biases we give to them. Descriptions of smells in text, or as we call them {`}smell experiences{'}, offer a window into these factors, but they must first be identified. To the best of our knowledge, no tool exists to extract references to smell experiences from text. In this paper, we present two variations on a semi-supervised approach to identify smell experiences in English literature. The combined set of patterns from both implementations offer significantly better performance than a keyword-based baseline.",Online,"Brate, Ryan  and
Groth, Paul  and
van Erp, Marieke","Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",,December,147--155,International Committee on Computational Linguistics,Towards Olfactory Information Extraction from Text: A Case Study on Detecting Smell Experiences in Novels,https://aclanthology.org/2020.latechclfl-1.18,2020,,,,,
415,inproceedings,shekarpour-etal-2020-qa2explanation,"In the era of Big Knowledge Graphs, Question Answering (QA) systems have reached a milestone in their performance and feasibility. However, their applicability, particularly in specific domains such as the biomedical domain, has not gained wide acceptance due to their {``}black box{''} nature, which hinders transparency, fairness, and accountability of QA systems. Therefore, users are unable to understand how and why particular questions have been answered, whereas some others fail. To address this challenge, in this paper, we develop an automatic approach for generating explanations during various stages of a pipeline-based QA system. Our approach is a supervised and automatic approach which considers three classes (i.e., success, no answer, and wrong answer) for annotating the output of involved QA components. Upon our prediction, a template explanation is chosen and integrated into the output of the corresponding component. To measure the effectiveness of the approach, we conducted a user survey as to how non-expert users perceive our generated explanations. The results of our study show a significant increase in the four dimensions of the human factor from the Human-computer interaction community.",Online,"Shekarpour, Saeedeh  and
Nadgeri, Abhishek  and
Singh, Kuldeep",Proceedings of the First Workshop on Interactive and Executable Semantic Parsing,10.18653/v1/2020.intexsempar-1.1,November,1--11,Association for Computational Linguistics,{QA}2{E}xplanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph,https://aclanthology.org/2020.intexsempar-1.1,2020,,,,,
416,inproceedings,thomson-etal-2020-studying,"It is unfair to expect neural data-to-text to produce high quality output when there are gaps between system input data and information contained in the training text. Thomson et al. (2020) identify and narrow information gaps in Rotowire, a popular data-to-text dataset. In this paper, we describe a study which finds that a state-of-the-art neural data-to-text system produces higher quality output, according to the information extraction (IE) based metrics, when additional input data is carefully selected from this newly available source. It remains to be shown, however, whether IE metrics used in this study correlate well with humans in judging text quality.","Dublin, Ireland","Thomson, Craig  and
Zhao, Zhijie  and
Sripada, Somayajulu",Proceedings of the 13th International Conference on Natural Language Generation,,December,35--40,Association for Computational Linguistics,Studying the Impact of Filling Information Gaps on the Output Quality of Neural Data-to-Text,https://aclanthology.org/2020.inlg-1.6,2020,,,,,
417,inproceedings,saldanha-etal-2020-understanding,"Massive digital disinformation is one of the main risks of modern society. Hundreds of models and linguistic analyses have been done to compare and contrast misleading and credible content online. However, most models do not remove the confounding factor of a topic or narrative when training, so the resulting models learn a clear topical separation for misleading versus credible content. We study the feasibility of using two strategies to disentangle the topic bias from the models to understand and explicitly measure linguistic and stylistic properties of content from misleading versus credible content. First, we develop conditional generative models to create news content that is characteristic of different credibility levels. We perform multi-dimensional evaluation of model performance on mimicking both the style and linguistic differences that distinguish news of different credibility using machine translation metrics and classification models. We show that even though generative models are able to imitate both the style and language of the original content, additional conditioning on both the news category and the topic leads to reduced performance. In a second approach, we perform deception style {``}transfer{''} by translating deceptive content into the style of credible content and vice versa. Extending earlier studies, we demonstrate that, when conditioned on a topic, deceptive content is shorter, less readable, more biased, and more subjective than credible content, and transferring the style from deceptive to credible content is more challenging than the opposite direction.","Dublin, Ireland","Saldanha, Emily  and
Garimella, Aparna  and
Volkova, Svitlana",Proceedings of the 13th International Conference on Natural Language Generation,,December,216--226,Association for Computational Linguistics,Understanding and Explicitly Measuring Linguistic and Stylistic Properties of Deception via Generation and Translation,https://aclanthology.org/2020.inlg-1.27,2020,,,,,
418,inproceedings,syed-etal-2020-task,"We propose a shared task on abstractive snippet generation for web pages, a novel task of generating query-biased abstractive summaries for documents that are to be shown on a search results page. Conventional snippets are extractive in nature, which recently gave rise to copyright claims from news publishers as well as a new copyright legislation being passed in the European Union, limiting the fair use of web page contents for snippets. At the same time, abstractive summarization has matured considerably in recent years, potentially allowing for more personalization of snippets in the future. Taken together, these facts render further research into generating abstractive snippets both timely and promising.","Dublin, Ireland","Syed, Shahbaz  and
Chen, Wei-Fan  and
Hagen, Matthias  and
Stein, Benno  and
Wachsmuth, Henning  and
Potthast, Martin",Proceedings of the 13th International Conference on Natural Language Generation,,December,237--241,Association for Computational Linguistics,Task Proposal: Abstractive Snippet Generation for Web Pages,https://aclanthology.org/2020.inlg-1.30,2020,,,,,
419,inproceedings,patil-etal-2020-optimized,"Building Chabot{'}s requires a large amount of conversational data. In this paper, a web crawler is designed to fetch multi-turn dialogues from websites such as Twitter, YouTube and Reddit in the form of a JavaScript Object Notation (JSON) file. Tools like Twitter Application Programming Interface (API), LXML Library, and JSON library are used to crawl Twitter, YouTube and Reddit to collect conversational chat data. The data obtained in a raw form cannot be used directly as it will have only text metadata such as author or name, time to provide more information on the chat data being scraped. The data collected has to be formatted for proper use case and the JSON library of python allows us to format the data easily. The scraped dialogues are further filtered based on the context of a search keyword without introducing bias and with flexible strictness of classification.","Patna, India","Patil, Annapurna P  and
Subramanian, Rajarajeswari  and
Karkal, Gaurav  and
Purushotham, Keerthana  and
Wadhwa, Jugal  and
Reddy, K Dhanush  and
Sawood, Meer",Proceedings of the Workshop on Joint NLP Modelling for Conversational AI @ ICON 2020,,December,33--39,NLP Association of India (NLPAI),Optimized Web-Crawling of Conversational Data from Social Media and Context-Based Filtering,https://aclanthology.org/2020.icon-workshop.5,2020,,,,,
420,inproceedings,ghosh-etal-2020-annotated,"Emotion recognition is a very well-attended problem in Natural Language Processing (NLP). Most of the existing works on emotion recognition focus on the general domain and in some cases to specific domains like fairy tales, blogs, weather, Twitter etc. But emotion analysis systems in the domains of security, social issues, technology, politics, sports, etc. are very rare. In this paper, we create a benchmark setup for emotion recognition in these specialised domains. First, we construct a corpus of 18,921 tweets in English annotated with Paul Ekman{'}s six basic emotions (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and a non-emotive class Others. Thereafter, we propose a deep neural framework to perform emotion recognition in an end-to-end setting. We build various models based on Convolutional Neural Network (CNN), Bi-directional Long Short Term Memory (Bi-LSTM), Bi-directional Gated Recurrent Unit (Bi-GRU). We propose a Hierarchical Attention-based deep neural network for Emotion Detection (HAtED). We also develop multiple systems by considering different sets of emotion classes for each system and report the detailed comparative analysis of the results. Experiments show the hierarchical attention-based model achieves best results among the considered baselines with accuracy of 69{\%}.","Indian Institute of Technology Patna, Patna, India","Ghosh, Soumitra  and
Ekbal, Asif  and
Bhattacharyya, Pushpak  and
Saha, Sriparna  and
Tyagi, Vipin  and
Kumar, Alka  and
Srivastava, Shikha  and
Kumar, Nitish",Proceedings of the 17th International Conference on Natural Language Processing (ICON),,December,460--469,NLP Association of India (NLPAI),Annotated Corpus of Tweets in {E}nglish from Various Domains for Emotion Detection,https://aclanthology.org/2020.icon-main.62,2020,,,,,
421,inproceedings,bartl-etal-2020-unmasking,"Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.","Barcelona, Spain (Online)","Bartl, Marion  and
Nissim, Malvina  and
Gatt, Albert",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,1--16,Association for Computational Linguistics,Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias,https://aclanthology.org/2020.gebnlp-1.1,2020,,,,,
422,inproceedings,jiang-fellbaum-2020-interdependencies,"Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.","Barcelona, Spain (Online)","Jiang, May  and
Fellbaum, Christiane",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,17--25,Association for Computational Linguistics,Interdependencies of Gender and Race in Contextualized Word Embeddings,https://aclanthology.org/2020.gebnlp-1.2,2020,,,,,
423,inproceedings,costa-jussa-de-jorge-2020-fine,"Misrepresentation of certain communities in datasets is causing big disruptions in artificial intelligence applications. In this paper, we propose using an automatically extracted gender-balanced dataset parallel corpus from Wikipedia. This balanced set is used to perform fine-tuning techniques from a bigger model trained on unbalanced datasets to mitigate gender biases in neural machine translation.","Barcelona, Spain (Online)","Costa-juss{\`a}, Marta R.  and
de Jorge, Adri{\`a}",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,26--34,Association for Computational Linguistics,Fine-tuning Neural Machine Translation on Gender-Balanced Datasets,https://aclanthology.org/2020.gebnlp-1.3,2020,,,,,
424,inproceedings,saunders-etal-2020-neural,"Neural Machine Translation (NMT) has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level. In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation. We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language.","Barcelona, Spain (Online)","Saunders, Danielle  and
Sallis, Rosie  and
Byrne, Bill",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,35--43,Association for Computational Linguistics,Neural Machine Translation Doesn{'}t Translate Gender Coreference Right Unless You Make It,https://aclanthology.org/2020.gebnlp-1.4,2020,,,,,
425,inproceedings,takeshita-etal-2020-existing,"It is known that word embeddings exhibit biases inherited from the corpus, and those biases reflect social stereotypes. Recently, many studies have been conducted to analyze and mitigate biases in word embeddings. Unsupervised Bias Enumeration (UBE) (Swinger et al., 2019) is one of approach to analyze biases for English, and Hard Debias (Bolukbasi et al., 2016) is the common technique to mitigate gender bias. These methods focused on English, or, in smaller extent, on Indo-European languages. However, it is not clear whether these methods can be generalized to other languages. In this paper, we apply these analyzing and mitigating methods, UBE and Hard Debias, to Japanese word embeddings. Additionally, we examine whether these methods can be used for Japanese. We experimentally show that UBE and Hard Debias cannot be sufficiently adapted to Japanese embeddings.","Barcelona, Spain (Online)","Takeshita, Masashi  and
Katsumata, Yuki  and
Rzepka, Rafal  and
Araki, Kenji",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,44--55,Association for Computational Linguistics,Can Existing Methods Debias Languages Other than {E}nglish? First Attempt to Analyze and Mitigate {J}apanese Word Embeddings,https://aclanthology.org/2020.gebnlp-1.5,2020,,,,,
426,inproceedings,chavez-mulsa-spanakis-2020-evaluating,"Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications. This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch. We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks. The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings. We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language. Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2{\%} decrease in performance in contextualized embeddings. Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.","Barcelona, Spain (Online)","Ch{\'a}vez Mulsa, Rodrigo Alejandro  and
Spanakis, Gerasimos",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,56--71,Association for Computational Linguistics,Evaluating Bias In {D}utch Word Embeddings,https://aclanthology.org/2020.gebnlp-1.6,2020,,,,,
427,inproceedings,devinney-etal-2020-semi,"Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.","Barcelona, Spain (Online)","Devinney, Hannah  and
Bj{\""o}rklund, Jenny  and
Bj{\""o}rklund, Henrik",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,79--92,Association for Computational Linguistics,Semi-Supervised Topic Modeling for Gender Bias Discovery in {E}nglish and {S}wedish,https://aclanthology.org/2020.gebnlp-1.8,2020,,,,,
428,inproceedings,sheng-uthus-2020-investigating,"There is a growing collection of work analyzing and mitigating societal biases in language understanding, generation, and retrieval tasks, though examining biases in creative tasks remains underexplored. Creative language applications are meant for direct interaction with users, so it is important to quantify and mitigate societal biases in these applications. We introduce a novel study on a pipeline to mitigate societal biases when retrieving next verse suggestions in a poetry composition system. Our results suggest that data augmentation through sentiment style transfer has potential for mitigating societal biases.","Barcelona, Spain (Online)","Sheng, Emily  and
Uthus, David",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,93--106,Association for Computational Linguistics,Investigating Societal Biases in a Poetry Composition System,https://aclanthology.org/2020.gebnlp-1.9,2020,,,,,
429,inproceedings,havens-etal-2020-situated,"We propose a bias-aware methodology to engage with power relations in natural language processing (NLP) research. NLP research rarely engages with bias in social contexts, limiting its ability to mitigate bias. While researchers have recommended actions, technical methods, and documentation practices, no methodology exists to integrate critical reflections on bias with technical NLP methods. In this paper, after an extensive and interdisciplinary literature review, we contribute a bias-aware methodology for NLP research. We also contribute a definition of biased text, a discussion of the implications of biased NLP systems, and a case study demonstrating how we are executing the bias-aware methodology in research on archival metadata descriptions.","Barcelona, Spain (Online)","Havens, Lucy  and
Terras, Melissa  and
Bach, Benjamin  and
Alex, Beatrice",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,107--124,Association for Computational Linguistics,"Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research",https://aclanthology.org/2020.gebnlp-1.10,2020,,,,,
430,inproceedings,touileb-etal-2020-gender,"Gender bias in models and datasets is widely studied in NLP. The focus has usually been on analysing how females and males express themselves, or how females and males are described. However, a less studied aspect is the combination of these two perspectives, how female and male describe the same or opposite gender. In this paper, we present a new gender annotated sentiment dataset of critics reviewing the works of female and male authors. We investigate if this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively.","Barcelona, Spain (Online)","Touileb, Samia  and
{\O}vrelid, Lilja  and
Velldal, Erik",Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,,December,125--138,Association for Computational Linguistics,"Gender and sentiment, critics and authors: a dataset of {N}orwegian book reviews",https://aclanthology.org/2020.gebnlp-1.11,2020,,,,,
431,inproceedings,huang-etal-2020-reducing,"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model{'}s latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.",Online,"Huang, Po-Sen  and
Zhang, Huan  and
Jiang, Ray  and
Stanforth, Robert  and
Welbl, Johannes  and
Rae, Jack  and
Maini, Vishal  and
Yogatama, Dani  and
Kohli, Pushmeet",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.7,November,65--83,Association for Computational Linguistics,Reducing Sentiment Bias in Language Models via Counterfactual Evaluation,https://aclanthology.org/2020.findings-emnlp.7,2020,,,,,
432,inproceedings,li-etal-2020-active,"Distant supervision has been a widely used method for neural relation extraction for its convenience of automatically labeling datasets. However, existing works on distantly supervised relation extraction suffer from the low quality of test set, which leads to considerable biased performance evaluation. These biases not only result in unfair evaluations but also mislead the optimization of neural relation extraction. To mitigate this problem, we propose a novel evaluation method named active testing through utilizing both the noisy test set and a few manual annotations. Experiments on a widely used benchmark show that our proposed approach can yield approximately unbiased evaluations for distantly supervised relation extractors.",Online,"Li, Pengshuai  and
Zhang, Xinsong  and
Jia, Weijia  and
Zhao, Wei",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.20,November,204--211,Association for Computational Linguistics,Active Testing: An Unbiased Evaluation Method for Distantly Supervised Relation Extraction,https://aclanthology.org/2020.findings-emnlp.20,2020,,,,,
433,inproceedings,zhang-etal-2020-minimize,"Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length associated with error accumulation. These methods introduce exposure bias, which may cause the models overfit to the frequent label combination, thus limiting the generalization ability. We propose a novel Sequence-to-Unordered-Multi-Tree (Seq2UMTree) model to minimize the effects of exposure bias by limiting the decoding length to three within a triplet and removing the order among triplets. We evaluate our model on two datasets, DuIE and NYT, and systematically study how exposure bias alters the performance of Seq2Seq models. Experiments show that the state-of-the-art Seq2Seq model overfits to both datasets while Seq2UMTree shows significantly better generalization. Our code is available at \url{https://github.com/WindChimeRan/OpenJERE}.",Online,"Zhang, Ranran Haoran  and
Liu, Qianying  and
Fan, Aysa Xuemo  and
Ji, Heng  and
Zeng, Daojian  and
Cheng, Fei  and
Kawahara, Daisuke  and
Kurohashi, Sadao",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.23,November,236--246,Association for Computational Linguistics,Minimize Exposure Bias of {S}eq2{S}eq Models in Joint Entity and Relation Extraction,https://aclanthology.org/2020.findings-emnlp.23,2020,,,,,
434,inproceedings,udagawa-etal-2020-linguistic,"Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this problem, we make two design choices: first, we focus on OneCommon Corpus (CITATION), a simple yet challenging common grounding dataset which contains minimal bias by design. Second, we analyze their linguistic structures based on spatial expressions and provide comprehensive and reliable annotation for 600 dialogues. We show that our annotation captures important linguistic structures including predicate-argument structure, modification and ellipsis. In our experiments, we assess the model{'}s understanding of these structures through reference resolution. We demonstrate that our annotation can reveal both the strengths and weaknesses of baseline models in essential levels of detail. Overall, we propose a novel framework and resource for investigating fine-grained language understanding in visually grounded dialogues.",Online,"Udagawa, Takuma  and
Yamazaki, Takato  and
Aizawa, Akiko",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.67,November,750--765,Association for Computational Linguistics,A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions,https://aclanthology.org/2020.findings-emnlp.67,2020,,,,,
435,inproceedings,wu-etal-2020-improving,"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.",Online,"Wu, Mingzhu  and
Moosavi, Nafise Sadat  and
R{\""u}ckl{\'e}, Andreas  and
Gurevych, Iryna",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.74,November,839--853,Association for Computational Linguistics,Improving {QA} Generalization by Concurrent Modeling of Multiple Biases,https://aclanthology.org/2020.findings-emnlp.74,2020,,,,,
436,inproceedings,kulshreshtha-etal-2020-cross,"Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior work propose several approaches to align contextualised embeddings. In this paper we analyse how different forms of cross-lingual supervision and various alignment methods influence the transfer capability of mBERT in zero-shot setting. Specifically, we compare parallel corpora vs dictionary-based supervision and rotational vs fine-tuning based alignment methods. We evaluate the performance of different alignment methodologies across eight languages on two tasks: Name Entity Recognition and Semantic Slot Filling. In addition, we propose a novel normalisation method which consistently improves the performance of rotation-based alignment including a notable 3{\%} F1 improvement for distant and typologically dissimilar languages. Importantly we identify the biases of the alignment methods to the type of task and proximity to the transfer language. We also find that supervision from parallel corpus is generally superior to dictionary alignments.",Online,"Kulshreshtha, Saurabh  and
Redondo Garcia, Jose Luis  and
Chang, Ching-Yun",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.83,November,933--942,Association for Computational Linguistics,Cross-lingual Alignment Methods for Multilingual {BERT}: A Comparative Study,https://aclanthology.org/2020.findings-emnlp.83,2020,,,,,
437,inproceedings,liu-etal-2020-conditional,"Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is an unmet need to generate a variety of summaries for different users. In this paper, we propose a neural framework that can flexibly control summary generation by introducing a set of sub-aspect functions (i.e. importance, diversity, position). These sub-aspect functions are regulated by a set of control codes to decide which sub-aspect to focus on during summary generation. We demonstrate that extracted summaries with minimal position bias is comparable with those generated by standard models that take advantage of position preference. We also show that news summaries generated with a focus on diversity can be more preferred by human raters. These results suggest that a more flexible neural summarization framework providing more control options could be desirable in tailoring to different user preferences, which is useful since it is often impractical to articulate such preferences for different applications a priori.",Online,"Liu, Zhengyuan  and
Shi, Ke  and
Chen, Nancy",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.131,November,1453--1463,Association for Computational Linguistics,Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization,https://aclanthology.org/2020.findings-emnlp.131,2020,,,,,
438,inproceedings,yang-etal-2020-ted,"Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose TED, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in news articles to pretrain the model on millions of unlabeled corpora. Next, we finetune TED on target domains through theme modeling and a denoising autoencoder to enhance the quality of generated summaries. Notably, TED outperforms all unsupervised abstractive baselines on NYT, CNN/DM and English Gigaword datasets with various document styles. Further analysis shows that the summaries generated by TED are highly abstractive, and each component in the objective function of TED is highly effective.",Online,"Yang, Ziyi  and
Zhu, Chenguang  and
Gmyr, Robert  and
Zeng, Michael  and
Huang, Xuedong  and
Darve, Eric",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.168,November,1865--1874,Association for Computational Linguistics,{TED}: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising,https://aclanthology.org/2020.findings-emnlp.168,2020,,,,,
439,inproceedings,ni-etal-2020-learning,"Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists{'} workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.",Online,"Ni, Jianmo  and
Hsu, Chun-Nan  and
Gentili, Amilcare  and
McAuley, Julian",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.176,November,1954--1960,Association for Computational Linguistics,Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest {X}-rays,https://aclanthology.org/2020.findings-emnlp.176,2020,,,,,
440,inproceedings,peyrard-west-2020-klearn,"The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to background knowledge. In contrast, this work puts background knowledge in the foreground. Building on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge, we develop and compare techniques for inferring background knowledge from summarization data. Based on this framework, we define summary scoring functions that explicitly model background knowledge, and show that these scoring functions fit human judgments significantly better than baselines. We illustrate some of the many potential applications of our framework. First, we provide insights into human information importance priors. Second, we demonstrate that averaging the background knowledge of multiple, potentially biased annotators or corpora greatly improves summaryscoring performance. Finally, we discuss potential applications of our framework beyond summarization.",Online,"Peyrard, Maxime  and
West, Robert",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.188,November,2073--2085,Association for Computational Linguistics,{KL}earn: Background Knowledge Inference from Summarization Data,https://aclanthology.org/2020.findings-emnlp.188,2020,,,,,
441,inproceedings,lyu-etal-2020-differentially,"It has been demonstrated that hidden representation learned by deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.",Online,"Lyu, Lingjuan  and
He, Xuanli  and
Li, Yitong",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.213,November,2355--2365,Association for Computational Linguistics,Differentially Private Representation for {NLP}: Formal Guarantee and An Empirical Study on Privacy and Fairness,https://aclanthology.org/2020.findings-emnlp.213,2020,,,,,
442,inproceedings,liu-etal-2020-context,"Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED model. This paper aims to fill this research gap by stressing the importance of robustness modeling in ED models. We first pinpoint three stark cases demonstrating the brittleness of the existing ED models. After analyzing the underlying reason, we propose a new training mechanism, called context-selective mask generalization for ED, which can effectively mine context-specific patterns for learning and robustify an ED model. The experimental results have confirmed the effectiveness of our model regarding defending against adversarial attacks, exploring unseen predicates, and tackling ambiguity cases. Moreover, a deeper analysis suggests that our approach can learn a complementary predictive bias with most ED models that use full context for feature learning.",Online,"Liu, Jian  and
Chen, Yubo  and
Liu, Kang  and
Jia, Yantao  and
Sheng, Zhicheng",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.229,November,2523--2532,Association for Computational Linguistics,How Does Context Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization,https://aclanthology.org/2020.findings-emnlp.229,2020,,,,,
443,inproceedings,saphra-lopez-2020-lstms,"Recent work in NLP shows that LSTM language models capture compositional structure in language data. In contrast to existing work, we consider the \textit{learning} process that leads to compositional behavior. For a closer look at how an LSTM{'}s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence (DI) between word meanings in an LSTM, based on their gate interactions. We support this measure with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than on learning the longer-range relations independently.",Online,"Saphra, Naomi  and
Lopez, Adam",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.252,November,2797--2809,Association for Computational Linguistics,{LSTM}s Compose{---}and {L}earn{---}{B}ottom-Up,https://aclanthology.org/2020.findings-emnlp.252,2020,,,,,
444,inproceedings,dey-etal-2020-corpora,"Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other datasets. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/LCS2-IIITD/summarization{\_}bias.git",Online,"Dey, Alvin  and
Chowdhury, Tanya  and
Kumar, Yash  and
Chakraborty, Tanmoy",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.254,November,2830--2840,Association for Computational Linguistics,Corpora Evaluation and System Bias Detection in Multi-document Summarization,https://aclanthology.org/2020.findings-emnlp.254,2020,,,,,
445,inproceedings,clark-etal-2020-learning,"Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences contain the word {``}not{''}, and image recognition datasets can have tell-tale object-background correlations if dogs are always indoors. In this paper, we propose a method that can automatically detect and ignore these kinds of dataset-specific patterns, which we call dataset biases. Our method trains a lower capacity model in an ensemble with a higher capacity model. During training, the lower capacity model learns to capture relatively shallow correlations, which we hypothesize are likely to reflect dataset bias. This frees the higher capacity model to focus on patterns that should generalize better. We ensure the models learn non-overlapping approaches by introducing a novel method to make them conditionally independent. Importantly, our approach does not require the bias to be known in advance. We evaluate performance on synthetic datasets, and four datasets built to penalize models that exploit known biases on textual entailment, visual question answering, and image recognition tasks. We show improvement in all settings, including a 10 point gain on the visual question answering dataset.",Online,"Clark, Christopher  and
Yatskar, Mark  and
Zettlemoyer, Luke",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.272,November,3031--3045,Association for Computational Linguistics,Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles,https://aclanthology.org/2020.findings-emnlp.272,2020,,,,,
446,inproceedings,raunak-etal-2020-long,"State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during inference. In this work, we quantitatively characterize such long-tailed phenomena at two levels of abstraction, namely, token classification and sequence generation. We propose a new loss function, the Anti-Focal loss, to better adapt model training to the structural dependencies of conditional text generation by incorporating the inductive biases of beam search in the training process. We show the efficacy of the proposed technique on a number of Machine Translation (MT) datasets, demonstrating that it leads to significant gains over cross-entropy across different language pairs, especially on the generation of low-frequency words. We have released the code to reproduce our results.",Online,"Raunak, Vikas  and
Dalmia, Siddharth  and
Gupta, Vivek  and
Metze, Florian",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.276,November,3088--3095,Association for Computational Linguistics,On Long-Tailed Phenomena in Neural Machine Translation,https://aclanthology.org/2020.findings-emnlp.276,2020,,,,,
447,inproceedings,shin-etal-2020-neutralizing,"Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear subspace for debiasing, we introduce a Latent Disentanglement method with a siamese auto-encoder structure with an adapted gradient reversal layer. Our structure enables the separation of the semantic latent information and gender latent information of given word into the disjoint latent dimensions. Afterwards, we introduce a Counterfactual Generation to convert the gender information of words, so the original and the modified embeddings can produce a gender-neutralized word embedding after geometric alignment regularization, without loss of semantic information. From the various quantitative and qualitative debiasing experiments, our method shows to be better than existing debiasing methods in debiasing word embeddings. In addition, Our method shows the ability to preserve semantic information during debiasing by minimizing the semantic information losses for extrinsic NLP downstream tasks.",Online,"Shin, Seungjae  and
Song, Kyungwoo  and
Jang, JoonHo  and
Kim, Hyemi  and
Joo, Weonyoung  and
Moon, Il-Chul",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.280,November,3126--3140,Association for Computational Linguistics,Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation,https://aclanthology.org/2020.findings-emnlp.280,2020,,,,,
448,inproceedings,sheng-etal-2020-towards,"We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze two scenarios: 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics. The former scenario enables us to detect the types of biases present in the model. Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics. The second scenario is useful for mitigating biases in downstream applications such as dialogue generation. In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall.",Online,"Sheng, Emily  and
Chang, Kai-Wei  and
Natarajan, Prem  and
Peng, Nanyun",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.291,November,3239--3254,Association for Computational Linguistics,Towards {C}ontrollable {B}iases in {L}anguage {G}eneration,https://aclanthology.org/2020.findings-emnlp.291,2020,,,,,
449,inproceedings,delobelle-etal-2020-robbert,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model{'}s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.",Online,"Delobelle, Pieter  and
Winters, Thomas  and
Berendt, Bettina",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.292,November,3255--3265,Association for Computational Linguistics,{R}ob{BERT}: a {D}utch {R}o{BERT}a-based {L}anguage {M}odel,https://aclanthology.org/2020.findings-emnlp.292,2020,,,,,
450,inproceedings,kang-etal-2020-regularization,"Unsupervised question answering (UQA) has been proposed to avoid the high cost of creating high-quality datasets for QA. One approach to UQA is to train a QA model with questions generated automatically. However, the generated questions are either too similar to a word sequence in the context or too drifted from the semantics of the context, thereby making it difficult to train a robust QA model. We propose a novel regularization method based on teacher-student architecture to avoid bias toward a particular question generation strategy and modulate the process of generating individual words when a question is generated. Our experiments demonstrate that we have achieved the goal of generating higher-quality questions for UQA across diverse QA datasets and tasks. We also show that this method can be useful for creating a QA model with few-shot learning.",Online,"Kang, Junmo  and
Hong, Giwon  and
Puerto San Roman, Haritz  and
Myaeng, Sung-Hyon",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.293,November,3266--3277,Association for Computational Linguistics,Regularization of Distinct Strategies for Unsupervised Question Generation,https://aclanthology.org/2020.findings-emnlp.293,2020,,,,,
451,inproceedings,li-etal-2020-unqovering,"While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.",Online,"Li, Tao  and
Khashabi, Daniel  and
Khot, Tushar  and
Sabharwal, Ashish  and
Srikumar, Vivek",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.311,November,3475--3489,Association for Computational Linguistics,{UNQOVER}ing Stereotyping Biases via Underspecified Questions,https://aclanthology.org/2020.findings-emnlp.311,2020,,,,,
452,inproceedings,pouran-ben-veyseh-etal-2020-graph,"The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks (GTNs) to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.",Online,"Pouran Ben Veyseh, Amir  and
Nguyen, Tuan Ngo  and
Nguyen, Thien Huu",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.326,November,3651--3661,Association for Computational Linguistics,Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction,https://aclanthology.org/2020.findings-emnlp.326,2020,,,,,
453,inproceedings,medina-maza-etal-2020-event,"Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.",Online,"Medina Maza, Salvador  and
Spiliopoulou, Evangelia  and
Hovy, Eduard  and
Hauptmann, Alexander",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.344,November,3858--3868,Association for Computational Linguistics,Event-Related Bias Removal for Real-time Disaster Events,https://aclanthology.org/2020.findings-emnlp.344,2020,,,,,
454,inproceedings,kim-etal-2020-arramon,"For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ARRAMON. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language (English) instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work.",Online,"Kim, Hyounghun  and
Zala, Abhaysinh  and
Burri, Graham  and
Tan, Hao  and
Bansal, Mohit",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.348,November,3910--3927,Association for Computational Linguistics,{A}rra{M}on: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments,https://aclanthology.org/2020.findings-emnlp.348,2020,,,,,
455,inproceedings,chen-etal-2020-detecting,"Media plays an important role in shaping public opinion. Biased media can influence people in undesirable directions and hence should be unmasked as such. We observe that feature-based and neural text classification approaches which rely only on the distribution of low-level lexical information fail to detect media bias. This weakness becomes most noticeable for articles on new events, where words appear in new contexts and hence their {``}bias predictiveness{''} is unclear. In this paper, we therefore study how second-order information about biased statements in an article helps to improve detection effectiveness. In particular, we utilize the probability distributions of the frequency, positions, and sequential order of lexical and informational sentence-level bias in a Gaussian Mixture Model. On an existing media bias dataset, we find that the frequency and positions of biased statements strongly impact article-level bias, whereas their exact sequential order is secondary. Using a standard model for sentence-level bias detection, we provide empirical evidence that article-level bias detectors that use second-order information clearly outperform those without.",Online,"Chen, Wei-Fan  and
Al Khatib, Khalid  and
Stein, Benno  and
Wachsmuth, Henning",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.383,November,4290--4300,Association for Computational Linguistics,Detecting Media Bias in News Articles using {G}aussian Bias Distributions,https://aclanthology.org/2020.findings-emnlp.383,2020,,,,,
456,inproceedings,li-etal-2020-branching,"Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will inflate the performances on languages with the same branch it biases to. In this work, we propose quantitatively measuring the branching bias by comparing the performance gap on a language and its reversed language, which is agnostic to both language models and extracting methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely feature definitions, parsing algorithms, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias.",Online,"Li, Huayang  and
Liu, Lemao  and
Huang, Guoping  and
Shi, Shuming",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.401,November,4473--4478,Association for Computational Linguistics,On the Branching Bias of Syntax Extracted from Pre-trained Language Models,https://aclanthology.org/2020.findings-emnlp.401,2020,,,,,
457,inproceedings,huguet-cabot-etal-2020-pragmatics,"There has been an increased interest in modelling political discourse within the natural language processing (NLP) community, in tasks such as political bias and misinformation detection, among others. Metaphor-rich and emotion-eliciting communication strategies are ubiquitous in political rhetoric, according to social science research. Yet, none of the existing computational models of political discourse has incorporated these phenomena. In this paper, we present the first joint models of metaphor, emotion and political rhetoric, and demonstrate that they advance performance in three tasks: predicting political perspective of news articles, party affiliation of politicians and framing of policy issues.",Online,"Huguet Cabot, Pere-Llu{\'\i}s  and
Dankers, Verna  and
Abadi, David  and
Fischer, Agneta  and
Shutova, Ekaterina",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.402,November,4479--4488,Association for Computational Linguistics,"{T}he {P}ragmatics behind {P}olitics: {M}odelling {M}etaphor, {F}raming and {E}motion in {P}olitical {D}iscourse",https://aclanthology.org/2020.findings-emnlp.402,2020,,,,,
458,inproceedings,maini-etal-2020-pool,"Pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. However, the reasons for their enhanced performance are largely unexamined. In this work, we examine three commonly used pooling techniques (mean-pooling, max-pooling, and attention, and propose *max-attention*, a novel variant that captures interactions among predictive tokens in a sentence. Using novel experiments, we demonstrate that pooling architectures substantially differ from their non-pooling equivalents in their learning ability and positional biases: (i) pooling facilitates better gradient flow than BiLSTMs in initial training epochs, and (ii) BiLSTMs are biased towards tokens at the beginning and end of the input, whereas pooling alleviates this bias. Consequently, we find that pooling yields large gains in low resource scenarios, and instances when salient words lie towards the middle of the input. Across several text classification tasks, we find max-attention to frequently outperform other pooling techniques.",Online,"Maini, Pratyush  and
Kolluru, Keshav  and
Pruthi, Danish  and
{Mausam}",Findings of the Association for Computational Linguistics: EMNLP 2020,10.18653/v1/2020.findings-emnlp.410,November,4568--4586,Association for Computational Linguistics,Why and when should you pool? Analyzing Pooling in Recurrent Architectures,https://aclanthology.org/2020.findings-emnlp.410,2020,,,,,
459,inproceedings,strzalkowski-etal-2020-generating,"In this paper we describe computational ethnography study to demonstrate how machine learning techniques can be utilized to exploit bias resident in language data produced by communities with online presence. Specifically, we leverage the use of figurative language (i.e., the choice of metaphors) in online text (e.g., news media, blogs) produced by distinct communities to obtain models of community worldviews that can be shown to be distinctly biased and thus different from other communities{'} models. We automatically construct metaphor-based community models for two distinct scenarios: gun rights and marriage equality. We then conduct a series of experiments to validate the hypothesis that the metaphors found in each community{'}s online language convey the bias in the community{'}s worldview.",Online,"Strzalkowski, Tomek  and
Newheiser, Anna  and
Kemper, Nathan  and
Sa, Ning  and
Acharya, Bharvee  and
Katsios, Gregorios",Proceedings of the Second Workshop on Figurative Language Processing,10.18653/v1/2020.figlang-1.23,July,165--175,Association for Computational Linguistics,Generating Ethnographic Models from Communities{'} Online Data,https://aclanthology.org/2020.figlang-1.23,2020,,,,,
460,inproceedings,stemle-onysko-2020-testing,"This paper describes the adaptation and application of a neural network system for the automatic detection of metaphors. The LSTM BiRNN system participated in the shared task of metaphor identification that was part of the Second Workshop of Figurative Language Processing (FigLang2020) held at the Annual Conference of the Association for Computational Linguistics (ACL2020). The particular focus of our approach is on the potential influence that the metadata given in the ETS Corpus of Non-Native Written English might have on the automatic detection of metaphors in this dataset. The article first discusses the annotated ETS learner data, highlighting some of its peculiarities and inherent biases of metaphor use. A series of evaluations follow in order to test whether specific metadata influence the system performance in the task of automatic metaphor identification. The system is available under the APLv2 open-source license.",Online,"Stemle, Egon  and
Onysko, Alexander",Proceedings of the Second Workshop on Figurative Language Processing,10.18653/v1/2020.figlang-1.35,July,256--263,Association for Computational Linguistics,Testing the role of metadata in metaphor identification,https://aclanthology.org/2020.figlang-1.35,2020,,,,,
461,inproceedings,schoch-etal-2020-problem,"Despite recent efforts reviewing current human evaluation practices for natural language generation (NLG) research, the lack of reported question wording and potential for framing effects or cognitive biases influencing results has been widely overlooked. In this opinion paper, we detail three possible framing effects and cognitive biases that could be imposed on human evaluation in NLG. Based on this, we make a call for increased transparency for human evaluation in NLG and propose the concept of human evaluation statements. We make several recommendations for design details to report that could potentially influence results, such as question wording, and suggest that reporting pertinent design details can help increase comparability across studies as well as reproducibility of results.","Online (Dublin, Ireland)","Schoch, Stephanie  and
Yang, Diyi  and
Ji, Yangfeng",Proceedings of the 1st Workshop on Evaluating NLG Evaluation,,December,10--16,Association for Computational Linguistics,"{``}This is a Problem, Don{'}t You Agree?{''} Framing and Bias in Human Evaluation for Natural Language Generation",https://aclanthology.org/2020.evalnlgeval-1.2,2020,,,,,
462,inproceedings,freitag-etal-2020-bleu,"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.",Online,"Freitag, Markus  and
Grangier, David  and
Caswell, Isaac",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.5,November,61--71,Association for Computational Linguistics,{BLEU} might be Guilty but References are not Innocent,https://aclanthology.org/2020.emnlp-main.5,2020,,,,,
463,inproceedings,banerjee-baral-2020-self,"The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",Online,"Banerjee, Pratyay  and
Baral, Chitta",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.11,November,151--162,Association for Computational Linguistics,Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering,https://aclanthology.org/2020.emnlp-main.11,2020,,,,,
464,inproceedings,warstadt-etal-2020-learning,"One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning. We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa{\_}BASE. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa{\_}BASE does consistently demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",Online,"Warstadt, Alex  and
Zhang, Yian  and
Li, Xiaocheng  and
Liu, Haokun  and
Bowman, Samuel R.",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.16,November,217--235,Association for Computational Linguistics,Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually),https://aclanthology.org/2020.emnlp-main.16,2020,,,,,
465,inproceedings,dinan-etal-2020-multi,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a new, crowdsourced evaluation benchmark. Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers. We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",Online,"Dinan, Emily  and
Fan, Angela  and
Wu, Ledell  and
Weston, Jason  and
Kiela, Douwe  and
Williams, Adina",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.23,November,314--331,Association for Computational Linguistics,Multi-Dimensional Gender Bias Classification,https://aclanthology.org/2020.emnlp-main.23,2020,,,,,
466,inproceedings,lertvittayakumjorn-etal-2020-find,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND {--} a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).",Online,"Lertvittayakumjorn, Piyawat  and
Specia, Lucia  and
Toni, Francesca",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.24,November,332--348,Association for Computational Linguistics,{FIND}: {H}uman-in-the-{L}oop {D}ebugging {D}eep {T}ext {C}lassifiers,https://aclanthology.org/2020.emnlp-main.24,2020,,,,,
467,inproceedings,field-tsvetkov-2020-unsupervised,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.",Online,"Field, Anjalie  and
Tsvetkov, Yulia",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.44,November,596--608,Association for Computational Linguistics,Unsupervised Discovery of Implicit Gender Bias,https://aclanthology.org/2020.emnlp-main.44,2020,,,,,
468,inproceedings,wu-etal-2020-de,"Court{'}s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes. In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders. The attentional encoder leverages the plaintiff{'}s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized. The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court{'}s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model. Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.",Online,"Wu, Yiquan  and
Kuang, Kun  and
Zhang, Yating  and
Liu, Xiaozhong  and
Sun, Changlong  and
Xiao, Jun  and
Zhuang, Yueting  and
Si, Luo  and
Wu, Fei",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.56,November,763--780,Association for Computational Linguistics,De-Biased Court{'}s View Generation with Causality,https://aclanthology.org/2020.emnlp-main.56,2020,,,,,
469,inproceedings,hahn-etal-2020-localization,"We present WHERE ARE YOU? (WAY), a dataset of {\textasciitilde}6k dialogs in which two humans {--} an Observer and a Locator {--} complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator must localize the Observer in a detailed top-down map by asking questions and giving instructions. Based on this dataset, we define three challenging tasks: Localization from Embodied Dialog or LED (localizing the Observer from dialog history), Embodied Visual Dialog (modeling the Observer), and Cooperative Localization (modeling both agents). In this paper, we focus on the LED task {--} providing a strong baseline model with detailed ablations characterizing both dataset biases and the importance of various modeling choices. Our best model achieves 32.7{\%} success at identifying the Observer{'}s location within 3m in unseen buildings, vs. 70.4{\%} for human Locators.",Online,"Hahn, Meera  and
Krantz, Jacob  and
Batra, Dhruv  and
Parikh, Devi  and
Rehg, James  and
Lee, Stefan  and
Anderson, Peter",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.59,November,806--822,Association for Computational Linguistics,Where Are You? Localization from Embodied Dialog,https://aclanthology.org/2020.emnlp-main.59,2020,,,,,
470,inproceedings,liu-etal-2020-mitigating,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people{'}s gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.",Online,"Liu, Haochen  and
Wang, Wentao  and
Wang, Yiqi  and
Liu, Hui  and
Liu, Zitao  and
Tang, Jiliang",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.64,November,893--903,Association for Computational Linguistics,Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,https://aclanthology.org/2020.emnlp-main.64,2020,,,,,
471,inproceedings,upadhye-etal-2020-predicting,"Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.",Online,"Upadhye, Shiva  and
Bergen, Leon  and
Kehler, Andrew",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.70,November,977--982,Association for Computational Linguistics,Predicting Reference: What do Language Models Learn about Discourse Models?,https://aclanthology.org/2020.emnlp-main.70,2020,,,,,
472,inproceedings,yan-etal-2020-multi,"Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT{'}14 English-to-German and WMT{'}18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1{\%}). In addition, our methods also surpass the Transformer-Big model, with only 54{\%} of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.",Online,"Yan, Jianhao  and
Meng, Fandong  and
Zhou, Jie",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.77,November,1047--1059,Association for Computational Linguistics,Multi-Unit Transformers for Neural Machine Translation,https://aclanthology.org/2020.emnlp-main.77,2020,,,,,
473,inproceedings,ko-etal-2020-look,"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48{\%} to 81.64{\%} when trained on a biased SQuAD dataset.",Online,"Ko, Miyoung  and
Lee, Jinhyuk  and
Kim, Hyunjae  and
Kim, Gangwoo  and
Kang, Jaewoo",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.84,November,1109--1121,Association for Computational Linguistics,Look at the First Sentence: Position Bias in Question Answering,https://aclanthology.org/2020.emnlp-main.84,2020,,,,,
474,inproceedings,gao-etal-2020-setconv,"For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution. We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.",Online,"Gao, Yang  and
Li, Yi-Fan  and
Lin, Yu  and
Aggarwal, Charu  and
Khan, Latifur",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.98,November,1284--1294,Association for Computational Linguistics,{S}et{C}onv: {A} {N}ew {A}pproach for {L}earning from {I}mbalanced {D}ata,https://aclanthology.org/2020.emnlp-main.98,2020,,,,,
475,inproceedings,qiu-etal-2020-structured,"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.",Online,"Qiu, Liang  and
Zhao, Yizhou  and
Shi, Weiyan  and
Liang, Yuan  and
Shi, Feng  and
Yuan, Tao  and
Yu, Zhou  and
Zhu, Song-Chun",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.148,November,1889--1899,Association for Computational Linguistics,Structured Attention for Unsupervised Dialogue Structure Induction,https://aclanthology.org/2020.emnlp-main.148,2020,,,,,
476,inproceedings,nangia-etal-2020-crows,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",Online,"Nangia, Nikita  and
Vania, Clara  and
Bhalerao, Rasika  and
Bowman, Samuel R.",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.154,November,1953--1967,Association for Computational Linguistics,{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,https://aclanthology.org/2020.emnlp-main.154,2020,,,,,
477,inproceedings,zhao-chang-2020-logan,"Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus. However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model. In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region. To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering. Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",Online,"Zhao, Jieyu  and
Chang, Kai-Wei",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.155,November,1968--1977,Association for Computational Linguistics,{LOGAN}: Local Group Bias Detection by Clustering,https://aclanthology.org/2020.emnlp-main.155,2020,,,,,
478,inproceedings,munro-morrison-2020-detecting,"We report that state-of-the-art parsers consistently failed to identify {``}hers{''} and {``}theirs{''} as pronouns but identified the masculine equivalent {``}his{''}. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is {\_}amplified{\_} in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.",Online,"Munro, Robert  and
Morrison, Alex (Carmen)",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.157,November,2011--2017,Association for Computational Linguistics,Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation,https://aclanthology.org/2020.emnlp-main.157,2020,,,,,
479,inproceedings,omura-etal-2020-method,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9{\%}), the accuracy of a high-performance transfer learning model is reasonably low (76.0{\%}). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research.",Online,"Omura, Kazumasa  and
Kawahara, Daisuke  and
Kurohashi, Sadao",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.192,November,2450--2460,Association for Computational Linguistics,A Method for Building a Commonsense Inference Dataset based on Basic Events,https://aclanthology.org/2020.emnlp-main.192,2020,,,,,
480,inproceedings,gonzalez-etal-2020-type,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are {``}hallucinatory{''}, e.g., disambiguating gender-ambiguous occurrences of {`}doctor{'} as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of {`}the doctor removed his mask{'} is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.",Online,"Gonz{\'a}lez, Ana Valeria  and
Barrett, Maria  and
Hvingelby, Rasmus  and
Webster, Kellie  and
S{\o}gaard, Anders",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.209,November,2637--2648,Association for Computational Linguistics,Type {B} Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,https://aclanthology.org/2020.emnlp-main.209,2020,,,,,
481,inproceedings,vargas-cotterell-2020-exploring,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",Online,"Vargas, Francisco  and
Cotterell, Ryan",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.232,November,2902--2913,Association for Computational Linguistics,Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,https://aclanthology.org/2020.emnlp-main.232,2020,,,,,
482,inproceedings,chen-lee-2020-incorporating,"Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns. This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice. Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.",Online,"Chen, Ruey-Cheng  and
Lee, Chia-Jung",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.251,November,3105--3110,Association for Computational Linguistics,Incorporating Behavioral Hypotheses for Query Generation,https://aclanthology.org/2020.emnlp-main.251,2020,,,,,
483,inproceedings,de-cao-etal-2020-decisions,"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure{'}s objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model {`}knows{'} it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network {`}knows{'} at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.",Online,"De Cao, Nicola  and
Schlichtkrull, Michael Sejr  and
Aziz, Wilker  and
Titov, Ivan",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.262,November,3243--3255,Association for Computational Linguistics,How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,https://aclanthology.org/2020.emnlp-main.262,2020,,,,,
484,inproceedings,chen-etal-2020-bridging,"Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance. However, these models suffer from a huge gap between prior and posterior knowledge selection. Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information. Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference. Here, we deal with these issues on two aspects: (1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); (2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.",Online,"Chen, Xiuyi  and
Meng, Fandong  and
Li, Peng  and
Chen, Feilong  and
Xu, Shuang  and
Xu, Bo  and
Zhou, Jie",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.275,November,3426--3437,Association for Computational Linguistics,Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,https://aclanthology.org/2020.emnlp-main.275,2020,,,,,
485,inproceedings,ku-etal-2020-room,"We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more references to visible entities. Furthermore, each word in an instruction is time-aligned to the virtual poses of instruction creators and validators. We establish baseline scores for monolingual and multilingual settings and multitask learning when including Room-to-Room annotations (Anderson et al., 2018). We also provide results for a model that learns from synchronized pose traces by focusing only on portions of the panorama attended to in human demonstrations. The size, scope and detail of RxR dramatically expands the frontier for research on embodied language agents in photorealistic simulated environments.",Online,"Ku, Alexander  and
Anderson, Peter  and
Patel, Roma  and
Ie, Eugene  and
Baldridge, Jason",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.356,November,4392--4412,Association for Computational Linguistics,Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,https://aclanthology.org/2020.emnlp-main.356,2020,,,,,
486,inproceedings,hawkins-etal-2020-investigating,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker{'}s choice of construction is known to depend on multiple factors, including the choice of main verb {--} a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",Online,"Hawkins, Robert  and
Yamakoshi, Takateru  and
Griffiths, Thomas  and
Goldberg, Adele",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.376,November,4653--4663,Association for Computational Linguistics,Investigating representations of verb bias in neural language models,https://aclanthology.org/2020.emnlp-main.376,2020,,,,,
487,inproceedings,ethayarajh-jurafsky-2020-utility,"Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values in models, such as compactness, fairness, and energy efficiency. In this opinion paper, we study the divergence between what is incentivized by leaderboards and what is useful in practice through the lens of microeconomic theory. We frame both the leaderboard and NLP practitioners as consumers and the benefit they get from a model as its utility to them. With this framing, we formalize how leaderboards {--} in their current form {--} can be poor proxies for the NLP community at large. For example, a highly inefficient model would provide less utility to practitioners but not to a leaderboard, since it is a cost that only the former must bear. To allow practitioners to better estimate a model{'}s utility to them, we advocate for more transparency on leaderboards, such as the reporting of statistics that are of practical concern (e.g., model size, energy efficiency, and inference latency).",Online,"Ethayarajh, Kawin  and
Jurafsky, Dan",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.393,November,4846--4853,Association for Computational Linguistics,Utility is in the Eye of the User: A Critique of {NLP} Leaderboards,https://aclanthology.org/2020.emnlp-main.393,2020,,,,,
488,inproceedings,yordanov-etal-2020-objective,"Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have been introduced. The variety of training datasets and pre-trained language models used in these works makes it unclear whether the choice of training objective is critical. In this work, we make a fair comparison of the performance and seed-wise stability of four models that represent the four categories of objectives. Our experiments show that the objective of sequence ranking performs the best in-domain, while the objective of semantic similarity between candidates and pronoun performs the best out-of-domain. We also observe a seed-wise instability of the model using sequence ranking, which is not the case when the other objectives are used.",Online,"Yordanov, Yordan  and
Camburu, Oana-Maria  and
Kocijan, Vid  and
Lukasiewicz, Thomas",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.402,November,4963--4969,Association for Computational Linguistics,Does the {O}bjective {M}atter? {C}omparing {T}raining {O}bjectives for {P}ronoun {R}esolution,https://aclanthology.org/2020.emnlp-main.402,2020,,,,,
489,inproceedings,baly-etal-2020-detect,"We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology {--}left, center, or right{--}, which is well-balanced across both topics and media. We further use a challenging experimental setup where the test examples come from media that were not seen during training, which prevents the model from learning to detect the source of the target news article instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a specially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improving article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup.",Online,"Baly, Ramy  and
Da San Martino, Giovanni  and
Glass, James  and
Nakov, Preslav",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.404,November,4982--4991,Association for Computational Linguistics,We Can Detect Your Bias: Predicting the Political Ideology of News Articles,https://aclanthology.org/2020.emnlp-main.404,2020,,,,,
490,inproceedings,welleck-etal-2020-consistency,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms {--} greedy search, beam search, top-k sampling, and nucleus sampling {--} are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.",Online,"Welleck, Sean  and
Kulikov, Ilia  and
Kim, Jaedeok  and
Pang, Richard Yuanzhe  and
Cho, Kyunghyun",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.448,November,5553--5568,Association for Computational Linguistics,Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,https://aclanthology.org/2020.emnlp-main.448,2020,,,,,
491,inproceedings,han-etal-2020-domain,"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",Online,"Han, Rujun  and
Zhou, Yichao  and
Peng, Nanyun",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.461,November,5717--5729,Association for Computational Linguistics,Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,https://aclanthology.org/2020.emnlp-main.461,2020,,,,,
492,inproceedings,hua-etal-2020-shot,"Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level. This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions. Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data. The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set. Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1{\%} of the training set. We have released our code at https://github.com/DevinJake/MRL-CQA.",Online,"Hua, Yuncheng  and
Li, Yuan-Fang  and
Haffari, Gholamreza  and
Qi, Guilin  and
Wu, Tongtong",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.469,November,5827--5837,Association for Computational Linguistics,Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,https://aclanthology.org/2020.emnlp-main.469,2020,,,,,
493,inproceedings,lin-etal-2020-autoregressive,"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.",Online,"Lin, Alexander  and
Wohlwend, Jeremy  and
Chen, Howard  and
Lei, Tao",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.494,November,6121--6133,Association for Computational Linguistics,Autoregressive Knowledge Distillation through Imitation Learning,https://aclanthology.org/2020.emnlp-main.494,2020,,,,,
494,inproceedings,liu-etal-2020-exploring,"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10{\%} on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at \url{https://github.com/thunlp/explore-and-evaluate}.",Online,"Liu, Zhiyuan  and
Cao, Yixin  and
Pan, Liangming  and
Li, Juanzi  and
Liu, Zhiyuan  and
Chua, Tat-Seng",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.515,November,6355--6364,Association for Computational Linguistics,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment",https://aclanthology.org/2020.emnlp-main.515,2020,,,,,
495,inproceedings,hegel-etal-2020-substance,"Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently. In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint. We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs. Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document. Finally, we analyze our model{'}s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.",Online,"Hegel, Allison  and
Rao, Sudha  and
Celikyilmaz, Asli  and
Dolan, Bill",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.526,November,6485--6504,Association for Computational Linguistics,Substance over {S}tyle: {D}ocument-{L}evel {T}argeted {C}ontent {T}ransfer,https://aclanthology.org/2020.emnlp-main.526,2020,,,,,
496,inproceedings,papadimitriou-jurafsky-2020-learning,"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.",Online,"Papadimitriou, Isabel  and
Jurafsky, Dan",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.554,November,6829--6839,Association for Computational Linguistics,{L}earning {M}usic {H}elps {Y}ou {R}ead: {U}sing Transfer to Study Linguistic Structure in Language Models,https://aclanthology.org/2020.emnlp-main.554,2020,,,,,
497,inproceedings,shwartz-etal-2020-grounded,"Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for {`}Donald is a{'} substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.",Online,"Shwartz, Vered  and
Rudinger, Rachel  and
Tafjord, Oyvind",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.556,November,6850--6861,Association for Computational Linguistics,{``}You are grounded!{''}: Latent Name Artifacts in Pre-trained Language Models,https://aclanthology.org/2020.emnlp-main.556,2020,,,,,
498,inproceedings,fisher-etal-2020-debiasing,"It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases. Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially. We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss. We then add sensitive attributes back on in whitelisted cases. Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.",Online,"Fisher, Joseph  and
Mittal, Arpit  and
Palfrey, Dave  and
Christodoulopoulos, Christos",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.595,November,7332--7345,Association for Computational Linguistics,Debiasing knowledge graph embeddings,https://aclanthology.org/2020.emnlp-main.595,2020,,,,,
499,inproceedings,ma-etal-2020-powertransformer,"Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless ({``}{\_}She daydreams about being a doctor{\_}{''}) while a man is portrayed as more proactive and powerful ({``}{\_}He pursues his dream of being a doctor{\_}{''}). We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals. We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates. One key challenge of our task is the lack of parallel corpora. To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models. Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks. Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.",Online,"Ma, Xinyao  and
Sap, Maarten  and
Rashkin, Hannah  and
Choi, Yejin",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.602,November,7426--7441,Association for Computational Linguistics,{P}ower{T}ransformer: Unsupervised Controllable Revision for Biased Language Correction,https://aclanthology.org/2020.emnlp-main.602,2020,,,,,
500,inproceedings,utama-etal-2020-towards,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models{'} reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.",Online,"Utama, Prasetya Ajie  and
Moosavi, Nafise Sadat  and
Gurevych, Iryna",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.613,November,7597--7610,Association for Computational Linguistics,Towards Debiasing {NLU} Models from Unknown Biases,https://aclanthology.org/2020.emnlp-main.613,2020,,,,,
501,inproceedings,shi-etal-2020-role,"We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein, 2018) on the same labeled examples they access. When training on the 1,700 examples, or even when using only 50 examples for training and 5 for development, such a few-shot parsing approach can outperform all the unsupervised parsing methods by a significant margin. Few-shot parsing can be further improved by a simple data augmentation method and self-training. This suggests that, in order to arrive at fair conclusions, we should carefully consider the amount of labeled data used for model development. We propose two protocols for future work on unsupervised parsing: (i) use fully unsupervised criteria for hyperparameter tuning and model selection; (ii) use as few labeled examples as possible for model development, and compare to few-shot parsing trained on the same labeled examples.",Online,"Shi, Haoyue  and
Livescu, Karen  and
Gimpel, Kevin",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.614,November,7611--7621,Association for Computational Linguistics,On the Role of Supervision in Unsupervised Constituency Parsing,https://aclanthology.org/2020.emnlp-main.614,2020,,,,,
502,inproceedings,han-tsvetkov-2020-fortifying,"Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity. Just a handful of probing examples are used to surface orders of magnitude more disguised offenses. We augment the toxic speech detector{'}s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",Online,"Han, Xiaochuang  and
Tsvetkov, Yulia",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.622,November,7732--7739,Association for Computational Linguistics,Fortifying Toxic Speech Detectors Against Veiled Toxicity,https://aclanthology.org/2020.emnlp-main.622,2020,,,,,
503,inproceedings,ein-dor-etal-2020-active,"Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.",Online,"Ein-Dor, Liat  and
Halfon, Alon  and
Gera, Ariel  and
Shnarch, Eyal  and
Dankin, Lena  and
Choshen, Leshem  and
Danilevsky, Marina  and
Aharonov, Ranit  and
Katz, Yoav  and
Slonim, Noam",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.638,November,7949--7962,Association for Computational Linguistics,{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy,https://aclanthology.org/2020.emnlp-main.638,2020,,,,,
504,inproceedings,scialom-etal-2020-mlsum,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages {--} namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset.",Online,"Scialom, Thomas  and
Dray, Paul-Alexis  and
Lamprier, Sylvain  and
Piwowarski, Benjamin  and
Staiano, Jacopo",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.647,November,8051--8067,Association for Computational Linguistics,{MLSUM}: The Multilingual Summarization Corpus,https://aclanthology.org/2020.emnlp-main.647,2020,,,,,
505,inproceedings,dinan-etal-2020-queens,"Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods{---}including the quantity of gendered words, a dialogue safety classifier, and human assessments{---}all of which show that our models generate less gendered, but equally engaging chit-chat responses.",Online,"Dinan, Emily  and
Fan, Angela  and
Williams, Adina  and
Urbanek, Jack  and
Kiela, Douwe  and
Weston, Jason",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.656,November,8173--8188,Association for Computational Linguistics,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,https://aclanthology.org/2020.emnlp-main.656,2020,,,,,
506,inproceedings,zhou-etal-2020-curse,"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models.",Online,"Zhou, Xiang  and
Nie, Yixin  and
Tan, Hao  and
Bansal, Mohit",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.659,November,8215--8228,Association for Computational Linguistics,"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions",https://aclanthology.org/2020.emnlp-main.659,2020,,,,,
507,inproceedings,stacey-etal-2020-avoiding,"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other NLI datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.",Online,"Stacey, Joe  and
Minervini, Pasquale  and
Dubossarsky, Haim  and
Riedel, Sebastian  and
Rockt{\""a}schel, Tim",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.665,November,8281--8291,Association for Computational Linguistics,{A}voiding the {H}ypothesis-{O}nly {B}ias in {N}atural {L}anguage {I}nference via {E}nsemble {A}dversarial {T}raining,https://aclanthology.org/2020.emnlp-main.665,2020,,,,,
508,inproceedings,goodman-etal-2020-teaforn,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.",Online,"Goodman, Sebastian  and
Ding, Nan  and
Soricut, Radu",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.702,November,8704--8717,Association for Computational Linguistics,{T}ea{F}or{N}: Teacher-Forcing with N-grams,https://aclanthology.org/2020.emnlp-main.702,2020,,,,,
509,inproceedings,gyanendro-singh-etal-2020-sentiment,"Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The generated representations are further ensembled and classified using a neural-based early fusion approach. Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network. From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts. Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.",Online,"Gyanendro Singh, Loitongbam  and
Mitra, Anasua  and
Ranbir Singh, Sanasam",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.718,November,8932--8946,Association for Computational Linguistics,Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,https://aclanthology.org/2020.emnlp-main.718,2020,,,,,
510,inproceedings,li-etal-2020-improving-text,"Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes. We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation. An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences. The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.",Online,"Li, Jianqiao  and
Li, Chunyuan  and
Wang, Guoyin  and
Fu, Hao  and
Lin, Yuhchen  and
Chen, Liqun  and
Zhang, Yizhe  and
Tao, Chenyang  and
Zhang, Ruiyi  and
Wang, Wenlin  and
Shen, Dinghan  and
Yang, Qian  and
Carin, Lawrence",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),10.18653/v1/2020.emnlp-main.735,November,9144--9156,Association for Computational Linguistics,Improving Text Generation with Student-Forcing Optimal Transport,https://aclanthology.org/2020.emnlp-main.735,2020,,,,,
511,inproceedings,tenney-etal-2020-language,"We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models{---}including classification, seq2seq, and structured prediction{---}and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.",Online,"Tenney, Ian  and
Wexler, James  and
Bastings, Jasmijn  and
Bolukbasi, Tolga  and
Coenen, Andy  and
Gehrmann, Sebastian  and
Jiang, Ellen  and
Pushkarna, Mahima  and
Radebaugh, Carey  and
Reif, Emily  and
Yuan, Ann",Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/2020.emnlp-demos.15,October,107--118,Association for Computational Linguistics,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models",https://aclanthology.org/2020.emnlp-demos.15,2020,,,,,
512,inproceedings,shimura-etal-2020-aspect,"Many e-commerce services provide customer review systems. Previous laboratory studies have indicated that the ratings recorded by these systems differ from the actual evaluations of the users, owing to the influence of historical ratings in the system. Some studies have proposed using real-world datasets to model rating prediction. Herein, we propose an aspect-similarity-aware historical influence model for rating prediction using natural language processing techniques. In general, each user provides a rating considering different aspects. Thus, it can be assumed that historical ratings provided considering similar aspects to those of later ones will influence evaluations of users more. By focusing on the review-topic similarities, we show that our method predicts ratings more accurately than the previous historical-inference-aware model. In addition, we examine whether our model can predict {``}intrinsic rating,{''} which is given if users were not influenced by historical ratings. We performed an intrinsic rating prediction task, and showed that our model achieved improved performance. Our method can be useful to debias user ratings collected by customer review systems. The debiased ratings help users to make decision properly and systems to provide helpful recommendations. This might improve the user experience of e-commerce services.","Barcelona, Spain","Shimura, Ryo  and
Misawa, Shotaro  and
Sato, Masahiro  and
Taniguchi, Tomoki  and
Ohkuma, Tomoko",Proceedings of Workshop on Natural Language Processing in E-Commerce,,December,76--86,Association for Computational Linguistics,Aspect-Similarity-Aware Historical Influence Modeling for Rating Prediction,https://aclanthology.org/2020.ecomnlp-1.8,2020,,,,,
513,inproceedings,taylor-keselj-2020-e,"In recent years, the focus of e-Commerce research has been on better understanding the relationship between the internet marketplace, customers, and goods and services. This has been done by examining information that can be gleaned from consumer information, recommender systems, click rates, or the way purchasers go about making buying decisions, for example. This paper takes a very different approach and examines the companies themselves. In the past ten years, e-Commerce giants such as Amazon, Skymall, Wayfair, and Groupon have been embroiled in class action security lawsuits promulgated under Rule 10b(5), which, in short, is one of the Securities and Exchange Commission{'}s main rules surrounding fraud. Lawsuits are extremely expensive to the company and can damage a company{'}s brand extensively, with the shareholders left to suffer the consequences. We examined the Management Discussion and Analysis and the Market Risks for 96 companies using sentiment analysis on selected financial measures and found that we were able to predict the outcome of the lawsuits in our dataset using sentiment (tone) alone to a recall of 0.8207 using the Random Forest classifier. We believe that this is an important contribution as it has cross-domain implications and potential, and opens up new areas of research in e-Commerce, finance, and law, as the settlements from the class action lawsuits in our dataset alone are in excess of {\$}1.6 billion dollars, in aggregate.","Seattle, WA, USA","Taylor, Stacey  and
Keselj, Vlado",Proceedings of The 3rd Workshop on e-Commerce and NLP,10.18653/v1/2020.ecnlp-1.12,July,77--85,Association for Computational Linguistics,e-Commerce and Sentiment Analysis: Predicting Outcomes of Class Action Lawsuits,https://aclanthology.org/2020.ecnlp-1.12,2020,,,,,
514,inproceedings,gois-etal-2020-learning,"Recent research in neural machine translation has explored flexible generation orders, as an alternative to left-to-right generation. However, training non-monotonic models brings a new complication: how to search for a good ordering when there is a combinatorial explosion of orderings arriving at the same final result? Also, how do these automatic orderings compare with the actual behaviour of human translators? Current models rely on manually built biases or are left to explore all possibilities on their own. In this paper, we analyze the orderings produced by human post-editors and use them to train an automatic post-editing system. We compare the resulting system with those trained with left-to-right and random post-editing orderings. We observe that humans tend to follow a nearly left-to-right order, but with interesting deviations, such as preferring to start by correcting punctuation or verbs.","Lisboa, Portugal","G{\'o}is, Ant{\'o}nio  and
Cho, Kyunghyun  and
Martins, Andr{\'e}",Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,,November,205--214,European Association for Machine Translation,Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings,https://aclanthology.org/2020.eamt-1.22,2020,,,,,
515,inproceedings,exel-etal-2020-terminology,"This paper examines approaches to bias a neural machine translation model to adhere to terminology constraints in an industrial setup. In particular, we investigate variations of the approach by Dinu et al. (2019), which uses inline annotation of the target terms in the source segment plus source factor embeddings during training and inference, and compare them to constrained decoding. We describe the challenges with respect to terminology in our usage scenario at SAP and show how far the investigated methods can help to overcome them. We extend the original study to a new language pair and provide an in-depth evaluation including an error classification and a human evaluation.","Lisboa, Portugal","Exel, Miriam  and
Buschbeck, Bianka  and
Brandt, Lauritz  and
Doneva, Simona",Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,,November,271--280,European Association for Machine Translation,Terminology-Constrained Neural Machine Translation at {SAP},https://aclanthology.org/2020.eamt-1.29,2020,,,,,
516,inproceedings,castilho-2020-document,"Document-level (doc-level) human eval-uation of machine translation (MT) has raised interest in the community after a fewattempts have disproved claims of {``}human parity{''} (Toral et al., 2018; Laubli et al.,2018). However, little is known about bestpractices regarding doc-level human evalu-ation. The goal of this project is to identifywhich methodologies better cope with i)the current state-of-the-art (SOTA) humanmetrics, ii) a possible complexity when as-signing a single score to a text consisted of{`}good{'} and {`}bad{'} sentences, iii) a possibletiredness bias in doc-level set-ups, and iv)the difference in inter-annotator agreement(IAA) between sentence and doc-level set-ups.","Lisboa, Portugal","Castilho, Sheila",Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,,November,455--456,European Association for Machine Translation,"Document-Level Machine Translation Evaluation Project: Methodology, Effort and Inter-Annotator Agreement",https://aclanthology.org/2020.eamt-1.49,2020,,,,,
517,inproceedings,abzianidze-etal-2020-drs,"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks.",Online,"Abzianidze, Lasha  and
Bos, Johan  and
Oepen, Stephan",Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,10.18653/v1/2020.conll-shared.2,November,23--32,Association for Computational Linguistics,{DRS} at {MRP} 2020: Dressing up Discourse Representation Structures as Graphs,https://aclanthology.org/2020.conll-shared.2,2020,,,,,
518,inproceedings,pannitto-herbelot-2020-recurrent,"Recurrent Neural Networks (RNNs) have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training an LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model{'}s generated output (its {`}babbling{'}), compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.",Online,"Pannitto, Ludovica  and
Herbelot, Aur{\'e}lie",Proceedings of the 24th Conference on Computational Natural Language Learning,10.18653/v1/2020.conll-1.13,November,165--176,Association for Computational Linguistics,Recurrent babbling: evaluating the acquisition of grammar from limited input data,https://aclanthology.org/2020.conll-1.13,2020,,,,,
519,inproceedings,liu-etal-2020-empirical,"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models{'} generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it{'}s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models.",Online,"Liu, Tianyu  and
Xin, Zheng  and
Ding, Xiaoan  and
Chang, Baobao  and
Sui, Zhifang",Proceedings of the 24th Conference on Computational Natural Language Learning,10.18653/v1/2020.conll-1.48,November,596--608,Association for Computational Linguistics,An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference,https://aclanthology.org/2020.conll-1.48,2020,,,,,
520,inproceedings,vorakitphan-etal-2020-regrexit,"Emotion analysis in polarized contexts represents a challenge for Natural Language Processing modeling. As a step in the aforementioned direction, we present a methodology to extend the task of Aspect-based Sentiment Analysis (ABSA) toward the affect and emotion representation in polarized settings. In particular, we adopt the three-dimensional model of affect based on Valence, Arousal, and Dominance (VAD). We then present a Brexit scenario that proves how affect varies toward the same aspect when politically polarized stances are presented. Our approach captures aspect-based polarization from newspapers regarding the Brexit scenario of 1.2m entities at sentence-level. We demonstrate how basic constituents of emotions can be mapped to the VAD model, along with their interactions respecting the polarized context in ABSA settings using biased key-concepts (e.g., {``}stop Brexit{''} vs. {``}support Brexit{''}). Quite intriguingly, the framework achieves to produce coherent aspect evidences of Brexit{'}s stance from key-concepts, showing that VAD influence the support and opposition aspects.","Barcelona, Spain (Online)","Vorakitphan, Vorakit  and
Guerini, Marco  and
Cabrio, Elena  and
Villata, Serena",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.19,December,219--224,International Committee on Computational Linguistics,Regrexit or not Regrexit: Aspect-based Sentiment Analysis in Polarized Contexts,https://aclanthology.org/2020.coling-main.19,2020,,,,,
521,inproceedings,porco-goldwasser-2020-predicting,"The ability to change a person{'}s mind on a given issue depends both on the arguments they are presented with and on their underlying perspectives and biases on that issue. Predicting stance changes require characterizing both aspects and the interaction between them, especially in realistic settings in which stance changes are very rare. In this paper, we suggest a modular learning approach, which decomposes the task into multiple modules, focusing on different aspects of the interaction between users, their beliefs, and the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs.","Barcelona, Spain (Online)","Porco, Aldo  and
Goldwasser, Dan",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.35,December,396--406,International Committee on Computational Linguistics,Predicting Stance Change Using Modular Architectures,https://aclanthology.org/2020.coling-main.35,2020,,,,,
522,inproceedings,vu-etal-2020-multimodal,"Users express their opinions towards entities (e.g., restaurants) via online reviews which can be in diverse forms such as text, ratings, and images. Modeling reviews are advantageous for user behavior understanding which, in turn, supports various user-oriented tasks such as recommendation, sentiment analysis, and review generation. In this paper, we propose MG-PriFair, a multimodal neural-based framework, which generates personalized reviews with privacy and fairness awareness. Motivated by the fact that reviews might contain personal information and sentiment bias, we propose a novel differentially private (dp)-embedding model for training privacy guaranteed embeddings and an evaluation approach for sentiment fairness in the food-review domain. Experiments on our novel review dataset show that MG-PriFair is capable of generating plausibly long reviews while controlling the amount of exploited user data and using the least sentiment biased word embeddings. To the best of our knowledge, we are the first to bring user privacy and sentiment fairness into the review generation task. The dataset and source codes are available at https://github.com/ReML-AI/MG-PriFair.","Barcelona, Spain (Online)","Vu, Xuan-Son  and
Nguyen, Thanh-Son  and
Le, Duc-Trong  and
Jiang, Lili",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.37,December,414--425,International Committee on Computational Linguistics,Multimodal Review Generation with Privacy and Fairness Awareness,https://aclanthology.org/2020.coling-main.37,2020,,,,,
523,inproceedings,rohanian-hough-2020-framing,"We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, language modelling, part-of-speech tagging and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance and is beneficial for future use in conversational agents in psychiatric treatment.","Barcelona, Spain (Online)","Rohanian, Morteza  and
Hough, Julian",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.43,December,497--507,International Committee on Computational Linguistics,Re-framing Incremental Deep Language Models for Dialogue Processing with Multi-task Learning,https://aclanthology.org/2020.coling-main.43,2020,,,,,
524,inproceedings,galassi-etal-2020-cross,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods. Our results indicate that a combination of word embeddings and dynamic time warping performs best.","Barcelona, Spain (Online)","Galassi, Andrea  and
Drazewski, Kasper  and
Lippi, Marco  and
Torroni, Paolo",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.79,December,915--926,International Committee on Computational Linguistics,Cross-lingual Annotation Projection in Legal Texts,https://aclanthology.org/2020.coling-main.79,2020,,,,,
525,inproceedings,li-etal-2020-docbank,"Document layout analysis usually relies on computer vision models to understand documents while ignoring textual information that is vital to capture. Meanwhile, high quality labeled datasets with both visual and textual information are still insufficient. In this paper, we present DocBank, a benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the LaTeX documents available on the arXiv.com. With DocBank, models from different modalities can be compared fairly and multi-modal approaches will be further investigated and boost the performance of document layout analysis. We build several strong baselines and manually split train/dev/test sets for evaluation. Experiment results show that models trained on DocBank accurately recognize the layout information for a variety of documents. The DocBank dataset is publicly available at https://github.com/doc-analysis/DocBank.","Barcelona, Spain (Online)","Li, Minghao  and
Xu, Yiheng  and
Cui, Lei  and
Huang, Shaohan  and
Wei, Furu  and
Li, Zhoujun  and
Zhou, Ming",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.82,December,949--960,International Committee on Computational Linguistics,{D}oc{B}ank: A Benchmark Dataset for Document Layout Analysis,https://aclanthology.org/2020.coling-main.82,2020,,,,,
526,inproceedings,feng-etal-2020-exploring,"We explore end-to-end trained differentiable models that integrate natural logic with neural networks, aiming to keep the backbone of natural language reasoning based on the natural logic formalism while introducing subsymbolic vector representations and neural components. The proposed model adapts module networks to model natural logic operations, which is enhanced with a memory component to model contextual information. Experiments show that the proposed framework can effectively model monotonicity-based reasoning, compared to the baseline neural network models without built-in inductive bias for monotonicity-based reasoning. Our proposed model shows to be robust when transferred from upward to downward inference. We perform further analyses on the performance of the proposed model on aggregation, showing the effectiveness of the proposed subcomponents on helping achieve better intermediate aggregation performance.","Barcelona, Spain (Online)","Feng, Yufei  and
Zheng, Zi{'}ou  and
Liu, Quan  and
Greenspan, Michael  and
Zhu, Xiaodan",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.101,December,1172--1185,International Committee on Computational Linguistics,Exploring End-to-End Differentiable Natural Logic Modeling,https://aclanthology.org/2020.coling-main.101,2020,,,,,
527,inproceedings,anthonio-roth-2020-learn,"In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that {`}revised means better{'} actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70{\%}, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the {`}better version{'} with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity.","Barcelona, Spain (Online)","Anthonio, Talita  and
Roth, Michael",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.117,December,1359--1370,International Committee on Computational Linguistics,What Can We Learn from Noun Substitutions in Revision Histories?,https://aclanthology.org/2020.coling-main.117,2020,,,,,
528,inproceedings,wang-etal-2020-tplinker,"Extracting entities and relations from unstructured text has attracted increasing attention in recent years but remains challenging, due to the intrinsic difficulty in identifying overlapping relations with shared entities. Prior works show that joint learning can result in a noticeable performance gain. However, they usually involve sequential interrelated steps and suffer from the problem of exposure bias. At training time, they predict with the ground truth conditions while at inference it has to make extraction from scratch. This discrepancy leads to error accumulation. To mitigate the issue, we propose in this paper a one-stage joint extraction model, namely, TPLinker, which is capable of discovering overlapping relations sharing one or both entities while being immune from the exposure bias. TPLinker formulates joint extraction as a token pair linking problem and introduces a novel handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Experiment results show that TPLinker performs significantly better on overlapping and multiple relation extraction, and achieves state-of-the-art performance on two public datasets.","Barcelona, Spain (Online)","Wang, Yucheng  and
Yu, Bowen  and
Zhang, Yueyang  and
Liu, Tingwen  and
Zhu, Hongsong  and
Sun, Limin",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.138,December,1572--1582,International Committee on Computational Linguistics,{TPL}inker: Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking,https://aclanthology.org/2020.coling-main.138,2020,,,,,
529,inproceedings,lepori-2020-unequal,"We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.","Barcelona, Spain (Online)","Lepori, Michael",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.151,December,1720--1728,International Committee on Computational Linguistics,Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis,https://aclanthology.org/2020.coling-main.151,2020,,,,,
530,inproceedings,yang-etal-2020-predicting-personal,"Predicting users{'} opinions in their response to social events has important real-world applications, many of which political and social impacts. Existing approaches derive a population{'}s opinion on a going event from large scores of user generated content. In certain scenarios, we may not be able to acquire such content and thus cannot infer an unbiased opinion on those emerging events. To address this problem, we propose to explore opinion on unseen articles based on one{'}s fingerprinting: the prior reading and commenting history. This work presents a focused study on modeling and leveraging fingerprinting techniques to predict a user{'}s future opinion. We introduce a recurrent neural network based model that integrates fingerprinting. We collect a large dataset that consists of event-comment pairs from six news websites. We evaluate the proposed model on this dataset. The results show substantial performance gains demonstrating the effectiveness of our approach.","Barcelona, Spain (Online)","Yang, Fan  and
Dragut, Eduard  and
Mukherjee, Arjun",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.162,December,1802--1807,International Committee on Computational Linguistics,Predicting Personal Opinion on Future Events with Fingerprints,https://aclanthology.org/2020.coling-main.162,2020,,,,,
531,inproceedings,liu-etal-2020-personalized,"The automatic feedback of school assignments is an important application of AI in education. In this work, we focus on the task of personalized multimodal feedback generation, which aims to generate personalized feedback for teachers to evaluate students{'} assignments involving multimodal inputs such as images, audios, and texts. This task involves the representation and fusion of multimodal information and natural language generation, which presents the challenges from three aspects: (1) how to encode and integrate multimodal inputs; (2) how to generate feedback specific to each modality; and (3) how to fulfill personalized feedback generation. In this paper, we propose a novel Personalized Multimodal Feedback Generation Network (PMFGN) armed with a modality gate mechanism and a personalized bias mechanism to address these challenges. Extensive experiments on real-world K-12 education data show that our model significantly outperforms baselines by generating more accurate and diverse feedback. In addition, detailed ablation experiments are conducted to deepen our understanding of the proposed framework.","Barcelona, Spain (Online)","Liu, Haochen  and
Liu, Zitao  and
Wu, Zhongqin  and
Tang, Jiliang",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.166,December,1826--1840,International Committee on Computational Linguistics,Personalized Multimodal Feedback Generation in Education,https://aclanthology.org/2020.coling-main.166,2020,,,,,
532,inproceedings,kim-etal-2020-retrieval,"In this paper, we study review generation given a set of attribute identifiers which are user ID, product ID and rating. This is a difficult subtask of natural language generation since models are limited to the given identifiers, without any specific descriptive information regarding the inputs, when generating the text. The capacity of these models is thus confined and dependent to how well the models can capture vector representations of attributes. We thus propose to additionally leverage references, which are selected from a large pool of texts labeled with one of the attributes, as textual information that enriches inductive biases of given attributes. With these references, we can now pose the problem as an instance of text-to-text generation, which makes the task easier since texts that are syntactically, semantically similar with the output text are provided as input. Using this framework, we address issues such as selecting references from a large candidate set without textual context and improving the model complexity for generation. Our experiments show that our models improve over previous approaches on both automatic and human evaluation metrics.","Barcelona, Spain (Online)","Kim, Jihyeok  and
Choi, Seungtaek  and
Amplayo, Reinald Kim  and
Hwang, Seung-won",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.207,December,2284--2295,International Committee on Computational Linguistics,Retrieval-Augmented Controllable Review Generation,https://aclanthology.org/2020.coling-main.207,2020,,,,,
533,inproceedings,xie-etal-2020-exploring,"Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poorer question quality. The code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.","Barcelona, Spain (Online)","Xie, Yuxi  and
Pan, Liangming  and
Wang, Dongzhe  and
Kan, Min-Yen  and
Feng, Yansong",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.228,December,2534--2546,International Committee on Computational Linguistics,Exploring Question-Specific Rewards for Generating Deep Questions,https://aclanthology.org/2020.coling-main.228,2020,,,,,
534,inproceedings,garimella-etal-2020-judge,"The subjective nature of humor makes computerized humor generation a challenging task. We propose an automatic humor generation framework for filling the blanks in Mad LibsÂ® stories, while accounting for the demographic backgrounds of the desired audience. We collect a dataset consisting of such stories, which are filled in and judged by carefully selected workers on Amazon Mechanical Turk. We build upon the BERT platform to predict location-biased word fillings in incomplete sentences, and we fine-tune BERT to classify location-specific humor in a sentence. We leverage these components to produce YodaLib, a fully-automated Mad Libs style humor generation framework, which selects and ranks appropriate candidate words and sentences in order to generate a coherent and funny story tailored to certain demographics. Our experimental results indicate that YodaLib outperforms a previous semi-automated approach proposed for this task, while also surpassing human annotators in both qualitative and quantitative analyses.","Barcelona, Spain (Online)","Garimella, Aparna  and
Banea, Carmen  and
Hossain, Nabil  and
Mihalcea, Rada",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.253,December,2814--2825,International Committee on Computational Linguistics,"{``}Judge me by my size (noun), do you?{''} {Y}oda{L}ib: A Demographic-Aware Humor Generation Framework",https://aclanthology.org/2020.coling-main.253,2020,,,,,
535,inproceedings,nicolai-silfverberg-2020-noise,"Morphological inflection, like many sequence-to-sequence tasks, sees great performance from recurrent neural architectures when data is plentiful, but performance falls off sharply in lower-data settings. We investigate one aspect of neural seq2seq models that we hypothesize contributes to overfitting - teacher forcing. By creating different training and test conditions, exposure bias increases the likelihood that a system too closely models its training data. Experiments show that teacher-forced models struggle to recover when they enter unknown territory. However, a simple modification to the training algorithm to more closely mimic test conditions creates models that are better able to generalize to unseen environments.","Barcelona, Spain (Online)","Nicolai, Garrett  and
Silfverberg, Miikka",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.255,December,2837--2846,International Committee on Computational Linguistics,Noise Isn{'}t Always Negative: Countering Exposure Bias in Sequence-to-Sequence Inflection Models,https://aclanthology.org/2020.coling-main.255,2020,,,,,
536,inproceedings,conia-navigli-2020-conception,"To date, the most successful word, word sense, and concept modelling techniques have used large corpora and knowledge resources to produce dense vector representations that capture semantic similarities in a relatively low-dimensional space. Most current approaches, however, suffer from a monolingual bias, with their strength depending on the amount of data available across languages. In this paper we address this issue and propose Conception, a novel technique for building language-independent vector representations of concepts which places multilinguality at its core while retaining explicit relationships between concepts. Our approach results in high-coverage representations that outperform the state of the art in multilingual and cross-lingual Semantic Word Similarity and Word Sense Disambiguation, proving particularly robust on low-resource languages. Conception {--} its software and the complete set of representations {--} is available at https://github.com/SapienzaNLP/conception.","Barcelona, Spain (Online)","Conia, Simone  and
Navigli, Roberto",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.291,December,3268--3284,International Committee on Computational Linguistics,"Conception: Multilingually-Enhanced, Human-Readable Concept Vector Representations",https://aclanthology.org/2020.coling-main.291,2020,,,,,
537,inproceedings,gaido-etal-2020-breeding,"In automatic speech translation (ST), traditional cascade approaches involving separate transcription and translation steps are giving ground to increasingly competitive and more robust direct solutions. In particular, by translating speech audio data without intermediate transcription, direct ST models are able to leverage and preserve essential information present in the input (e.g.speaker{'}s vocal characteristics) that is otherwise lost in the cascade framework. Although such ability proved to be useful for gender translation, direct ST is nonetheless affected by gender bias just like its cascade counterpart, as well as machine translation and numerous other natural language processing applications. Moreover, direct ST systems that exclusively rely on vocal biometric features as a gender cue can be unsuitable or even potentially problematic for certain users. Going beyond speech signals, in this paper we compare different approaches to inform direct ST models about the speaker{'}s gender and test their ability to handle gender translation from English into Italian and French. To this aim, we manually annotated large datasets with speak-ers{'} gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong {--} but gender-unaware {--} direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality.","Barcelona, Spain (Online)","Gaido, Marco  and
Savoldi, Beatrice  and
Bentivogli, Luisa  and
Negri, Matteo  and
Turchi, Marco",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.350,December,3951--3964,International Committee on Computational Linguistics,Breeding Gender-aware Direct Speech Translation Systems,https://aclanthology.org/2020.coling-main.350,2020,,,,,
538,inproceedings,liu-etal-2020-gender,"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as {``}gorillas{''}. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.","Barcelona, Spain (Online)","Liu, Haochen  and
Dacon, Jamell  and
Fan, Wenqi  and
Liu, Hui  and
Liu, Zitao  and
Tang, Jiliang",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.390,December,4403--4416,International Committee on Computational Linguistics,Does Gender Matter? Towards Fairness in Dialogue Systems,https://aclanthology.org/2020.coling-main.390,2020,,,,,
539,inproceedings,eikema-aziz-2020-map,"Recent studies have revealed a number of pathologies of neural machine translation (NMT) systems. Hypotheses explaining these mostly suggest there is something fundamentally wrong with NMT as a model or its training algorithm, maximum likelihood estimation (MLE). Most of this evidence was gathered using maximum a posteriori (MAP) decoding, a decision rule aimed at identifying the highest-scoring translation, i.e. the mode. We argue that the evidence corroborates the inadequacy of MAP decoding more than casts doubt on the model and its training algorithm. In this work, we show that translation distributions do reproduce various statistics of the data well, but that beam search strays from such statistics. We show that some of the known pathologies and biases of NMT are due to MAP decoding and not to NMT{'}s statistical assumptions nor MLE. In particular, we show that the most likely translations under the model accumulate so little probability mass that the mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation.","Barcelona, Spain (Online)","Eikema, Bryan  and
Aziz, Wilker",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.398,December,4506--4520,International Committee on Computational Linguistics,Is {MAP} Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation,https://aclanthology.org/2020.coling-main.398,2020,,,,,
540,inproceedings,li-etal-2020-emergent,"While state-of-the-art models that rely upon massively multilingual pretrained encoders achieve sample efficiency in downstream applications, they still require abundant amounts of unlabelled text. Nevertheless, most of the world{'}s languages lack such resources. Hence, we investigate a more radical form of unsupervised knowledge transfer in the absence of linguistic data. In particular, for the first time we pretrain neural networks via emergent communication from referential games. Our key assumption is that grounding communication on images{---}as a crude approximation of real-world environments{---}inductively biases the model towards learning natural languages. On the one hand, we show that this substantially benefits machine translation in few-shot settings. On the other hand, this also provides an extrinsic evaluation protocol to probe the properties of emergent languages ex vitro. Intuitively, the closer they are to natural languages, the higher the gains from pretraining on them should be. For instance, in this work we measure the influence of communication success and maximum sequence length on downstream performances. Finally, we introduce a customised adapter layer and annealing strategies for the regulariser of maximum-a-posteriori inference during fine-tuning. These turn out to be crucial to facilitate knowledge transfer and prevent catastrophic forgetting. Compared to a recurrent baseline, our method yields gains of 59.0{\%} 147.6{\%} in BLEU score with only 500 NMT training instances and 65.1{\%} 196.7{\%} with 1,000 NMT training instances across four language pairs. These proof-of-concept results reveal the potential of emergent communication pretraining for both natural language processing tasks in resource-poor settings and extrinsic evaluation of artificial languages.","Barcelona, Spain (Online)","Li, Yaoyiran  and
Ponti, Edoardo Maria  and
Vuli{\'c}, Ivan  and
Korhonen, Anna",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.416,December,4716--4731,International Committee on Computational Linguistics,Emergent Communication Pretraining for Few-Shot Machine Translation,https://aclanthology.org/2020.coling-main.416,2020,,,,,
541,inproceedings,amidei-etal-2020-identifying,"A basic step in any annotation effort is the measurement of the Inter Annotator Agreement (IAA). An important factor that can affect the IAA is the presence of annotator bias. In this paper we introduce a new interpretation and application of the Item Response Theory (IRT) to detect annotators{'} bias. Our interpretation of IRT offers an original bias identification method that can be used to compare annotators{'} bias and characterise annotation disagreement. Our method can be used to spot outlier annotators, improve annotation guidelines and provide a better picture of the annotation reliability. Additionally, because scales for IAA interpretation are not generally agreed upon, our bias identification method is valuable as a complement to the IAA value which can help with understanding the annotation disagreement.","Barcelona, Spain (Online)","Amidei, Jacopo  and
Piwek, Paul  and
Willis, Alistair",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.421,December,4787--4797,International Committee on Computational Linguistics,Identifying Annotator Bias: A new {IRT}-based method for bias identification,https://aclanthology.org/2020.coling-main.421,2020,,,,,
542,inproceedings,guo-etal-2020-inflating,"We investigate the impact of political ideology biases in training data. Through a set of comparison studies, we examine the propagation of biases in several widely-used NLP models and its effect on the overall retrieval accuracy. Our work highlights the susceptibility of large, complex models to propagating the biases from human-selected input, which may lead to a deterioration of retrieval accuracy, and the importance of controlling for these biases. Finally, as a way to mitigate the bias, we propose to learn a text representation that is invariant to political ideology while still judging topic relevance.","Barcelona, Spain (Online)","Guo, Meiqi  and
Hwa, Rebecca  and
Lin, Yu-Ru  and
Chung, Wen-Ting",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.428,December,4873--4885,International Committee on Computational Linguistics,Inflating Topic Relevance with Ideology: A Case Study of Political Ideology Bias in Social Topic Detection Models,https://aclanthology.org/2020.coling-main.428,2020,,,,,
543,inproceedings,liang-etal-2020-monolingual,"Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people{'}s lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.","Barcelona, Spain (Online)","Liang, Sheng  and
Dufter, Philipp  and
Sch{\""u}tze, Hinrich",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.446,December,5082--5093,International Committee on Computational Linguistics,Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations,https://aclanthology.org/2020.coling-main.446,2020,,,,,
544,inproceedings,mitchell-bowers-2020-priorless,"Recently, domain-general recurrent neural networks, without explicit linguistic inductive biases, have been shown to successfully reproduce a range of human language behaviours, such as accurately predicting number agreement between nouns and verbs. We show that such networks will also learn number agreement within unnatural sentence structures, i.e. structures that are not found within any natural languages and which humans struggle to process. These results suggest that the models are learning from their input in a manner that is substantially different from human language acquisition, and we undertake an analysis of how the learned knowledge is stored in the weights of the network. We find that while the model has an effective understanding of singular versus plural for individual sentences, there is a lack of a unified concept of number agreement connecting these processes across the full range of inputs. Moreover, the weights handling natural and unnatural structures overlap substantially, in a way that underlines the non-human-like nature of the knowledge learned by the network.","Barcelona, Spain (Online)","Mitchell, Jeff  and
Bowers, Jeffrey",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.451,December,5147--5158,International Committee on Computational Linguistics,Priorless Recurrent Networks Learn Curiously,https://aclanthology.org/2020.coling-main.451,2020,,,,,
545,inproceedings,rabinovich-etal-2020-pick,"A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers{'} gender, and perform a first large-scale empirical study of distinctions in the usage of figurative language between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language, (2) men{'}s and women{'}s idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases.","Barcelona, Spain (Online)","Rabinovich, Ella  and
Gonen, Hila  and
Stevenson, Suzanne",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.454,December,5181--5192,International Committee on Computational Linguistics,Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage,https://aclanthology.org/2020.coling-main.454,2020,,,,,
546,inproceedings,xu-etal-2020-data,"Utterance classification is a key component in many conversational systems. However, classifying real-world user utterances is challenging, as people may express their ideas and thoughts in manifold ways, and the amount of training data for some categories may be fairly limited, resulting in imbalanced data distributions. To alleviate these issues, we conduct a comprehensive survey regarding data augmentation approaches for text classification, including simple random resampling, word-level transformations, and neural text generation to cope with imbalanced data. Our experiments focus on multi-class datasets with a large number of data samples, which has not been systematically studied in previous work. The results show that the effectiveness of different data augmentation schemes depends on the nature of the dataset under consideration.","Barcelona, Spain (Online)","Xu, Binxia  and
Qiu, Siyuan  and
Zhang, Jie  and
Wang, Yafang  and
Shen, Xiaoyu  and
de Melo, Gerard",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.479,December,5494--5506,International Committee on Computational Linguistics,Data Augmentation for Multiclass Utterance Classification {--} A Systematic Study,https://aclanthology.org/2020.coling-main.479,2020,,,,,
547,inproceedings,jiang-etal-2020-chinese,"Discourse structure tree construction is the fundamental task of discourse parsing and most previous work focused on English. Due to the cultural and linguistic differences, existing successful methods on English discourse parsing cannot be transformed into Chinese directly, especially in paragraph level suffering from longer discourse units and fewer explicit connectives. To alleviate the above issues, we propose two reading modes, i.e., the global backward reading and the local reverse reading, to construct Chinese paragraph level discourse trees. The former processes discourse units from the end to the beginning in a document to utilize the left-branching bias of discourse structure in Chinese, while the latter reverses the position of paragraphs in a discourse unit to enhance the differentiation of coherence between adjacent discourse units. The experimental results on Chinese MCDTB demonstrate that our model outperforms all strong baselines.","Barcelona, Spain (Online)","Jiang, Feng  and
Chu, Xiaomin  and
Li, Peifeng  and
Kong, Fang  and
Zhu, Qiaoming",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.506,December,5749--5759,International Committee on Computational Linguistics,{C}hinese Paragraph-level Discourse Parsing with Global Backward and Local Reverse Reading,https://aclanthology.org/2020.coling-main.506,2020,,,,,
548,inproceedings,aktas-stede-2020-variation,"In response to (i) inconclusive results in the literature as to the properties of coreference chains in written versus spoken language, and (ii) a general lack of work on automatic coreference resolution on both spoken language and social media, we undertake a corpus study involving the various genre sections of Ontonotes, the Switchboard corpus, and a corpus of Twitter conversations. Using a set of measures that previously have been applied individually to different data sets, we find fairly clear patterns of {``}behavior{''} for the different genres/media. Besides their role for psycholinguistic investigation (why do we employ different coreference strategies when we write or speak) and for the placement of Twitter in the spoken{--}written continuum, we see our results as a contribution to approaching genre-/media-specific coreference resolution.","Barcelona, Spain (Online)","Akta{\c{s}}, Berfin  and
Stede, Manfred",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.508,December,5774--5785,International Committee on Computational Linguistics,Variation in Coreference Strategies across Genres and Production Media,https://aclanthology.org/2020.coling-main.508,2020,,,,,
549,inproceedings,nouri-etal-2020-mining,"Crowdsourcing is used in academia and industry to solve tasks that are easy for humans but hard for computers, in natural language processing mostly to annotate data. The quality of annotations is affected by problems in the task design, task operation, and task evaluation that workers face with requesters in crowdsourcing processes. To learn about the major problems, we provide a short but comprehensive survey based on two complementary studies: (1) a literature review where we collect and organize problems known from interviews with workers, and (2) an empirical data analysis where we use topic modeling to mine workers{'} complaints from a new English corpus of workers{'} forum discussions. While literature covers all process phases, problems in the task evaluation are prevalent, including unfair rejections, late payments, and unjustified blockings of workers. According to the data, however, poor task design in terms of malfunctioning environments, bad workload estimation, and privacy violations seems to bother the workers most. Our findings form the basis for future research on how to improve crowdsourcing processes.","Barcelona, Spain (Online)","Nouri, Zahra  and
Wachsmuth, Henning  and
Engels, Gregor",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.551,December,6264--6276,International Committee on Computational Linguistics,Mining Crowdsourcing Problems from Discussion Forums of Workers,https://aclanthology.org/2020.coling-main.551,2020,,,,,
550,inproceedings,van-den-berg-markert-2020-context,"Informational bias is bias conveyed through sentences or clauses that provide tangential, speculative or background information that can sway readers{'} opinions towards entities. By nature, informational bias is context-dependent, but previous work on informational bias detection has not explored the role of context beyond the sentence. In this paper, we explore four kinds of context for informational bias in English news articles: neighboring sentences, the full article, articles on the same event from other news publishers, and articles from the same domain (but potentially different events). We find that integrating event context improves classification performance over a very strong baseline. In addition, we perform the first error analysis of models on this task. We find that the best-performing context-inclusive model outperforms the baseline on longer sentences, and sentences from politically centrist articles.","Barcelona, Spain (Online)","van den Berg, Esther  and
Markert, Katja",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.556,December,6315--6326,International Committee on Computational Linguistics,Context in Informational Bias Detection,https://aclanthology.org/2020.coling-main.556,2020,,,,,
551,inproceedings,jung-shim-2020-dual,"Relation extraction (RE) has been extensively studied due to its importance in real-world applications such as knowledge base construction and question answering. Most of the existing works train the models on either distantly supervised data or human-annotated data. To take advantage of the high accuracy of human annotation and the cheap cost of distant supervision, we propose the dual supervision framework which effectively utilizes both types of data. However, simply combining the two types of data to train a RE model may decrease the prediction accuracy since distant supervision has labeling bias. We employ two separate prediction networks HA-Net and DS-Net to predict the labels by human annotation and distant supervision, respectively, to prevent the degradation of accuracy by the incorrect labeling of distant supervision. Furthermore, we propose an additional loss term called disagreement penalty to enable HA-Net to learn from distantly supervised labels. In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information. Our performance study on sentence-level and document-level REs confirms the effectiveness of the dual supervision framework.","Barcelona, Spain (Online)","Jung, Woohwan  and
Shim, Kyuseok",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.564,December,6411--6423,International Committee on Computational Linguistics,Dual Supervision Framework for Relation Extraction with Distant Supervision and Human Annotation,https://aclanthology.org/2020.coling-main.564,2020,,,,,
552,inproceedings,wachsmuth-werner-2020-intrinsic,"Several quality dimensions of natural language arguments have been investigated. Some are likely to be reflected in linguistic features (e.g., an argument{'}s arrangement), whereas others depend on context (e.g., relevance) or topic knowledge (e.g., acceptability). In this paper, we study the intrinsic computational assessment of 15 dimensions, i.e., only learning from an argument{'}s text. In systematic experiments with eight feature types on an existing corpus, we observe moderate but significant learning success for most dimensions. Rhetorical quality seems hardest to assess, and subjectivity features turn out strong, although length bias in the corpus impedes full validity. We also find that human assessors differ more clearly to each other than to our approach.","Barcelona, Spain (Online)","Wachsmuth, Henning  and
Werner, Till",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.592,December,6739--6745,International Committee on Computational Linguistics,Intrinsic Quality Assessment of Arguments,https://aclanthology.org/2020.coling-main.592,2020,,,,,
553,inproceedings,xu-etal-2020-embedding,"Network embedding has recently emerged as a promising technique to embed nodes of a network into low-dimensional vectors. While fairly successful, most existing works focus on the embedding techniques for static networks. But in practice, there are many networks that are evolving over time and hence are dynamic, e.g., the social networks. To address this issue, a high-order spatio-temporal embedding model is developed to track the evolutions of dynamic networks. Specifically, an activeness-aware neighborhood embedding method is first proposed to extract the high-order neighborhood information at each given timestamp. Then, an embedding prediction framework is further developed to capture the temporal correlations, in which the attention mechanism is employed instead of recurrent neural networks (RNNs) for its efficiency in computing and flexibility in modeling. Extensive experiments are conducted on four real-world datasets from three different areas. It is shown that the proposed method outperforms all the baselines by a substantial margin for the tasks of dynamic link prediction and node classification, which demonstrates the effectiveness of the proposed methods on tracking the evolutions of dynamic networks.","Barcelona, Spain (Online)","Xu, Zenan  and
Ou, Zijing  and
Su, Qinliang  and
Yu, Jianxing  and
Quan, Xiaojun  and
Lin, ZhenKun",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.600,December,6809--6819,International Committee on Computational Linguistics,Embedding Dynamic Attributed Networks by Modeling the Evolution Processes,https://aclanthology.org/2020.coling-main.600,2020,,,,,
554,inproceedings,ramponi-plank-2020-neural,"Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.","Barcelona, Spain (Online)","Ramponi, Alan  and
Plank, Barbara",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.603,December,6838--6855,International Committee on Computational Linguistics,Neural Unsupervised Domain Adaptation in {NLP}{---}{A} Survey,https://aclanthology.org/2020.coling-main.603,2020,,,,,
555,inproceedings,shwartz-choi-2020-neural,"Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.","Barcelona, Spain (Online)","Shwartz, Vered  and
Choi, Yejin",Proceedings of the 28th International Conference on Computational Linguistics,10.18653/v1/2020.coling-main.605,December,6863--6870,International Committee on Computational Linguistics,Do Neural Language Models Overcome Reporting Bias?,https://aclanthology.org/2020.coling-main.605,2020,,,,,
556,inproceedings,chakravarti-etal-2020-towards,"Industry-scale NLP systems necessitate two features. 1. Robustness: {``}zero-shot transfer learning{''} (ZSTL) performance has to be commendable and 2. Efficiency: systems have to train efficiently and respond instantaneously. In this paper, we introduce the development of a production model called GAAMA (Go Ahead Ask Me Anything) which possess the above two characteristics. For robustness, it trains on the recently introduced Natural Questions (NQ) dataset. NQ poses additional challenges over older datasets like SQuAD: (a) QA systems need to read and comprehend an entire Wikipedia article rather than a small passage, and (b) NQ does not suffer from observation bias during construction, resulting in less lexical overlap between the question and the article. GAAMA consists of Attention-over-Attention, diversity among attention heads, hierarchical transfer learning, and synthetic data augmentation while being computationally inexpensive. Building on top of the powerful BERTQA model, GAAMA provides a âˆ¼2.0{\%} absolute boost in F1 over the industry-scale state-of-the-art (SOTA) system on NQ. Further, we show that GAAMA transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks: the BioASQ and the newly introduced CovidQA datasets.",Online,"Chakravarti, Rishav  and
Ferritto, Anthony  and
Iyer, Bhavani  and
Pan, Lin  and
Florian, Radu  and
Roukos, Salim  and
Sil, Avi",Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,10.18653/v1/2020.coling-industry.9,December,90--101,International Committee on Computational Linguistics,Towards building a Robust Industry-scale Question Answering System,https://aclanthology.org/2020.coling-industry.9,2020,,,,,
557,inproceedings,zhang-etal-2020-industry,"Embedding-based entity alignment has been widely investigated in recent years, but most proposed methods still rely on an ideal supervised learning setting with a large number of unbiased seed mappings for training and validation, which significantly limits their usage. In this study, we evaluate those state-of-the-art methods in an industrial context, where the impact of seed mappings with different sizes and different biases is explored. Besides the popular benchmarks from DBpedia and Wikidata, we contribute and evaluate a new industrial benchmark that is extracted from two heterogeneous knowledge graphs (KGs) under deployment for medical applications. The experimental results enable the analysis of the advantages and disadvantages of these alignment methods and the further discussion of suitable strategies for their industrial deployment.",Online,"Zhang, Ziheng  and
Liu, Hualuo  and
Chen, Jiaoyan  and
Chen, Xi  and
Liu, Bo  and
Xiang, YueJia  and
Zheng, Yefeng",Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,10.18653/v1/2020.coling-industry.17,December,179--189,International Committee on Computational Linguistics,An Industry Evaluation of Embedding-based Entity Alignment,https://aclanthology.org/2020.coling-industry.17,2020,,,,,
558,inproceedings,kalouli-etal-2020-xplainli,"Advances in Natural Language Inference (NLI) have helped us understand what state-of-the-art models really learn and what their generalization power is. Recent research has revealed some heuristics and biases of these models. However, to date, there is no systematic effort to capitalize on those insights through a system that uses these to explain the NLI decisions. To this end, we propose XplaiNLI, an eXplainable, interactive, visualization interface that computes NLI with different methods and provides explanations for the decisions made by the different approaches.","Barcelona, Spain (Online)","Kalouli, Aikaterini-Lida  and
Sevastjanova, Rita  and
de Paiva, Valeria  and
Crouch, Richard  and
El-Assady, Mennatallah",Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations,10.18653/v1/2020.coling-demos.9,December,48--52,International Committee on Computational Linguistics (ICCL),{X}plai{NLI}: Explainable Natural Language Inference through Visual Analytics,https://aclanthology.org/2020.coling-demos.9,2020,,,,,
559,inproceedings,chen-etal-2020-exploring,"Clinical machine learning is increasingly multimodal, collected in both structured tabular formats and unstructured forms such as free text. We propose a novel task of exploring \textit{fairness} on a multimodal clinical dataset, adopting \textit{equalized odds} for the downstream medical prediction tasks. To this end, we investigate a modality-agnostic fairness algorithm - equalized odds post processing - and compare it to a text-specific fairness algorithm: debiased clinical word embeddings. Despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups, we show that a text-specific approach to fairness may simultaneously achieve a good balance of performance classical notions of fairness. Our work opens the door for future work at the critical intersection of clinical NLP and fairness.",Online,"Chen, John  and
Berlot-Attwell, Ian  and
Wang, Xindi  and
Hossain, Safwan  and
Rudzicz, Frank",Proceedings of the 3rd Clinical Natural Language Processing Workshop,10.18653/v1/2020.clinicalnlp-1.33,November,301--312,Association for Computational Linguistics,Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical {NLP},https://aclanthology.org/2020.clinicalnlp-1.33,2020,,,,,
560,article,nissim-etal-2020-fair,"Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",,"Nissim, Malvina  and
van Noord, Rik  and
van der Goot, Rob",,10.1162/coli_a_00379,June,487--497,,Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor,https://aclanthology.org/2020.cl-2.7,2020,Computational Linguistics,46,2,,
561,inproceedings,celikkanat-etal-2020-controlling,"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets.",Online,"Celikkanat, Hande  and
Virpioja, Sami  and
Tiedemann, J{\""o}rg  and
Apidianaki, Marianna",Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/2020.blackboxnlp-1.13,November,136--148,Association for Computational Linguistics,Controlling the Imprint of Passivization and Negation in Contextualized Representations,https://aclanthology.org/2020.blackboxnlp-1.13,2020,,,,,
562,inproceedings,mccoy-etal-2020-berts,"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6{\%} and 84.8{\%}. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that {``}the doctor visited the lawyer{''} does not entail {``}the lawyer visited the doctor{''}), accuracy ranged from 0.0{\%} to 66.2{\%}. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.",Online,"McCoy, R. Thomas  and
Min, Junghyun  and
Linzen, Tal",Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/2020.blackboxnlp-1.21,November,217--227,Association for Computational Linguistics,{BERT}s of a feather do not generalize together: Large variability in generalization across models with similar test set performance,https://aclanthology.org/2020.blackboxnlp-1.21,2020,,,,,
563,inproceedings,thrush-etal-2020-investigating,"Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT{'}s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.",Online,"Thrush, Tristan  and
Wilcox, Ethan  and
Levy, Roger",Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/2020.blackboxnlp-1.25,November,265--275,Association for Computational Linguistics,Investigating Novel Verb Learning in {BERT}: Selectional Preference Classes and Alternation-Based Syntactic Generalization,https://aclanthology.org/2020.blackboxnlp-1.25,2020,,,,,
564,inproceedings,rios-etal-2020-quantifying,"Gender bias in biomedical research can have an adverse impact on the health of real people. For example, there is evidence that heart disease-related funded research generally focuses on men. Health disparities can form between men and at-risk groups of women (i.e., elderly and low-income) if there is not an equal number of heart disease-related studies for both genders. In this paper, we study temporal bias in biomedical research articles by measuring gender differences in word embeddings. Specifically, we address multiple questions, including, How has gender bias changed over time in biomedical research, and what health-related concepts are the most biased? Overall, we find that traditional gender stereotypes have reduced over time. However, we also find that the embeddings of many medical conditions are as biased today as they were 60 years ago (e.g., concepts related to drug addiction and body dysmorphia).",Online,"Rios, Anthony  and
Joshi, Reenam  and
Shin, Hejin",Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing,10.18653/v1/2020.bionlp-1.1,July,1--13,Association for Computational Linguistics,Quantifying 60 Years of Gender Bias in Biomedical Research with Word Embeddings,https://aclanthology.org/2020.bionlp-1.1,2020,,,,,
565,inproceedings,spliethover-wachsmuth-2020-argument,"Social bias in language - towards genders, ethnicities, ages, and other social groups - poses a problem with ethical impact for many NLP applications. Recent research has shown that machine learning models trained on respective data may not only adopt, but even amplify the bias. So far, however, little attention has been paid to bias in computational argumentation. In this paper, we study the existence of social biases in large English debate portals. In particular, we train word embedding models on portal-specific corpora and systematically evaluate their bias using WEAT, an existing metric to measure bias in word embeddings. In a word co-occurrence analysis, we then investigate causes of bias. The results suggest that all tested debate corpora contain unbalanced and biased data, mostly in favor of male people with European-American names. Our empirical insights contribute towards an understanding of bias in argumentative data sources.",Online,"Splieth{\""o}ver, Maximilian  and
Wachsmuth, Henning",Proceedings of the 7th Workshop on Argument Mining,,December,76--87,Association for Computational Linguistics,Argument from Old Man{'}s View: Assessing Social Bias in Argumentation,https://aclanthology.org/2020.argmining-1.9,2020,,,,,
566,inproceedings,razo-kubler-2020-investigating,"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al. (2019) to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.",Online,"Razo, Dante  and
K{\""u}bler, Sandra",Proceedings of the Fourth Workshop on Online Abuse and Harms,10.18653/v1/2020.alw-1.9,November,70--78,Association for Computational Linguistics,Investigating Sampling Bias in Abusive Language Detection,https://aclanthology.org/2020.alw-1.9,2020,,,,,
567,inproceedings,price-etal-2020-six,"We present a new dataset of approximately 44000 comments labeled by crowdworkers. Each comment is labelled as either {`}healthy{'} or {`}unhealthy{'}, in addition to binary labels for the presence of six potentially {`}unhealthy{'} sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score. We argue that there is a need for datasets which enable research based on a broad notion of {`}unhealthy online conversation{'}. We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation. For some of these attributes, this is the first publicly available dataset of this scale. We explore the quality of the dataset, present some summary statistics and initial models to illustrate the utility of this data, and highlight limitations and directions for further research.",Online,"Price, Ilan  and
Gifford-Moore, Jordan  and
Flemming, Jory  and
Musker, Saul  and
Roichman, Maayan  and
Sylvain, Guillaume  and
Thain, Nithum  and
Dixon, Lucas  and
Sorensen, Jeffrey",Proceedings of the Fourth Workshop on Online Abuse and Harms,10.18653/v1/2020.alw-1.15,November,114--124,Association for Computational Linguistics,Six Attributes of Unhealthy Conversations,https://aclanthology.org/2020.alw-1.15,2020,,,,,
568,inproceedings,nejadgholi-kiritchenko-2020-cross,"NLP research has attained high performances in abusive language detection as a supervised classification task. While in research settings, training and test datasets are usually obtained from similar data samples, in practice systems are often applied on data that are different from the training set in topic and class distributions. Also, the ambiguity in class definitions inherited in this task aggravates the discrepancies between source and target datasets. We explore the topic bias and the task formulation bias in cross-dataset generalization. We show that the benign examples in the Wikipedia Detox dataset are biased towards platform-specific topics. We identify these examples using unsupervised topic modeling and manual inspection of topics{'} keywords. Removing these topics increases cross-dataset generalization, without reducing in-domain classification performance. For a robust dataset design, we suggest applying inexpensive unsupervised methods to inspect the collected data and downsize the non-generalizable content before manually annotating for class labels.",Online,"Nejadgholi, Isar  and
Kiritchenko, Svetlana",Proceedings of the Fourth Workshop on Online Abuse and Harms,10.18653/v1/2020.alw-1.20,November,173--183,Association for Computational Linguistics,On Cross-Dataset Generalization in Automatic Detection of Online Abuse,https://aclanthology.org/2020.alw-1.20,2020,,,,,
569,inproceedings,choi-etal-2020-toward,"Scene graph is a graph representation that explicitly represents high-level semantic knowledge of an image such as objects, attributes of objects and relationships between objects. Various tasks have been proposed for the scene graph, but the problem is that they have a limited vocabulary and biased information due to their own hypothesis. Therefore, results of each task are not generalizable and difficult to be applied to other down-stream tasks. In this paper, we propose Entity Synset Alignment(ESA), which is a method to create a general scene graph by aligning various semantic knowledge efficiently to solve this bias problem. The ESA uses a large-scale lexical database, WordNet and Intersection of Union (IoU) to align the object labels in multiple scene graphs/semantic knowledge. In experiment, the integrated scene graph is applied to the image-caption retrieval task as a down-stream task. We confirm that integrating multiple scene graphs helps to get better representations of images.",Online,"Choi, Woo Suk  and
On, Kyoung-Woon  and
Heo, Yu-Jung  and
Zhang, Byoung-Tak",Proceedings of the First Workshop on Advances in Language and Vision Research,10.18653/v1/2020.alvr-1.2,July,7--11,Association for Computational Linguistics,Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment,https://aclanthology.org/2020.alvr-1.2,2020,,,,,
570,inproceedings,ishihara-2020-influence,"This study investigates the robustness and stability of a likelihood ratio{--}based (LR-based) forensic text comparison (FTC) system against the size of background population data. Focus is centred on a score-based approach for estimating authorship LRs. Each document is represented with a bag-of-words model, and the Cosine distance is used as the score-generating function. A set of population data that differed in the number of scores was synthesised 20 times using the Monte-Carol simulation technique. The FTC system{'}s performance with different population sizes was evaluated by a gradient metric of the log{--}LR cost (Cllr). The experimental results revealed two outcomes: 1) that the score-based approach is rather robust against a small population size{---}in that, with the scores obtained from the 40{\textasciitilde}60 authors in the database, the stability and the performance of the system become fairly comparable to the system with a maximum number of authors (720); and 2) that poor performance in terms of Cllr, which occurred because of limited background population data, is largely due to poor calibration. The results also indicated that the score-based approach is more robust against data scarcity than the feature-based approach; however, this finding obliges further study.",Virtual Workshop,"Ishihara, Shunichi",Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,,December,21--31,Australasian Language Technology Association,The Influence of Background Data Size on the Performance of a Score-based Likelihood Ratio System: A Case of Forensic Text Comparison,https://aclanthology.org/2020.alta-1.3,2020,,,,,
571,inproceedings,das-zhang-2020-absa,"Aspect-Based Sentiment Analysis (ABSA)has gained much attention in recent years. It is the task of identifying fine-grained opinionpolarity towards a specific aspect associated with a given target. However, there is a lack of benchmarking platform to provide a unified environment under consistent evaluation criteria for ABSA, resulting in the difficulties for fair comparisons. In this work, we address this issue and define a benchmark, ABSA-Bench, by unifying the evaluation protocols and the pre-processed publicly available datasets in a Web-based platform. ABSA-Bench provides two means of evaluations for participants to submit their predictions or models for online evaluation. Performances are ranked in the leader board and a discussion forum is supported to serve as a collaborative platform for academics and researchers to discuss queries.",Virtual Workshop,"Das, Abhishek  and
Zhang, Wei Emma",Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,,December,65--71,Australasian Language Technology Association,{ABSA}-Bench: Towards the Unified Evaluation of Aspect-based Sentiment Analysis Research,https://aclanthology.org/2020.alta-1.7,2020,,,,,
572,inproceedings,cohen-etal-2020-reviewing,"This tutorial will cover the theory and practice of reviewing research in natural language processing. Heavy reviewing burdens on natural language processing researchers have made it clear that our community needs to increase the size of our pool of potential reviewers. Simultaneously, notable {``}false negatives{''}---rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences{---}have raised awareness of the fact that our reviewing practices leave something to be desired. We do not often talk about {``}false positives{''} with respect to conference papers, but leaders in the field have noted that we seem to have a publication bias towards papers that report high performance, with perhaps not much else of interest in them. It need not be this way. Reviewing is a learnable skill, and you will learn it here via lectures and a considerable amount of hands-on practice.",Online,"Cohen, Kevin  and
Fort, Kar{\""e}n  and
Mieskes, Margot  and
N{\'e}v{\'e}ol, Aur{\'e}lie",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,10.18653/v1/2020.acl-tutorials.4,July,16--18,Association for Computational Linguistics,Reviewing Natural Language Processing Research,https://aclanthology.org/2020.acl-tutorials.4,2020,,,,,
573,inproceedings,sap-etal-2020-commonsense,"Commonsense knowledge, such as knowing that {``}bumping into people annoys them{''} or {``}rain makes the road slippery{''}, helps humans navigate everyday situations seamlessly. Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades. In recent years, commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community, yielding exploratory studies in automated commonsense understanding. We organize this tutorial to provide researchers with the critical foundations and recent advances in commonsense representation and reasoning, in the hopes of casting a brighter light on this promising area of future research. In our tutorial, we will (1) outline the various types of commonsense (e.g., physical, social), and (2) discuss techniques to gather and represent commonsense knowledge, while highlighting the challenges specific to this type of knowledge (e.g., reporting bias). We will then (3) discuss the types of commonsense knowledge captured by modern NLP systems (e.g., large pretrained language models), and (4) present ways to measure systems{'} commonsense reasoning abilities. We will finish with (5) a discussion of various ways in which commonsense reasoning can be used to improve performance on NLP tasks, exemplified by an (6) interactive session on integrating commonsense into a downstream task.",Online,"Sap, Maarten  and
Shwartz, Vered  and
Bosselut, Antoine  and
Choi, Yejin  and
Roth, Dan",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,10.18653/v1/2020.acl-tutorials.7,July,27--33,Association for Computational Linguistics,Commonsense Reasoning for Natural Language Processing,https://aclanthology.org/2020.acl-tutorials.7,2020,,,,,
574,inproceedings,hamborg-2020-media,"Media bias can strongly impact the public perception of topics reported in the news. A difficult to detect, yet powerful form of slanted news coverage is called bias by word choice and labeling (WCL). WCL bias can occur, for example, when journalists refer to the same semantic concept by using different terms that frame the concept differently and consequently may lead to different assessments by readers, such as the terms {``}freedom fighters{''} and {``}terrorists,{''} or {``}gun rights{''} and {``}gun control.{''} In this research project, I aim to devise methods that identify instances of WCL bias and estimate the frames they induce, e.g., not only is {``}terrorists{''} of negative polarity but also ascribes to aggression and fear. To achieve this, I plan to research methods using natural language processing and deep learning while employing models and using analysis concepts from the social sciences, where researchers have studied media bias for decades. The first results indicate the effectiveness of this interdisciplinary research approach. My vision is to devise a system that helps news readers to become aware of the differences in media coverage caused by bias.",Online,"Hamborg, Felix",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/2020.acl-srw.12,July,79--87,Association for Computational Linguistics,"Media Bias, the Social Sciences, and {NLP}: Automating Frame Analyses to Identify Bias by Word Choice and Labeling",https://aclanthology.org/2020.acl-srw.12,2020,,,,,
575,inproceedings,bhatt-etal-2020-much,"Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully.",Online,"Bhatt, Gantavya  and
Bansal, Hritik  and
Singh, Rishubh  and
Agarwal, Sumeet",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/2020.acl-srw.33,July,244--254,Association for Computational Linguistics,How much complexity does an {RNN} architecture need to learn syntax-sensitive dependencies?,https://aclanthology.org/2020.acl-srw.33,2020,,,,,
576,inproceedings,cao-etal-2020-expertise,"The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.",Online,"Cao, Yixin  and
Shui, Ruihao  and
Pan, Liangming  and
Kan, Min-Yen  and
Liu, Zhiyuan  and
Chua, Tat-Seng",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.100,July,1061--1071,Association for Computational Linguistics,Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen,https://aclanthology.org/2020.acl-main.100,2020,,,,,
577,inproceedings,rybak-etal-2020-klej,"In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based models.",Online,"Rybak, Piotr  and
Mroczkowski, Robert  and
Tracz, Janusz  and
Gawlik, Ireneusz",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.111,July,1191--1201,Association for Computational Linguistics,{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding,https://aclanthology.org/2020.acl-main.111,2020,,,,,
578,inproceedings,oprea-magdy-2020-isarcasm,"We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our dataset showed low performance compared to previously studied datasets, which indicates that these datasets might be biased or obvious and sarcasm could be a phenomenon under-studied computationally thus far. By providing the iSarcasm dataset, we aim to encourage future NLP research to develop methods for detecting sarcasm in text as intended by the authors of the text, not as labeled under assumptions that we demonstrate to be sub-optimal.",Online,"Oprea, Silviu  and
Magdy, Walid",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.118,July,1279--1289,Association for Computational Linguistics,i{S}arcasm: A Dataset of Intended Sarcasm,https://aclanthology.org/2020.acl-main.118,2020,,,,,
579,inproceedings,liang-etal-2020-beyond,"Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this problem, we formulate dialog evaluation as a comparison task. We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them. Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples. Our experiments show that CMADE achieves 89.2{\%} accuracy in the dialog comparison task.",Online,"Liang, Weixin  and
Zou, James  and
Yu, Zhou",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.126,July,1363--1374,Association for Computational Linguistics,Beyond User Self-Reported {L}ikert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation,https://aclanthology.org/2020.acl-main.126,2020,,,,,
580,inproceedings,alt-etal-2020-probing,"Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find that the bias induced by the architecture and the inclusion of linguistic features are clearly expressed in the probing task performance. For example, adding contextualized word representations greatly increases performance on probing tasks with a focus on named entity and part-of-speech information, and yields better results in RE. In contrast, entity masking improves RE, but considerably lowers performance on entity type related probing tasks.",Online,"Alt, Christoph  and
Gabryszak, Aleksandra  and
Hennig, Leonhard",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.140,July,1534--1545,Association for Computational Linguistics,Probing Linguistic Features of Sentence-Level Representations in Neural Relation Extraction,https://aclanthology.org/2020.acl-main.140,2020,,,,,
581,inproceedings,hovy-etal-2020-sound,"The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages {``}sound{''} older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic considerations into account.",Online,"Hovy, Dirk  and
Bianchi, Federico  and
Fornaciari, Tommaso",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.154,July,1686--1690,Association for Computational Linguistics,{``}You Sound Just Like Your Father{''} Commercial Machine Translation Systems Include Stylistic Biases,https://aclanthology.org/2020.acl-main.154,2020,,,,,
582,inproceedings,papalampidi-etal-2020-screenplay,"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models. We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes). Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",Online,"Papalampidi, Pinelopi  and
Keller, Frank  and
Frermann, Lea  and
Lapata, Mirella",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.174,July,1920--1933,Association for Computational Linguistics,Screenplay Summarization Using Latent Narrative Structure,https://aclanthology.org/2020.acl-main.174,2020,,,,,
583,inproceedings,davis-van-schijndel-2020-recurrent,"A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",Online,"Davis, Forrest  and
van Schijndel, Marten",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.179,July,1979--1990,Association for Computational Linguistics,Recurrent Neural Network Language Models Always Learn {E}nglish-Like Relative Clause Attachment,https://aclanthology.org/2020.acl-main.179,2020,,,,,
584,inproceedings,smith-etal-2020-put,"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages. We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.",Online,"Smith, Eric Michael  and
Williamson, Mary  and
Shuster, Kurt  and
Weston, Jason  and
Boureau, Y-Lan",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.183,July,2021--2030,Association for Computational Linguistics,Can You Put it All Together: Evaluating Conversational Agents{'} Ability to Blend Skills,https://aclanthology.org/2020.acl-main.183,2020,,,,,
585,inproceedings,murty-etal-2020-expbert,"Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to {``}interpret{''} these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3{--}20x less labeled data and improves on the baseline by 3{--}10 F1 points with the same amount of labeled data.",Online,"Murty, Shikhar  and
Koh, Pang Wei  and
Liang, Percy",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.190,July,2106--2113,Association for Computational Linguistics,{E}xp{BERT}: Representation Engineering with Natural Language Explanations,https://aclanthology.org/2020.acl-main.190,2020,,,,,
586,inproceedings,demeter-etal-2020-stolen,"Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot-product distance metric forms part of the inductive bias of NNLMs. Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability. We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.",Online,"Demeter, David  and
Kimmel, Gregory  and
Downey, Doug",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.198,July,2191--2197,Association for Computational Linguistics,Stolen Probability: A Structural Weakness of Neural Language Models,https://aclanthology.org/2020.acl-main.198,2020,,,,,
587,inproceedings,salazar-etal-2020-masked,"Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model{'}s WER by 30{\%} relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL{'}s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",Online,"Salazar, Julian  and
Liang, Davis  and
Nguyen, Toan Q.  and
Kirchhoff, Katrin",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.240,July,2699--2712,Association for Computational Linguistics,Masked Language Model Scoring,https://aclanthology.org/2020.acl-main.240,2020,,,,,
588,inproceedings,tang-etal-2020-showing,"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled {``}Show Your Work: Improved Reporting of Experimental Results{''} (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach. We analytically show that their estimator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/castorini/meanmax.",Online,"Tang, Raphael  and
Lee, Jaejun  and
Xin, Ji  and
Liu, Xinyu  and
Yu, Yaoliang  and
Lin, Jimmy",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.246,July,2766--2772,Association for Computational Linguistics,Showing Your Work Doesn{'}t Always Work,https://aclanthology.org/2020.acl-main.246,2020,,,,,
589,inproceedings,zhao-etal-2020-gender,"Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",Online,"Zhao, Jieyu  and
Mukherjee, Subhabrata  and
Hosseini, Saghar  and
Chang, Kai-Wei  and
Hassan Awadallah, Ahmed",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.260,July,2896--2907,Association for Computational Linguistics,Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer,https://aclanthology.org/2020.acl-main.260,2020,,,,,
590,inproceedings,ethayarajh-2020-classifier,"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias. In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. We provide empirical evidence that a 95{\%} confidence interval derived this way consistently bounds the true bias. In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. For example, consider a co-reference resolution system that is 5{\%} more accurate on gender-stereotypical sentences {--} to claim it is biased with 95{\%} confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",Online,"Ethayarajh, Kawin",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.262,July,2914--2919,Association for Computational Linguistics,Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds,https://aclanthology.org/2020.acl-main.262,2020,,,,,
591,inproceedings,tan-etal-2020-morphin,"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.",Online,"Tan, Samson  and
Joty, Shafiq  and
Kan, Min-Yen  and
Socher, Richard",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.263,July,2920--2935,Association for Computational Linguistics,It{'}s Morphin{'} Time! {C}ombating Linguistic Discrimination with Inflectional Perturbations,https://aclanthology.org/2020.acl-main.263,2020,,,,,
592,inproceedings,jia-etal-2020-mitigating,"Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models{'} top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels. We further propose a bias mitigation approach based on posterior regularization. With little performance loss, our method can almost remove the bias amplification in the distribution. Our study sheds the light on understanding the bias amplification.",Online,"Jia, Shengyu  and
Meng, Tao  and
Zhao, Jieyu  and
Chang, Kai-Wei",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.264,July,2936--2942,Association for Computational Linguistics,Mitigating Gender Bias Amplification in Distribution by Posterior Regularization,https://aclanthology.org/2020.acl-main.264,2020,,,,,
593,inproceedings,gaut-etal-2020-towards,"Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10{\%} human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different. However, such disparity does not appear when extracting relations such as birthDate or birthPlace. We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases. Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in NRE.",Online,"Gaut, Andrew  and
Sun, Tony  and
Tang, Shirlyn  and
Huang, Yuxin  and
Qian, Jing  and
ElSherief, Mai  and
Zhao, Jieyu  and
Mirza, Diba  and
Belding, Elizabeth  and
Chang, Kai-Wei  and
Wang, William Yang",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.265,July,2943--2953,Association for Computational Linguistics,Towards Understanding Gender Bias in Relation Extraction,https://aclanthology.org/2020.acl-main.265,2020,,,,,
594,inproceedings,lepori-etal-2020-representations,"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement. Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.",Online,"Lepori, Michael  and
Linzen, Tal  and
McCoy, R. Thomas",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.303,July,3306--3316,Association for Computational Linguistics,Representations of Syntax {[MASK]} Useful: {E}ffects of Constituency and Dependency Structure in Recursive {LSTM}s,https://aclanthology.org/2020.acl-main.303,2020,,,,,
595,inproceedings,yu-etal-2020-improving-multimodal,"In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions. Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.",Online,"Yu, Jianfei  and
Jiang, Jing  and
Yang, Li  and
Xia, Rui",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.306,July,3342--3352,Association for Computational Linguistics,Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer,https://aclanthology.org/2020.acl-main.306,2020,,,,,
596,inproceedings,baly-etal-2020-written,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This makes it possible to detect likely {``}fake news{''} the moment they are published, by simply checking the reliability of their source. From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context. Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium{'}s audience on social media). We further study (iii) what was written about the target medium (in Wikipedia). The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",Online,"Baly, Ramy  and
Karadzhov, Georgi  and
An, Jisun  and
Kwak, Haewoon  and
Dinkov, Yoan  and
Ali, Ahmed  and
Glass, James  and
Nakov, Preslav",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.308,July,3364--3374,Association for Computational Linguistics,What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context,https://aclanthology.org/2020.acl-main.308,2020,,,,,
597,inproceedings,wang-sennrich-2020-exposure,"The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.",Online,"Wang, Chaojun  and
Sennrich, Rico",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.326,July,3544--3552,Association for Computational Linguistics,"On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation",https://aclanthology.org/2020.acl-main.326,2020,,,,,
598,inproceedings,stanojevic-steedman-2020-max,"Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank. We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias. While known techniques for tackling these biases improve results, they still do not make the parser state of the art. Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation. The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",Online,"Stanojevi{\'c}, Milo{\v{s}}  and
Steedman, Mark",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.378,July,4111--4122,Association for Computational Linguistics,Max-Margin Incremental {CCG} Parsing,https://aclanthology.org/2020.acl-main.378,2020,,,,,
599,inproceedings,zhang-etal-2020-demographics,"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., {``}gay{''}, {``}black{''}) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like {``}She makes me happy to be gay{''} as abusive simply because of the word {``}gay.{''} In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models{'} generalization ability.",Online,"Zhang, Guanhua  and
Bai, Bing  and
Zhang, Junqi  and
Bai, Kun  and
Zhu, Conghui  and
Zhao, Tiejun",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.380,July,4134--4145,Association for Computational Linguistics,Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting,https://aclanthology.org/2020.acl-main.380,2020,,,,,
600,inproceedings,dayanik-pado-2020-masking,"A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias. We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.",Online,"Dayanik, Erenay  and
Pad{\'o}, Sebastian",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.404,July,4385--4391,Association for Computational Linguistics,Masking Actor Information Leads to Fairer Political Claims Detection,https://aclanthology.org/2020.acl-main.404,2020,,,,,
601,inproceedings,joseph-morgan-2020-word,"Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g. gender) than others (e.g. race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.",Online,"Joseph, Kenneth  and
Morgan, Jonathan",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.405,July,4392--4415,Association for Computational Linguistics,When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?,https://aclanthology.org/2020.acl-main.405,2020,,,,,
602,inproceedings,cao-daume-iii-2020-toward,"Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.",Online,"Cao, Yang Trista  and
Daum{\'e} III, Hal",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.418,July,4568--4595,Association for Computational Linguistics,Toward Gender-Inclusive Coreference Resolution,https://aclanthology.org/2020.acl-main.418,2020,,,,,
603,inproceedings,bommasani-etal-2020-interpreting,"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings {---} while more diverse and mature than those available for their dynamic counterparts {---} are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",Online,"Bommasani, Rishi  and
Davis, Kelly  and
Cardie, Claire",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.431,July,4758--4781,Association for Computational Linguistics,{I}nterpreting {P}retrained {C}ontextualized {R}epresentations via {R}eductions to {S}tatic {E}mbeddings,https://aclanthology.org/2020.acl-main.431,2020,,,,,
604,inproceedings,pruthi-etal-2020-learning,"Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention{'}s reliability as a tool for auditing algorithms in the context of fairness and accountability.",Online,"Pruthi, Danish  and
Gupta, Mansi  and
Dhingra, Bhuwan  and
Neubig, Graham  and
Lipton, Zachary C.",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.432,July,4782--4793,Association for Computational Linguistics,Learning to Deceive with Attention-Based Explanations,https://aclanthology.org/2020.acl-main.432,2020,,,,,
605,inproceedings,linzen-2020-accelerate,"This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.",Online,"Linzen, Tal",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.465,July,5210--5217,Association for Computational Linguistics,How Can We Accelerate Progress Towards Human-like Linguistic Generalization?,https://aclanthology.org/2020.acl-main.465,2020,,,,,
606,inproceedings,shah-etal-2020-predictive,"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",Online,"Shah, Deven Santosh  and
Schwartz, H. Andrew  and
Hovy, Dirk",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.468,July,5248--5264,Association for Computational Linguistics,Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview,https://aclanthology.org/2020.acl-main.468,2020,,,,,
607,inproceedings,keith-etal-2020-text,"Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual{'}s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions. Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.",Online,"Keith, Katherine  and
Jensen, David  and
O{'}Connor, Brendan",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.474,July,5332--5344,Association for Computational Linguistics,Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates,https://aclanthology.org/2020.acl-main.474,2020,,,,,
608,inproceedings,kim-etal-2020-implicit,"Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0. Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0. We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.",Online,"Kim, Najoung  and
Feng, Song  and
Gunasekara, Chulaka  and
Lastras, Luis",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.480,July,5404--5414,Association for Computational Linguistics,Implicit Discourse Relation Classification: We Need to Talk about Evaluation,https://aclanthology.org/2020.acl-main.480,2020,,,,,
609,inproceedings,wang-etal-2020-double,"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.",Online,"Wang, Tianlu  and
Lin, Xi Victoria  and
Rajani, Nazneen Fatema  and
McCann, Bryan  and
Ordonez, Vicente  and
Xiong, Caiming",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.484,July,5443--5453,Association for Computational Linguistics,Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation,https://aclanthology.org/2020.acl-main.484,2020,,,,,
610,inproceedings,blodgett-etal-2020-language,"We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",Online,"Blodgett, Su Lin  and
Barocas, Solon  and
Daum{\'e} III, Hal  and
Wallach, Hanna",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.485,July,5454--5476,Association for Computational Linguistics,Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP},https://aclanthology.org/2020.acl-main.485,2020,,,,,
611,inproceedings,sap-etal-2020-social,"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people{'}s judgments about others. For example, given a statement that {``}we shouldn{'}t lower our standards to hire more women,{''} most listeners will infer the implicature intended by the speaker - that {``}women (candidates) are less qualified.{''} Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80{\%} F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",Online,"Sap, Maarten  and
Gabriel, Saadia  and
Qin, Lianhui  and
Jurafsky, Dan  and
Smith, Noah A.  and
Choi, Yejin",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.486,July,5477--5490,Association for Computational Linguistics,Social Bias Frames: Reasoning about Social and Power Implications of Language,https://aclanthology.org/2020.acl-main.486,2020,,,,,
612,inproceedings,hutchinson-etal-2020-social,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",Online,"Hutchinson, Ben  and
Prabhakaran, Vinodkumar  and
Denton, Emily  and
Webster, Kellie  and
Zhong, Yu  and
Denuyl, Stephen",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.487,July,5491--5501,Association for Computational Linguistics,Social Biases in {NLP} Models as Barriers for Persons with Disabilities,https://aclanthology.org/2020.acl-main.487,2020,,,,,
613,inproceedings,liang-etal-2020-towards,"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",Online,"Liang, Paul Pu  and
Li, Irene Mengze  and
Zheng, Emily  and
Lim, Yao Chong  and
Salakhutdinov, Ruslan  and
Morency, Louis-Philippe",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.488,July,5502--5515,Association for Computational Linguistics,Towards Debiasing Sentence Representations,https://aclanthology.org/2020.acl-main.488,2020,,,,,
614,inproceedings,sun-etal-2020-evaluation,"Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.",Online,"Sun, Zhiqing  and
Vashishth, Shikhar  and
Sanyal, Soumya  and
Talukdar, Partha  and
Yang, Yiming",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.489,July,5516--5522,Association for Computational Linguistics,A Re-evaluation of Knowledge Graph Completion Methods,https://aclanthology.org/2020.acl-main.489,2020,,,,,
615,inproceedings,dua-etal-2020-benefits,"Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. We find that these intermediate annotations can provide two-fold benefits. First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref. Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.",Online,"Dua, Dheeru  and
Singh, Sameer  and
Gardner, Matt",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.497,July,5627--5634,Association for Computational Linguistics,Benefits of Intermediate Annotations in Reading Comprehension,https://aclanthology.org/2020.acl-main.497,2020,,,,,
616,inproceedings,zhang-etal-2020-winowhy,"In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC). For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning. Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions. Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy. Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution. WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.",Online,"Zhang, Hongming  and
Zhao, Xinran  and
Song, Yangqiu",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.508,July,5736--5745,Association for Computational Linguistics,{W}ino{W}hy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering {W}inograd Schema Challenge,https://aclanthology.org/2020.acl-main.508,2020,,,,,
617,inproceedings,tong-etal-2020-improving,"Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words. The source code is released on https://github.com/shuaiwa16/ekd.git.",Online,"Tong, Meihan  and
Xu, Bin  and
Wang, Shuai  and
Cao, Yixin  and
Hou, Lei  and
Li, Juanzi  and
Xie, Jun",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.522,July,5887--5897,Association for Computational Linguistics,Improving Event Detection via Open-domain Trigger Knowledge,https://aclanthology.org/2020.acl-main.522,2020,,,,,
618,inproceedings,inoue-etal-2020-r4c,"Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we introduce R4C, a new task for evaluating RC systems{'} internal reasoning. R4C requires giving not only answers but also derivations: explanations that justify predicted answers. We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e. 13.8k derivations). Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.",Online,"Inoue, Naoya  and
Stenetorp, Pontus  and
Inui, Kentaro",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.602,July,6740--6750,Association for Computational Linguistics,{R}4{C}: A Benchmark for Evaluating {RC} Systems to Get the Right Answer for the Right Reason,https://aclanthology.org/2020.acl-main.602,2020,,,,,
619,inproceedings,nguyen-etal-2020-learning,"Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples. This not only makes the classifier more robust but also boosts the overall ranking performance. Our model achieves a relative gain compared to baselines by over 26{\%} in F-score, and over 17{\%} in Area Under PR curve. On live search traffic, our model gains significant improvement in multiple countries.",Online,"Nguyen, Thanh  and
Rao, Nikhil  and
Subbian, Karthik",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.614,July,6861--6869,Association for Computational Linguistics,Learning Robust Models for e-Commerce Product Search,https://aclanthology.org/2020.acl-main.614,2020,,,,,
620,inproceedings,bentivogli-etal-2020-gender,"Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).",Online,"Bentivogli, Luisa  and
Savoldi, Beatrice  and
Negri, Matteo  and
Di Gangi, Mattia A.  and
Cattoni, Roldano  and
Turchi, Marco",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.619,July,6923--6933,Association for Computational Linguistics,Gender in Danger? Evaluating Speech Translation Technology on the {M}u{ST}-{SHE} Corpus,https://aclanthology.org/2020.acl-main.619,2020,,,,,
621,inproceedings,ravfogel-etal-2020-null,"The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",Online,"Ravfogel, Shauli  and
Elazar, Yanai  and
Gonen, Hila  and
Twiton, Michael  and
Goldberg, Yoav",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.647,July,7237--7256,Association for Computational Linguistics,Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection,https://aclanthology.org/2020.acl-main.647,2020,,,,,
622,inproceedings,boyd-graber-borschinger-2020-question,"In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems. We argue that creating a QA dataset{---}and the ubiquitous leaderboard that goes with it{---}closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner. However, the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant, fair, and effective question answering competitions. After detailing problems with existing QA datasets, we outline the key lessons{---}removing ambiguity, discriminating skill, and adjudicating disputes{---}that can transfer to QA research and how they might be implemented.",Online,"Boyd-Graber, Jordan  and
B{\""o}rschinger, Benjamin",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.662,July,7422--7435,Association for Computational Linguistics,What Question Answering can Learn from Trivia Nerds,https://aclanthology.org/2020.acl-main.662,2020,,,,,
623,inproceedings,shi-etal-2020-improving,"Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics. The code of our paper has been made publicly available.",Online,"Shi, Zhan  and
Zhou, Xu  and
Qiu, Xipeng  and
Zhu, Xiaodan",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.664,July,7454--7464,Association for Computational Linguistics,Improving Image Captioning with Better Use of Caption,https://aclanthology.org/2020.acl-main.664,2020,,,,,
624,inproceedings,tran-etal-2020-revisiting,"Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE.",Online,"Tran, Thy Thy  and
Le, Phong  and
Ananiadou, Sophia",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.669,July,7498--7505,Association for Computational Linguistics,Revisiting Unsupervised Relation Extraction,https://aclanthology.org/2020.acl-main.669,2020,,,,,
625,inproceedings,andreas-2020-good,"We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87{\%} on diagnostic tasks from the SCAN dataset and 16{\%} on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1{\%} on small corpora in several languages.",Online,"Andreas, Jacob",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.676,July,7556--7566,Association for Computational Linguistics,Good-Enough Compositional Data Augmentation,https://aclanthology.org/2020.acl-main.676,2020,,,,,
626,inproceedings,saunders-byrne-2020-reducing,"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a {`}balanced{'} dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is {`}catastrophic forgetting{'}, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",Online,"Saunders, Danielle  and
Byrne, Bill",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.690,July,7724--7736,Association for Computational Linguistics,Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem,https://aclanthology.org/2020.acl-main.690,2020,,,,,
627,inproceedings,riley-etal-2020-translationese,"Machine translation has an undesirable propensity to produce {``}translationese{''} artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model. Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency. Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality. We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.",Online,"Riley, Parker  and
Caswell, Isaac  and
Freitag, Markus  and
Grangier, David",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.691,July,7737--7746,Association for Computational Linguistics,Translationese as a Language in {``}Multilingual{''} {NMT},https://aclanthology.org/2020.acl-main.691,2020,,,,,
628,inproceedings,sellam-etal-2020-bleurt,"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.",Online,"Sellam, Thibault  and
Das, Dipanjan  and
Parikh, Ankur",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.704,July,7881--7892,Association for Computational Linguistics,{BLEURT}: Learning Robust Metrics for Text Generation,https://aclanthology.org/2020.acl-main.704,2020,,,,,
629,inproceedings,shrestha-etal-2020-negative,"Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors. For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements. Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.",Online,"Shrestha, Robik  and
Kafle, Kushal  and
Kanan, Christopher",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.727,July,8172--8181,Association for Computational Linguistics,A negative case analysis of visual grounding methods for {VQA},https://aclanthology.org/2020.acl-main.727,2020,,,,,
630,inproceedings,ryskina-etal-2020-phonetic,"Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages{---}namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion. We train our model directly on romanized data from two languages: Egyptian Arabic and Russian. We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model{'}s performance on both languages, yielding results much closer to the supervised skyline. Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.",Online,"Ryskina, Maria  and
Gormley, Matthew R.  and
Berg-Kirkpatrick, Taylor",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.737,July,8308--8319,Association for Computational Linguistics,Phonetic and Visual Priors for Decipherment of Informal {R}omanization,https://aclanthology.org/2020.acl-main.737,2020,,,,,
631,inproceedings,li-etal-2020-regularized,"Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.",Online,"Li, Xintong  and
Liu, Lemao  and
Wang, Rui  and
Huang, Guoping  and
Meng, Max",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.757,July,8555--8562,Association for Computational Linguistics,Regularized Context Gates on Transformer for Machine Translation,https://aclanthology.org/2020.acl-main.757,2020,,,,,
632,inproceedings,karimi-mahabadi-etal-2020-end,"Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models{'} predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in \url{https://github.com/rabeehk/robust-nli}.",Online,"Karimi Mahabadi, Rabeeh  and
Belinkov, Yonatan  and
Henderson, James",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.769,July,8706--8716,Association for Computational Linguistics,End-to-End Bias Mitigation by Modelling Biases in Corpora,https://aclanthology.org/2020.acl-main.769,2020,,,,,
633,inproceedings,utama-etal-2020-mind,"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.",Online,"Utama, Prasetya Ajie  and
Moosavi, Nafise Sadat  and
Gurevych, Iryna",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.770,July,8717--8729,Association for Computational Linguistics,Mind the Trade-off: Debiasing {NLU} Models without Degrading the In-distribution Performance,https://aclanthology.org/2020.acl-main.770,2020,,,,,
634,inproceedings,zhou-bansal-2020-towards,"While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method. Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance. The first approach aims to remove the label bias at the embedding level. The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models. We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.",Online,"Zhou, Xiang  and
Bansal, Mohit",Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/2020.acl-main.773,July,8759--8771,Association for Computational Linguistics,Towards Robustifying {NLI} Models Against Lexical Dataset Biases,https://aclanthology.org/2020.acl-main.773,2020,,,,,
635,inproceedings,yadav-etal-2020-unbiasing,User-generated contents{'} score-based prediction and item recommendation has become an inseparable part of the online recommendation systems. The ratings allow people to express their opinions and may affect the market value of items and consumer confidence in e-commerce decisions. A major problem with the models designed for user review prediction is that they unknowingly neglect the rating bias occurring due to personal user bias preferences. We propose a tendency-based approach that models the user and item tendency for score prediction along with text review analysis with respect to ratings.,"Suzhou, China","Yadav, Pranshi  and
Yadav, Priya  and
Nokhiz, Pegah  and
Gupta, Vivek",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,,December,50--56,Association for Computational Linguistics,Unbiasing Review Ratings with Tendency Based Collaborative Filtering,https://aclanthology.org/2020.aacl-srw.8,2020,,,,,
636,inproceedings,nakamachi-etal-2020-text,"We optimize rewards of reinforcement learning in text simplification using metrics that are highly correlated with human-perspectives. To address problems of exposure bias and loss-evaluation mismatch, text-to-text generation tasks employ reinforcement learning that rewards task-specific metrics. Previous studies in text simplification employ the weighted sum of sub-rewards from three perspectives: grammaticality, meaning preservation, and simplicity. However, the previous rewards do not align with human-perspectives for these perspectives. In this study, we propose to use BERT regressors fine-tuned for grammaticality, meaning preservation, and simplicity as reward estimators to achieve text simplification conforming to human-perspectives. Experimental results show that reinforcement learning with our rewards balances meaning preservation and simplicity. Additionally, human evaluation confirmed that simplified texts by our method are preferred by humans compared to previous studies.","Suzhou, China","Nakamachi, Akifumi  and
Kajiwara, Tomoyuki  and
Arase, Yuki",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,,December,153--159,Association for Computational Linguistics,"Text Simplification with Reinforcement Learning Using Supervised Rewards on Grammaticality, Meaning Preservation, and Simplicity",https://aclanthology.org/2020.aacl-srw.22,2020,,,,,
637,inproceedings,jin-schuler-2020-grounded,"Recent work in unsupervised parsing has tried to incorporate visual information into learning, but results suggest that these models need linguistic bias to compete against models that only rely on text. This work proposes grammar induction models which use visual information from images for labeled parsing, and achieve state-of-the-art results on grounded grammar induction on several languages. Results indicate that visual information is especially helpful in languages where high frequency words are more broadly distributed. Comparison between models with and without visual information shows that the grounded models are able to use visual information for proposing noun phrases, gathering useful information from images for unknown words, and achieving better performance at prepositional phrase attachment prediction.","Suzhou, China","Jin, Lifeng  and
Schuler, William",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,,December,396--408,Association for Computational Linguistics,Grounded {PCFG} Induction with Images,https://aclanthology.org/2020.aacl-main.42,2020,,,,,
638,inproceedings,zhang-etal-2020-robustness,"It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy {``}man is to computer-programmer as woman is to homemaker{''} is due to word similarity rather than bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings.","Suzhou, China","Zhang, Haiyang  and
Sneyd, Alison  and
Stevenson, Mark",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,,December,759--769,Association for Computational Linguistics,Robustness and Reliability of Gender Bias Assessment in Word Embeddings: The Role of Base Pairs,https://aclanthology.org/2020.aacl-main.76,2020,,,,,
639,inproceedings,jiang-etal-2020-recipe,"We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize (e.g., the primitive instructionsin MILK) and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation (e.g., ProPara). We thus frame the semantic comprehension of procedural text such as recipes, as fairly generic NLP subtasks, covering (i) entity recognition (ingredients, tools and actions), (ii) relation extraction (what ingredients and tools are involved in the actions), and (iii) zero anaphora resolution (link actions to implicit arguments, e.g., results from previous recipe steps). Further, our Recipe Instruction Semantic Corpus (RISeC) dataset includes textual descriptions for the zero anaphora, to facilitate language generation thereof. Besides the dataset itself, we contribute a pipeline neural architecture that addresses entity and relation extractionas well an identification of zero anaphora. These basic building blocks can facilitate more advanced downstream applications (e.g., question answering, conversational agents).","Suzhou, China","Jiang, Yiwei  and
Zaporojets, Klim  and
Deleu, Johannes  and
Demeester, Thomas  and
Develder, Chris",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,,December,821--826,Association for Computational Linguistics,Recipe Instruction Semantics Corpus ({RIS}e{C}): {R}esolving Semantic Structure and Zero Anaphora in Recipes,https://aclanthology.org/2020.aacl-main.82,2020,,,,,
640,inproceedings,wang-etal-2020-fairseq,"We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T) modeling tasks such as end-to-end speech recognition and speech-to-text translation. It follows fairseq{'}s careful design for scalability and extensibility. We provide end-to-end workflows from data pre-processing, model training to offline (online) inference. We implement state-of-the-art RNN-based as well as Transformer-based models and open-source detailed training recipes. Fairseq{'}s machine translation models and language models can be seamlessly integrated into S2T workflows for multi-task learning or transfer learning. Fairseq S2T is available at https://github.com/pytorch/fairseq/tree/master/examples/speech{\_}to{\_}text.","Suzhou, China","Wang, Changhan  and
Tang, Yun  and
Ma, Xutai  and
Wu, Anne  and
Okhonko, Dmytro  and
Pino, Juan",Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations,,December,33--39,Association for Computational Linguistics,Fairseq {S}2{T}: Fast Speech-to-Text Modeling with Fairseq,https://aclanthology.org/2020.aacl-demo.6,2020,,,,,
641,inproceedings,kulikov-etal-2019-importance,"We investigate the impact of search strategies in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a model-based Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics: log-probabilities assigned by the model and utterance diversity. Our experiments reveal that better search algorithms lead to higher rated conversations. However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.","Tokyo, Japan","Kulikov, Ilia  and
Miller, Alexander  and
Cho, Kyunghyun  and
Weston, Jason",Proceedings of the 12th International Conference on Natural Language Generation,10.18653/v1/W19-8609,October{--}November,76--87,Association for Computational Linguistics,Importance of Search and Evaluation Strategies in Neural Dialogue Modeling,https://aclanthology.org/W19-8609,2019,,,,,
642,inproceedings,xu-etal-2019-privacy,"Biased decisions made by automatic systems have led to growing concerns in research communities. Recent work from the NLP community focuses on building systems that make fair decisions based on text. Instead of relying on unknown decision systems or human decision-makers, we argue that a better way to protect data providers is to remove the trails of sensitive information before publishing the data. In light of this, we propose a new privacy-aware text rewriting task and explore two privacy-aware back-translation methods for the task, based on adversarial training and approximate fairness risk. Our extensive experiments on three real-world datasets with varying demographical attributes show that our methods are effective in obfuscating sensitive attributes. We have also observed that the fairness risk method retains better semantics and fluency, while the adversarial training method tends to leak less sensitive information.","Tokyo, Japan","Xu, Qiongkai  and
Qu, Lizhen  and
Xu, Chenchen  and
Cui, Ran",Proceedings of the 12th International Conference on Natural Language Generation,10.18653/v1/W19-8633,October{--}November,247--257,Association for Computational Linguistics,Privacy-Aware Text Rewriting,https://aclanthology.org/W19-8633,2019,,,,,
643,inproceedings,gehrmann-etal-2019-generating,"Neural abstractive document summarization is commonly approached by models that exhibit a mostly extractive behavior. This behavior is facilitated by a copy-attention which allows models to copy words from a source document. While models in the mostly extractive news summarization domain benefit from this inductive bias, they commonly fail to paraphrase or compress information from the source document. Recent advances in transfer-learning from large pretrained language models give rise to alternative approaches that do not rely on copy-attention and instead learn to generate concise and abstractive summaries. In this paper, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that transfer-learning can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of abstraction for a similar performance on the TL;DR challenge tasks, enabling true natural language compression.","Tokyo, Japan","Gehrmann, Sebastian  and
Ziegler, Zachary  and
Rush, Alexander",Proceedings of the 12th International Conference on Natural Language Generation,10.18653/v1/W19-8665,October{--}November,516--522,Association for Computational Linguistics,Generating Abstractive Summaries with Finetuned Language Models,https://aclanthology.org/W19-8665,2019,,,,,
644,inproceedings,damaschk-etal-2019-multiclass,"This paper discusses methods to improve the performance of text classification on data that is difficult to classify due to a large number of unbalanced classes with noisy examples. A variety of features are tested, in combination with three different neural-network-based methods with increasing complexity. The classifiers are applied to a songtext{--}artist dataset which is large, unbalanced and noisy. We come to the conclusion that substantial improvement can be obtained by removing unbalancedness and sparsity from the data. This fulfils a classification task unsatisfactorily{---}however, with contemporary methods, it is a practical step towards fairly satisfactory results.","Turku, Finland","Damaschk, Matthias  and
D{\""o}nicke, Tillmann  and
Lux, Florian",Proceedings of the First NLPL Workshop on Deep Learning for Natural Language Processing,,September,58--65,"Link{\""o}ping University Electronic Press","Multiclass Text Classification on Unbalanced, Sparse and Noisy Data",https://aclanthology.org/W19-6207,2019,,,,,
645,inproceedings,sahlgren-olsson-2019-gender,"This paper investigates the presence of gender bias in pretrained Swedish embeddings. We focus on a scenario where names are matched with occupations, and we demonstrate how a number of standard pretrained embeddings handle this task. Our experiments show some significant differences between the pretrained embeddings, with word-based methods showing the most bias and contextualized language models showing the least. We also demonstrate that the previously proposed debiasing method does not affect the performance of the various embeddings in this scenario.","Turku, Finland","Sahlgren, Magnus  and
Olsson, Fredrik",Proceedings of the 22nd Nordic Conference on Computational Linguistics,,September{--}October,35--43,"Link{\""o}ping University Electronic Press",Gender Bias in Pretrained {S}wedish Embeddings,https://aclanthology.org/W19-6104,2019,,,,,
646,inproceedings,lee-etal-2019-transformer,"This paper describes POSTECH{'}s submission to the WMT 2019 shared task on Automatic Post-Editing (APE). In this paper, we propose a new multi-source APE model by extending Transformer. The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the encoder and the decoder state in the decoder. Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias. Finally, we adopt the ensemble technique across variations of our model. Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline. Our primary submission achieves -0.73 in TER and +1.49 in BLEU compare to the baseline.","Florence, Italy","Lee, WonKee  and
Shin, Jaehun  and
Lee, Jong-Hyeok","Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",10.18653/v1/W19-5412,August,112--117,Association for Computational Linguistics,Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder,https://aclanthology.org/W19-5412,2019,,,,,
647,inproceedings,dabre-sumita-2019-nicts-supervised,"In this paper we describe our neural machine translation (NMT) systems for Japaneseâ†”English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japaneseâ†”English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.","Florence, Italy","Dabre, Raj  and
Sumita, Eiichiro","Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",10.18653/v1/W19-5362,August,533--536,Association for Computational Linguistics,{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task,https://aclanthology.org/W19-5362,2019,,,,,
648,inproceedings,freitag-etal-2019-ape,"In this work, we train an Automatic Post-Editing (APE) model and use it to reveal biases in standard MT evaluation procedures. The goal of our APE model is to correct typical errors introduced by the translation process, and convert the {``}translationese{''} output into natural text. Our APE model is trained entirely on monolingual data that has been round-trip translated through English, to mimic errors that are similar to the ones introduced by NMT. We apply our model to the output of existing NMT systems, and demonstrate that, while the human-judged quality improves in all cases, BLEU scores drop with forward-translated test sets. We verify these results for the WMT18 English to German, WMT15 English to French, and WMT16 English to Romanian tasks. Furthermore, we selectively apply our APE model on the output of the top submissions of the most recent WMT evaluation campaigns. We see quality improvements on all tasks of up to 2.5 BLEU points.","Florence, Italy","Freitag, Markus  and
Caswell, Isaac  and
Roy, Scott",Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers),10.18653/v1/W19-5204,August,34--44,Association for Computational Linguistics,{APE} at Scale and Its Implications on {MT} Evaluation Biases,https://aclanthology.org/W19-5204,2019,,,,,
649,inproceedings,savary-etal-2019-without,"Because most multiword expressions (MWEs), especially verbal ones, are semantically non-compositional, their automatic identification in running text is a prerequisite for semantically-oriented downstream applications. However, recent developments, driven notably by the PARSEME shared task on automatic identification of verbal MWEs, show that this task is harder than related tasks, despite recent contributions both in multilingual corpus annotation and in computational models. In this paper, we analyse possible reasons for this state of affairs. They lie in the nature of the MWE phenomenon, as well as in its distributional properties. We also offer a comparative analysis of the state-of-the-art systems, which exhibit particularly strong sensitivity to unseen data. On this basis, we claim that, in order to make strong headway in MWE identification, the community should bend its mind into coupling identification of MWEs with their discovery, via syntactic MWE lexicons. Such lexicons need not necessarily achieve a linguistically complete modelling of MWEs{'} behavior, but they should provide minimal morphosyntactic information to cover some potential uses, so as to complement existing MWE-annotated corpora. We define requirements for such minimal NLP-oriented lexicon, and we propose a roadmap for the MWE community driven by these requirements.","Florence, Italy","Savary, Agata  and
Cordeiro, Silvio  and
Ramisch, Carlos",Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019),10.18653/v1/W19-5110,August,79--91,Association for Computational Linguistics,"Without lexicons, multiword expression identification will never fly: A position statement",https://aclanthology.org/W19-5110,2019,,,,,
650,inproceedings,chauhan-etal-2019-reflex,"Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of preprocessing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area.","Florence, Italy","Chauhan, Geeticka  and
McDermott, Matthew B.A.  and
Szolovits, Peter",Proceedings of the 18th BioNLP Workshop and Shared Task,10.18653/v1/W19-5004,August,30--47,Association for Computational Linguistics,{RE}flex: Flexible Framework for Relation Extraction in Multiple Domains,https://aclanthology.org/W19-5004,2019,,,,,
651,inproceedings,alhuzali-ananiadou-2019-improving,"The availability of large-scale and real-time data on social media has motivated research into adverse drug reactions (ADRs). ADR classification helps to identify negative effects of drugs, which can guide health professionals and pharmaceutical companies in making medications safer and advocating patients{'} safety. Based on the observation that in social media, negative sentiment is frequently expressed towards ADRs, this study presents a neural model that combines sentiment analysis with transfer learning techniques to improve ADR detection in social media postings. Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus. We then apply transfer learning to adapt the model to the task of detecting ADRs in social media postings. We show that, in combination with rich representations of words and their contexts, transfer learning is beneficial, especially given the large degree of vocabulary overlap between the current affairs posts in the SemEval17-task4A corpus and posts about ADRs. We compare our results with previous approaches, and show that our model can outperform them by up to 3{\%} F-score.","Florence, Italy","Alhuzali, Hassan  and
Ananiadou, Sophia",Proceedings of the 18th BioNLP Workshop and Shared Task,10.18653/v1/W19-5036,August,339--347,Association for Computational Linguistics,Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning,https://aclanthology.org/W19-5036,2019,,,,,
652,inproceedings,gangula-etal-2019-detecting,"Language is a powerful tool which can be used to state the facts as well as express our views and perceptions. Most of the times, we find a subtle bias towards or against someone or something. When it comes to politics, media houses and journalists are known to create bias by shrewd means such as misinterpreting reality and distorting viewpoints towards some parties. This misinterpretation on a large scale can lead to the production of biased news and conspiracy theories. Automating bias detection in newspaper articles could be a good challenge for research in NLP. We proposed a headline attention network for this bias detection. Our model has two distinctive characteristics: (i) it has a structure that mirrors a person{'}s way of reading a news article (ii) it has attention mechanism applied on the article based on its headline, enabling it to attend to more critical content to predict bias. As the required datasets were not available, we created a dataset comprising of 1329 news articles collected from various Telugu newspapers and marked them for bias towards a particular political party. The experiments conducted on it demonstrated that our model outperforms various baseline methods by a substantial margin.","Florence, Italy","Gangula, Rama Rohit Reddy  and
Duggenpudi, Suma Reddy  and
Mamidi, Radhika",Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/W19-4809,August,77--84,Association for Computational Linguistics,Detecting Political Bias in News Articles Using Headline Attention,https://aclanthology.org/W19-4809,2019,,,,,
653,inproceedings,merrill-etal-2019-finding,"Neural network architectures have been augmented with differentiable stacks in order to introduce a bias toward learning hierarchy-sensitive regularities. It has, however, proven difficult to assess the degree to which such a bias is effective, as the operation of the differentiable stack is not always interpretable. In this paper, we attempt to detect the presence of latent representations of hierarchical structure through an exploration of the unsupervised learning of constituency structure. Using a technique due to Shen et al. (2018a,b), we extract syntactic trees from the pushing behavior of stack RNNs trained on language modeling and classification objectives. We find that our models produce parses that reflect natural language syntactic constituencies, demonstrating that stack RNNs do indeed infer linguistically relevant hierarchical structure.","Florence, Italy","Merrill, William  and
Khazan, Lenny  and
Amsel, Noah  and
Hao, Yiding  and
Mendelsohn, Simon  and
Frank, Robert",Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,10.18653/v1/W19-4823,August,224--232,Association for Computational Linguistics,Finding Hierarchical Structure in Neural Stacks Using Unsupervised Parsing,https://aclanthology.org/W19-4823,2019,,,,,
654,inproceedings,jawahar-seddah-2019-contextualized,"Diachronic word embeddings play a key role in capturing interesting patterns about how language evolves over time. Most of the existing work focuses on studying corpora spanning across several decades, which is understandably still not a possibility when working on social media-based user-generated content. In this work, we address the problem of studying semantic changes in a large Twitter corpus collected over five years, a much shorter period than what is usually the norm in diachronic studies. We devise a novel attentional model, based on Bernoulli word embeddings, that are conditioned on contextual extra-linguistic (social) features such as network, spatial and socio-economic variables, which are associated with Twitter users, as well as topic-based features. We posit that these social features provide an inductive bias that helps our model to overcome the narrow time-span regime problem. Our extensive experiments reveal that our proposed model is able to capture subtle semantic shifts without being biased towards frequency cues and also works well when certain contextual features are absent. Our model fits the data better than current state-of-the-art dynamic word embedding models and therefore is a promising tool to study diachronic semantic changes over small time periods.","Florence, Italy","Jawahar, Ganesh  and
Seddah, Djam{\'e}",Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,10.18653/v1/W19-4705,August,35--47,Association for Computational Linguistics,Contextualized Diachronic Word Representations,https://aclanthology.org/W19-4705,2019,,,,,
655,inproceedings,wevers-2019-using,"Contemporary debates on filter bubbles and polarization in public and social media raise the question to what extent news media of the past exhibited biases. This paper specifically examines bias related to gender in six Dutch national newspapers between 1950 and 1990. We measure bias related to gender by comparing local changes in word embedding models trained on newspapers with divergent ideological backgrounds. We demonstrate clear differences in gender bias and changes within and between newspapers over time. In relation to themes such as sexuality and leisure, we see the bias moving toward women, whereas, generally, the bias shifts in the direction of men, despite growing female employment number and feminist movements. Even though Dutch society became less stratified ideologically (depillarization), we found an increasing divergence in gender bias between religious and social-democratic on the one hand and liberal newspapers on the other. Methodologically, this paper illustrates how word embeddings can be used to examine historical language change. Future work will investigate how fine-tuning deep contextualized embedding models, such as ELMO, might be used for similar tasks with greater contextual information.","Florence, Italy","Wevers, Melvin",Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,10.18653/v1/W19-4712,August,92--97,Association for Computational Linguistics,"Using Word Embeddings to Examine Gender Bias in {D}utch Newspapers, 1950-1990",https://aclanthology.org/W19-4712,2019,,,,,
656,inproceedings,tripodi-etal-2019-tracing,"We investigate some aspects of the history of antisemitism in France, one of the cradles of modern antisemitism, using diachronic word embeddings. We constructed a large corpus of French books and periodicals issues that contain a keyword related to Jews and performed a diachronic word embedding over the 1789-1914 period. We studied the changes over time in the semantic spaces of 4 target words and performed embedding projections over 6 streams of antisemitic discourse. This allowed us to track the evolution of antisemitic bias in the religious, economic, socio-politic, racial, ethic and conspiratorial domains. Projections show a trend of growing antisemitism, especially in the years starting in the mid-80s and culminating in the Dreyfus affair. Our analysis also allows us to highlight the peculiar adverse bias towards Judaism in the broader context of other religions.","Florence, Italy","Tripodi, Rocco  and
Warglien, Massimo  and
Levis Sullam, Simon  and
Paci, Deborah",Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,10.18653/v1/W19-4715,August,115--125,Association for Computational Linguistics,Tracing Antisemitic Language Through Diachronic Embedding Projections: {F}rance 1789-1914,https://aclanthology.org/W19-4715,2019,,,,,
657,inproceedings,loukina-etal-2019-many,The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people{'}s lives especially when deployed as part of high-stakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers{'} native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.,"Florence, Italy","Loukina, Anastassia  and
Madnani, Nitin  and
Zechner, Klaus",Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,10.18653/v1/W19-4401,August,1--10,Association for Computational Linguistics,The many dimensions of algorithmic fairness in educational applications,https://aclanthology.org/W19-4401,2019,,,,,
658,inproceedings,mayfield-etal-2019-equity,"There is a long record of research on equity in schools. As machine learning researchers begin to study fairness and bias in earnest, language technologies in education have an unusually strong theoretical and applied foundation to build on. Here, we introduce concepts from culturally relevant pedagogy and other frameworks for teaching and learning, identifying future work on equity in NLP. We present case studies in a range of topics like intelligent tutoring systems, computer-assisted language learning, automated essay scoring, and sentiment analysis in classrooms, and provide an actionable agenda for research.","Florence, Italy","Mayfield, Elijah  and
Madaio, Michael  and
Prabhumoye, Shrimai  and
Gerritsen, David  and
McLaughlin, Brittany  and
Dixon-Rom{\'a}n, Ezekiel  and
Black, Alan W",Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,10.18653/v1/W19-4446,August,444--460,Association for Computational Linguistics,Equity Beyond Bias in Language Technologies for Education,https://aclanthology.org/W19-4446,2019,,,,,
659,inproceedings,webster-etal-2019-gendered,"The 1st ACL workshop on Gender Bias in Natural Language Processing included a shared task on gendered ambiguous pronoun (GAP) resolution. This task was based on the coreference challenge defined in Webster et al. (2018), designed to benchmark the ability of systems to resolve pronouns in real-world contexts in a gender-fair way. 263 teams competed via a Kaggle competition, with the winning system achieving logloss of 0.13667 and near gender parity. We review the approaches of eleven systems with accepted description papers, noting their effective use of BERT (Devlin et al., 2018), both via fine-tuning and for feature extraction, as well as ensembling.","Florence, Italy","Webster, Kellie  and
Costa-juss{\`a}, Marta R.  and
Hardmeier, Christian  and
Radford, Will",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3801,August,1--7,Association for Computational Linguistics,Gendered Ambiguous Pronoun ({GAP}) Shared Task at the Gender Bias in {NLP} Workshop 2019,https://aclanthology.org/W19-3801,2019,,,,,
660,inproceedings,hitti-etal-2019-proposed,"The purpose of this paper is to present an empirical study on gender bias in text. Current research in this field is focused on detecting and correcting for gender bias in existing machine learning models rather than approaching the issue at the dataset level. The underlying motivation is to create a dataset which could enable machines to learn to differentiate bias writing from non-bias writing. A taxonomy is proposed for structural and contextual gender biases which can manifest themselves in text. A methodology is proposed to fetch one type of structural gender bias, Gender Generalization. We explore the IMDB movie review dataset and 9 different corpora from Project Gutenberg. By filtering out irrelevant sentences, the remaining pool of candidate sentences are sent for human validation. A total of 6123 judgments are made on 1627 sentences and after a quality check on randomly selected sentences we obtain an accuracy of 75{\%}. Out of the 1627 sentences, 808 sentence were labeled as Gender Generalizations. The inter-rater reliability amongst labelers was of 61.14{\%}.","Florence, Italy","Hitti, Yasmeen  and
Jang, Eunbee  and
Moreno, Ines  and
Pelletier, Carolyne",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3802,August,8--17,Association for Computational Linguistics,Proposed Taxonomy for Gender Bias in Text; A Filtering Methodology for the Gender Generalization Subtype,https://aclanthology.org/W19-3802,2019,,,,,
661,inproceedings,friedman-etal-2019-relating,"Modern models for common NLP tasks often employ machine learning techniques and train on journalistic, social media, or other culturally-derived text. These have recently been scrutinized for racial and gender biases, rooting from inherent bias in their training text. These biases are often sub-optimal and recent work poses methods to rectify them; however, these biases may shed light on actual racial or gender gaps in the culture(s) that produced the training text, thereby helping us understand cultural context through big data. This paper presents an approach for quantifying gender bias in word embeddings, and then using them to characterize statistical gender gaps in education, politics, economics, and health. We validate these metrics on 2018 Twitter data spanning 51 U.S. regions and 99 countries. We correlate state and country word embedding biases with 18 international and 5 U.S.-based statistical gender gaps, characterizing regularities and predictive strength.","Florence, Italy","Friedman, Scott  and
Schmer-Galunder, Sonja  and
Chen, Anthony  and
Rye, Jeffrey",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3803,August,18--24,Association for Computational Linguistics,Relating Word Embedding Gender Biases to Gender Gaps: A Cross-Cultural Analysis,https://aclanthology.org/W19-3803,2019,,,,,
662,inproceedings,chaloner-maldonado-2019-measuring,"Prior work has shown that word embeddings capture human stereotypes, including gender bias. However, there is a lack of studies testing the presence of specific gender bias categories in word embeddings across diverse domains. This paper aims to fill this gap by applying the WEAT bias detection method to four sets of word embeddings trained on corpora from four different domains: news, social networking, biomedical and a gender-balanced corpus extracted from Wikipedia (GAP). We find that some domains are definitely more prone to gender bias than others, and that the categories of gender bias present also vary for each set of word embeddings. We detect some gender bias in GAP. We also propose a simple but novel method for discovering new bias categories by clustering word embeddings. We validate this method through WEAT{'}s hypothesis testing mechanism and find it useful for expanding the relatively small set of well-known gender bias word categories commonly used in the literature.","Florence, Italy","Chaloner, Kaytlin  and
Maldonado, Alfredo",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3804,August,25--32,Association for Computational Linguistics,Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories,https://aclanthology.org/W19-3804,2019,,,,,
663,inproceedings,basta-etal-2019-evaluating,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Florence, Italy","Basta, Christine  and
Costa-juss{\`a}, Marta R.  and
Casas, Noe",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3805,August,33--39,Association for Computational Linguistics,Evaluating the Underlying Gender Bias in Contextualized Word Embeddings,https://aclanthology.org/W19-3805,2019,,,,,
664,inproceedings,karve-etal-2019-conceptor,"Bias in word representations, such as Word2Vec, has been widely reported and investigated, and efforts made to debias them. We apply the debiasing conceptor for post-processing both traditional and contextualized word embeddings. Our method can simultaneously remove racial and gender biases from word representations. Unlike standard debiasing methods, the debiasing conceptor can utilize heterogeneous lists of biased words without loss in performance. Finally, our empirical experiments show that the debiasing conceptor diminishes racial and gender bias of word representations as measured using the Word Embedding Association Test (WEAT) of Caliskan et al. (2017).","Florence, Italy","Karve, Saket  and
Ungar, Lyle  and
Sedoc, Jo{\~a}o",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3806,August,40--48,Association for Computational Linguistics,Conceptor Debiasing of Word Representations Evaluated on {WEAT},https://aclanthology.org/W19-3806,2019,,,,,
665,inproceedings,sedoc-ungar-2019-role,"Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied. Standard debiasing methods require heterogeneous lists of target words to identify the {``}bias subspace{''}. We show show that using new contextualized word embeddings in conceptor debiasing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining ({''}Or{'}ing{''}) the debiasing conceptors of the different subsets.","Florence, Italy","Sedoc, Jo{\~a}o  and
Ungar, Lyle",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3808,August,55--61,Association for Computational Linguistics,The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations,https://aclanthology.org/W19-3808,2019,,,,,
666,inproceedings,bhaskaran-bhallamudi-2019-good,"In this work, we investigate the presence of occupational gender stereotypes in sentiment analysis models. Such a task has implications in reducing implicit biases in these models, which are being applied to an increasingly wide variety of downstream tasks. We release a new gender-balanced dataset of 800 sentences pertaining to specific professions and propose a methodology for using it as a test bench to evaluate sentiment analysis models. We evaluate the presence of occupational gender stereotypes in 3 different models using our approach, and explore their relationship with societal perceptions of occupations.","Florence, Italy","Bhaskaran, Jayadev  and
Bhallamudi, Isha",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3809,August,62--68,Association for Computational Linguistics,"Good Secretaries, Bad Truck Drivers? Occupational Gender Stereotypes in Sentiment Analysis",https://aclanthology.org/W19-3809,2019,,,,,
667,inproceedings,prost-etal-2019-debiasing,"(Bolukbasi et al., 2016) demonstrated that pretrained word embeddings can inherit gender bias from the data they were trained on. We investigate how this bias affects downstream classification tasks, using the case study of occupation classification (De-Arteaga et al., 2019). We show that traditional techniques for debiasing embeddings can actually worsen the bias of the downstream classifier by providing a less noisy channel for communicating gender information. With a relatively minor adjustment, however, we show how these same techniques can be used to simultaneously reduce bias and maintain high classification accuracy.","Florence, Italy","Prost, Flavien  and
Thain, Nithum  and
Bolukbasi, Tolga",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3810,August,69--75,Association for Computational Linguistics,Debiasing Embeddings for Reduced Gender Bias in Text Classification,https://aclanthology.org/W19-3810,2019,,,,,
668,inproceedings,bao-qiao-2019-transfer,"The paper describes the submission of the team {``}We used bert!{''} to the shared task Gendered Pronoun Resolution (Pair pronouns to their correct entities). Our final submission model based on the fine-tuned BERT (Bidirectional Encoder Representations from Transformers) ranks 14th among 838 teams with a multi-class logarithmic loss of 0.208. In this work, contribution of transfer learning technique to pronoun resolution systems is investigated and the gender bias contained in classification models is evaluated.","Florence, Italy","Bao, Xingce  and
Qiao, Qianqian",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3812,August,82--88,Association for Computational Linguistics,Transfer Learning from Pre-trained {BERT} for Pronoun Resolution,https://aclanthology.org/W19-3812,2019,,,,,
669,inproceedings,wang-2019-msnet,"The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https://github.com/ziliwang/MSnet-for-Gendered-Pronoun-Resolution","Florence, Italy","Wang, Zili",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3813,August,89--95,Association for Computational Linguistics,{MS}net: A {BERT}-based Network for Gendered Pronoun Resolution,https://aclanthology.org/W19-3813,2019,,,,,
670,inproceedings,xu-yang-2019-look,"Gender bias has been found in existing coreference resolvers. In order to eliminate gender bias, a gender-balanced dataset Gendered Ambiguous Pronouns (GAP) has been released and the best baseline model achieves only 66.9{\%} F1. Bidirectional Encoder Representations from Transformers (BERT) has broken several NLP task records and can be used on GAP dataset. However, fine-tune BERT on a specific task is computationally expensive. In this paper, we propose an end-to-end resolver by combining pre-trained BERT with Relational Graph Convolutional Network (R-GCN). R-GCN is used for digesting structural syntactic information and learning better task-specific embeddings. Empirical results demonstrate that, under explicit syntactic supervision and without the need to fine tune BERT, R-GCN{'}s embeddings outperform the original BERT embeddings on the coreference task. Our work significantly improves the snippet-context baseline F1 score on GAP dataset from 66.9{\%} to 80.3{\%}. We participated in the Gender Bias for Natural Language Processing 2019 shared task, and our codes are available online.","Florence, Italy","Xu, Yinchuan  and
Yang, Junlin",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3814,August,96--101,Association for Computational Linguistics,Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution,https://aclanthology.org/W19-3814,2019,,,,,
671,inproceedings,ionita-etal-2019-resolving,"Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73{\%} F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92{\%} F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.","Florence, Italy","Ionita, Matei  and
Kashnitsky, Yury  and
Krige, Ken  and
Larin, Vladimir  and
Atanasov, Atanas  and
Logvinenko, Dennis",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3817,August,113--119,Association for Computational Linguistics,Resolving Gendered Ambiguous Pronouns with {BERT},https://aclanthology.org/W19-3817,2019,,,,,
672,inproceedings,liu-2019-anonymized,"We present our 7th place solution to the Gendered Pronoun Resolution challenge, which uses BERT without fine-tuning and a novel augmentation strategy designed for contextual embedding token-level tasks. Our method anonymizes the referent by replacing candidate names with a set of common placeholder names. Besides the usual benefits of effectively increasing training data size, this approach diversifies idiosyncratic information embedded in names. Using same set of common first names can also help the model recognize names better, shorten token length, and remove gender and regional biases associated with names. The system scored 0.1947 log loss in stage 2, where the augmentation contributed to an improvements of 0.04. Post-competition analysis shows that, when using different embedding layers, the system scores 0.1799 which would be third place.","Florence, Italy","Liu, Bo",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3818,August,120--125,Association for Computational Linguistics,Anonymized {BERT}: An Augmentation Approach to the Gendered Pronoun Resolution Challenge,https://aclanthology.org/W19-3818,2019,,,,,
673,inproceedings,chada-2019-gendered,"The resolution of ambiguous pronouns is a longstanding challenge in Natural Language Understanding. Recent studies have suggested gender bias among state-of-the-art coreference resolution systems. As an example, Google AI Language team recently released a gender-balanced dataset and showed that performance of these coreference resolvers is significantly limited on the dataset. In this paper, we propose an extractive question answering (QA) formulation of pronoun resolution task that overcomes this limitation and shows much lower gender bias (0.99) on their dataset. This system uses fine-tuned representations from the pre-trained BERT model and outperforms the existing baseline by a significant margin (22.2{\%} absolute improvement in F1 score) without using any hand-engineered features. This QA framework is equally performant even without the knowledge of the candidate antecedents of the pronoun. An ensemble of QA and BERT-based multiple choice and sequence classification models further improves the F1 (23.3{\%} absolute improvement upon the baseline). This ensemble model was submitted to the shared task for the 1st ACL workshop on Gender Bias for Natural Language Processing. It ranked 9th on the final official leaderboard.","Florence, Italy","Chada, Rakesh",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3819,August,126--133,Association for Computational Linguistics,Gendered Pronoun Resolution using {BERT} and an Extractive Question Answering Formulation,https://aclanthology.org/W19-3819,2019,,,,,
674,inproceedings,attree-2019-gendered,"This paper presents a strong set of results for resolving gendered ambiguous pronouns on the Gendered Ambiguous Pronouns shared task. The model presented here draws upon the strengths of state-of-the-art language and coreference resolution models, and introduces a novel evidence-based deep learning architecture. Injecting evidence from the coreference models compliments the base architecture, and analysis shows that the model is not hindered by their weaknesses, specifically gender bias. The modularity and simplicity of the architecture make it very easy to extend for further improvement and applicable to other NLP problems. Evaluation on GAP test data results in a state-of-the-art performance at 92.5{\%} F1 (gender bias of 0.97), edging closer to the human performance of 96.6{\%}. The end-to-end solution presented here placed 1st in the Kaggle competition, winning by a significant lead.","Florence, Italy","Attree, Sandeep",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3820,August,134--146,Association for Computational Linguistics,Gendered Ambiguous Pronouns Shared Task: Boosting Model Confidence by Evidence Pooling,https://aclanthology.org/W19-3820,2019,,,,,
675,inproceedings,escude-font-costa-jussa-2019-equalizing,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","Florence, Italy","Escud{\'e} Font, Joel  and
Costa-juss{\`a}, Marta R.",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3821,August,147--154,Association for Computational Linguistics,Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques,https://aclanthology.org/W19-3821,2019,,,,,
676,inproceedings,habash-etal-2019-automatic,"The impressive progress in many Natural Language Processing (NLP) applications has increased the awareness of some of the biases these NLP systems have with regards to gender identities. In this paper, we propose an approach to extend biased single-output gender-blind NLP systems with gender-specific alternative reinflections. We focus on Arabic, a gender-marking morphologically rich language, in the context of machine translation (MT) from English, and for first-person-singular constructions only. Our contributions are the development of a system-independent gender-awareness wrapper, and the building of a corpus for training and evaluating first-person-singular gender identification and reinflection in Arabic. Our results successfully demonstrate the viability of this approach with 8{\%} relative increase in Bleu score for first-person-singular feminine, and 5.3{\%} comparable increase for first-person-singular masculine on top of a state-of-the-art gender-blind MT system on a held-out test set.","Florence, Italy","Habash, Nizar  and
Bouamor, Houda  and
Chung, Christine",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3822,August,155--165,Association for Computational Linguistics,Automatic Gender Identification and Reinflection in {A}rabic,https://aclanthology.org/W19-3822,2019,,,,,
677,inproceedings,kurita-etal-2019-measuring,"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","Florence, Italy","Kurita, Keita  and
Vyas, Nidhi  and
Pareek, Ayush  and
Black, Alan W  and
Tsvetkov, Yulia",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3823,August,166--172,Association for Computational Linguistics,Measuring Bias in Contextualized Word Representations,https://aclanthology.org/W19-3823,2019,,,,,
678,inproceedings,cho-etal-2019-measuring,"Ethics regarding social bias has recently thrown striking issues in natural language processing. Especially for gender-related topics, the need for a system that reduces the model bias has grown in areas such as image captioning, content recommendation, and automated employment. However, detection and evaluation of gender bias in the machine translation systems are not yet thoroughly investigated, for the task being cross-lingual and challenging to define. In this paper, we propose a scheme for making up a test set that evaluates the gender bias in a machine translation system, with Korean, a language with gender-neutral pronouns. Three word/phrase sets are primarily constructed, each incorporating positive/negative expressions or occupations; all the terms are gender-independent or at least not biased to one side severely. Then, additional sentence lists are constructed concerning formality of the pronouns and politeness of the sentences. With the generated sentence set of size 4,236 in total, we evaluate gender bias in conventional machine translation systems utilizing the proposed measure, which is termed here as translation gender bias index (TGBI). The corpus and the code for evaluation is available on-line.","Florence, Italy","Cho, Won Ik  and
Kim, Ji Won  and
Kim, Seok Min  and
Kim, Nam Soo",Proceedings of the First Workshop on Gender Bias in Natural Language Processing,10.18653/v1/W19-3824,August,173--181,Association for Computational Linguistics,On Measuring Gender Bias in Translation of Gender-neutral Pronouns,https://aclanthology.org/W19-3824,2019,,,,,
679,inproceedings,gonen-goldberg-2019-lipstick-pig,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Florence, Italy","Gonen, Hila  and
Goldberg, Yoav",Proceedings of the 2019 Workshop on Widening NLP,,August,60--63,Association for Computational Linguistics,Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them,https://aclanthology.org/W19-3621,2019,,,,,
680,inproceedings,gonen-etal-2019-grammatical-gender,"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias results in a positive effect on the quality of the resulting word embeddings, both in monolingual and cross lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","Florence, Italy","Gonen, Hila  and
Kementchedjhieva, Yova  and
Goldberg, Yoav",Proceedings of the 2019 Workshop on Widening NLP,,August,64--67,Association for Computational Linguistics,How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?,https://aclanthology.org/W19-3622,2019,,,,,
681,inproceedings,prabhumoye-etal-2019-principled,"We critique recent work on ethics in natural language processing. Those discussions have focused on data collection, experimental design, and interventions in modeling. But we argue that we ought to first understand the frameworks of ethics that are being used to evaluate the fairness and justice of algorithmic systems. Here, we begin that discussion by outlining deontological and consequentialist ethics, and make predictions on the research agenda prioritized by each.","Florence, Italy","Prabhumoye, Shrimai  and
Mayfield, Elijah  and
Black, Alan W",Proceedings of the 2019 Workshop on Widening NLP,,August,118--121,Association for Computational Linguistics,Principled Frameworks for Evaluating Ethics in {NLP} Systems,https://aclanthology.org/W19-3637,2019,,,,,
682,inproceedings,lepp-2019-pardon,"The United States Supreme Court plays a key role in defining the legal basis for gender discrimination throughout the country, yet there are few checks on gender bias within the court itself. In conversational turn-taking, interruptions have been documented as a marker of bias between speakers of different genders. The goal of this study is to automatically differentiate between respectful and disrespectful conversational turns taken during official hearings, which could help in detecting bias and finding remediation techniques for discourse in the courtroom. In this paper, I present a corpus of turns annotated by legal professionals, and describe the design of a semi-supervised classifier that will use acoustic and lexical features to analyze turn-taking at scale. On completion of annotations, this classifier will be trained to extract the likelihood that turns are respectful or disrespectful for use in studies of speech trends.","Florence, Italy","Lepp, Haley",Proceedings of the 2019 Workshop on Widening NLP,,August,143--145,Association for Computational Linguistics,Pardon the Interruption: Automatic Analysis of Gender and Competitive Turn-Taking in {U}nited {S}tates {S}upreme {C}ourt Hearings,https://aclanthology.org/W19-3645,2019,,,,,
683,inproceedings,dziri-etal-2019-evaluating-coherence,"Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. This paper has been accepted in NAACL 2019.","Florence, Italy","Dziri, Nouha  and
Kamalloo, Ehsan  and
Mathewson, Kory  and
Zaiane, Osmar",Proceedings of the 2019 Workshop on Widening NLP,,August,146--148,Association for Computational Linguistics,Evaluating Coherence in Dialogue Systems using Entailment,https://aclanthology.org/W19-3646,2019,,,,,
684,inproceedings,lee-etal-2019-exploring,"Exploring social bias in chatbot is an important, yet relatively unexplored problem. In this paper, we propose an approach to understand social bias in chatbots by leveraging stereotype knowledge. It allows interesting comparison of bias between chatbots and humans, and provides intuitive analysis of existing chatbots by borrowing the finer-grain concepts of sexism and racism.","Florence, Italy","Lee, Nayeon  and
Madotto, Andrea  and
Fung, Pascale",Proceedings of the 2019 Workshop on Widening NLP,,August,177--180,Association for Computational Linguistics,Exploring Social Bias in Chatbots using Stereotype Knowledge,https://aclanthology.org/W19-3655,2019,,,,,
685,inproceedings,zhai-etal-2019-hybrid,"Automatically generating globally coherent stories is a challenging problem. Neural text generation models have been shown to perform well at generating fluent sentences from data, but they usually fail to keep track of the overall coherence of the story after a couple of sentences. Existing work that incorporates a text planning module succeeded in generating recipes and dialogues, but appears quite data-demanding. We propose a novel story generation approach that generates globally coherent stories from a fairly small corpus. The model exploits a symbolic text planning module to produce text plans, thus reducing the demand of data; a neural surface realization module then generates fluent text conditioned on the text plan. Human evaluation showed that our model outperforms various baselines by a wide margin and generates stories which are fluent as well as globally coherent.","Florence, Italy","Zhai, Fangzhou  and
Demberg, Vera  and
Shkadzko, Pavel  and
Shi, Wei  and
Sayeed, Asad",Proceedings of the Second Workshop on Storytelling,10.18653/v1/W19-3404,August,34--45,Association for Computational Linguistics,A Hybrid Model for Globally Coherent Story Generation,https://aclanthology.org/W19-3404,2019,,,,,
686,inproceedings,nalabandian-ireland-2019-depressed,"Depression is characterized by a self-focused negative attentional bias, which is often reflected in everyday language use. In a prospective writing study, we explored whether the association between depressive symptoms and negative, self-focused language varies across social contexts. College students (N = 243) wrote about a recent interaction with a person they care deeply about. Depression symptoms positively correlated with negative emotion words and first-person singular pronouns (or negative self-focus) when writing about a recent interaction with romantic partners or, to a lesser extent, friends, but not family members. The pattern of results was more pronounced when participants perceived greater self-other overlap (i.e., interpersonal closeness) with their romantic partner. Findings regarding how the linguistic profile of depression differs by type of relationship may inform more effective methods of clinical diagnosis and treatment.","Minneapolis, Minnesota","Nalabandian, Taleen  and
Ireland, Molly",Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,10.18653/v1/W19-3008,June,62--73,Association for Computational Linguistics,Depressed Individuals Use Negative Self-Focused Language When Recalling Recent Interactions with Close Romantic Partners but Not Family or {F}riends,https://aclanthology.org/W19-3008,2019,,,,,
687,inproceedings,parish-morris-2019-computational,"Computational linguistics holds promise for improving scientific integrity in clinical psychology, and for reducing longstanding inequities in healthcare access and quality. This paper describes how computational linguistics approaches could address the {``}reproducibility crisis{''} facing social science, particularly with regards to reliable diagnosis of neurodevelopmental and psychiatric conditions including autism spectrum disorder (ASD). It is argued that these improvements in scientific integrity are poised to naturally reduce persistent healthcare inequities in neglected subpopulations, such as verbally fluent girls and women with ASD, but that concerted attention to this issue is necessary to avoid reproducing biases built into training data. Finally, it is suggested that computational linguistics is just one component of an emergent digital phenotyping toolkit that could ultimately be used for clinical decision support, to improve clinical care via precision medicine (i.e., personalized intervention planning), granular treatment response monitoring (including remotely), and for gene-brain-behavior studies aiming to pinpoint the underlying biological etiology of otherwise behaviorally-defined conditions like ASD.","Minneapolis, Minnesota","Parish-Morris, Julia",Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,10.18653/v1/W19-3011,June,94--102,Association for Computational Linguistics,Computational Linguistics for Enhancing Scientific Reproducibility and Reducing Healthcare Inequities,https://aclanthology.org/W19-3011,2019,,,,,
688,inproceedings,amir-etal-2019-mental,"The ability to track mental health conditions via social media opened the doors for large-scale, automated, mental health surveillance. However, inferring accurate population-level trends requires representative samples of the underlying population, which can be challenging given the biases inherent in social media data. While previous work has adjusted samples based on demographic estimates, the populations were selected based on specific outcomes, e.g. specific mental health conditions. We depart from these methods, by conducting analyses over demographically representative digital cohorts of social media users. To validated this approach, we constructed a cohort of US based Twitter users to measure the prevalence of depression and PTSD, and investigate how these illnesses manifest across demographic subpopulations. The analysis demonstrates that cohort-based studies can help control for sampling biases, contextualize outcomes, and provide deeper insights into the data.","Minneapolis, Minnesota","Amir, Silvio  and
Dredze, Mark  and
Ayers, John W.",Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,10.18653/v1/W19-3013,June,114--120,Association for Computational Linguistics,Mental Health Surveillance over Social Media with Digital Cohorts,https://aclanthology.org/W19-3013,2019,,,,,
689,inproceedings,lippincott-2019-graph,"This work considers a task from traditional literary criticism: annotating a structured, composite document with information about its sources. We take the Documentary Hypothesis, a prominent theory regarding the composition of the first five books of the Hebrew bible, extract stylistic features designed to avoid bias or overfitting, and train several classification models. Our main result is that the recently-introduced graph convolutional network architecture outperforms structurally-uninformed models. We also find that including information about the granularity of text spans is a crucial ingredient when employing hidden layers, in contrast to simple logistic regression. We perform error analysis at several levels, noting how some characteristic limitations of the models and simple features lead to misclassifications, and conclude with an overview of future work.","Minneapolis, USA","Lippincott, Tom","Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",10.18653/v1/W19-2510,June,76--81,Association for Computational Linguistics,Graph convolutional networks for exploring authorship hypotheses,https://aclanthology.org/W19-2510,2019,,,,,
690,inproceedings,lauretig-2019-identification,"Social scientists have recently turned to analyzing text using tools from natural language processing like word embeddings to measure concepts like ideology, bias, and affinity. However, word embeddings are difficult to use in the regression framework familiar to social scientists: embeddings are are neither identified, nor directly interpretable. I offer two advances on standard embedding models to remedy these problems. First, I develop Bayesian Word Embeddings with Automatic Relevance Determination priors, relaxing the assumption that all embedding dimensions have equal weight. Second, I apply work identifying latent variable models to anchor embeddings, identifying them, and making them interpretable and usable in a regression. I then apply this model and anchoring approach to two cases, the shift in internationalist rhetoric in the American presidents{'} inaugural addresses, and the relationship between bellicosity in American foreign policy decision-makers{'} deliberations. I find that inaugural addresses became less internationalist after 1945, which goes against the conventional wisdom, and that an increase in bellicosity is associated with an increase in hostile actions by the United States, showing that elite deliberations are not cheap talk, and helping confirm the validity of the model.","Minneapolis, Minnesota","Lauretig, Adam",Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/W19-2102,June,7--17,Association for Computational Linguistics,"Identification, Interpretability, and {B}ayesian Word Embeddings",https://aclanthology.org/W19-2102,2019,,,,,
691,inproceedings,gillani-levy-2019-simple,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset{---}talk radio shows from around the US{---}to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Minneapolis, Minnesota","Gillani, Nabeel  and
Levy, Roger",Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science,10.18653/v1/W19-2111,June,94--99,Association for Computational Linguistics,Simple dynamic word embeddings for mapping perceptions in the public sphere,https://aclanthology.org/W19-2111,2019,,,,,
692,inproceedings,grand-belinkov-2019-adversarial,"Visual question answering (VQA) models have been shown to over-rely on linguistic biases in VQA datasets, answering questions {``}blindly{''} without considering visual context. Adversarial regularization (AdvReg) aims to address this issue via an adversary sub-network that encourages the main model to learn a bias-free representation of the question. In this work, we investigate the strengths and shortcomings of AdvReg with the goal of better understanding how it affects inference in VQA models. Despite achieving a new state-of-the-art on VQA-CP, we find that AdvReg yields several undesirable side-effects, including unstable gradients and sharply reduced performance on in-domain examples. We demonstrate that gradual introduction of regularization during training helps to alleviate, but not completely solve, these issues. Through error analyses, we observe that AdvReg improves generalization to binary questions, but impairs performance on questions with heterogeneous answer distributions. Qualitatively, we also find that regularized models tend to over-rely on visual features, while ignoring important linguistic cues in the question. Our results suggest that AdvReg requires further refinement before it can be considered a viable bias mitigation technique for VQA.","Minneapolis, Minnesota","Grand, Gabriel  and
Belinkov, Yonatan",Proceedings of the Second Workshop on Shortcomings in Vision and Language,10.18653/v1/W19-1801,June,1--13,Association for Computational Linguistics,"Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects",https://aclanthology.org/W19-1801,2019,,,,,
693,inproceedings,wiriyathammabhum-etal-2019-referring,"This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.","Minneapolis, Minnesota","Wiriyathammabhum, Peratham  and
Shrivastava, Abhinav  and
Morariu, Vlad  and
Davis, Larry",Proceedings of the Second Workshop on Shortcomings in Vision and Language,10.18653/v1/W19-1802,June,14--25,Association for Computational Linguistics,Referring to Objects in Videos Using Spatio-Temporal Identifying Descriptions,https://aclanthology.org/W19-1802,2019,,,,,
694,inproceedings,conser-etal-2019-revisiting,"We revisit a particular visual grounding method: the {``}Image Retrieval Using Scene Graphs{''} (IRSG) system of Johnson et al. Our experiments indicate that the system does not effectively use its learned object-relationship models. We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it. We find that these datasets exhibit bias that allows methods that ignore relationships to perform relatively well. We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed. Our studies contribute to a more general effort: that of better understanding what machine-learning methods that combine language and vision actually learn and what popular datasets actually test.","Minneapolis, Minnesota","Conser, Erik  and
Hahn, Kennedy  and
Watson, Chandler  and
Mitchell, Melanie",Proceedings of the Second Workshop on Shortcomings in Vision and Language,10.18653/v1/W19-1804,June,37--46,Association for Computational Linguistics,Revisiting Visual Grounding,https://aclanthology.org/W19-1804,2019,,,,,
695,inproceedings,ghanimifard-dobnik-2019-neural,"Understanding and generating spatial descriptions requires knowledge about what objects are related, their functional interactions, and where the objects are geometrically located. Different spatial relations have different functional and geometric bias. The wide usage of neural language models in different areas including generation of image description motivates the study of what kind of knowledge is encoded in neural language models about individual spatial relations. With the premise that the functional bias of relations is expressed in their word distributions, we construct multi-word distributional vector representations and show that these representations perform well on intrinsic semantic reasoning tasks, thus confirming our premise. A comparison of our vector representations to human semantic judgments indicates that different bias (functional or geometric) is captured in different data collection tasks which suggests that the contribution of the two meaning modalities is dynamic, related to the context of the task.","Minneapolis, Minnesota","Ghanimifard, Mehdi  and
Dobnik, Simon",Proceedings of the Combined Workshop on Spatial Language Understanding ({S}p{LU}) and Grounded Communication for Robotics ({R}obo{NLP}),10.18653/v1/W19-1608,June,71--81,Association for Computational Linguistics,What a neural language model tells us about spatial relations,https://aclanthology.org/W19-1608,2019,,,,,
696,inproceedings,wartena-etal-2019-sentiment,"We present a simple method to find topics in user reviews that accompany ratings for products or services. Standard topic analysis will perform sub-optimal on such data since the word distributions in the documents are not only determined by the topics but by the sentiment as well. We reduce the influence of the sentiment on the topic selection by adding two explicit topics, representing positive and negative sentiment. We evaluate the proposed method on a set of over 15,000 hospital reviews. We show that the proposed method, Latent Semantic Analysis with explicit word features, finds topics with a much smaller bias for sentiments than other similar methods.","Gothenburg, Sweden","Wartena, Christian  and
Sander, Uwe  and
Patzelt, Christiane",Proceedings of the 13th International Conference on Computational Semantics - Short Papers,10.18653/v1/W19-0509,May,59--64,Association for Computational Linguistics,Sentiment Independent Topic Detection in Rated Hospital Reviews,https://aclanthology.org/W19-0509,2019,,,,,
697,inproceedings,leonandya-etal-2019-fast,"Learning to follow human instructions is a long-pursued goal in artificial intelligence. The task becomes particularly challenging if no prior knowledge of the employed language is assumed while relying only on a handful of examples to learn from. Work in the past has relied on hand-coded components or manually engineered features to provide strong inductive biases that make learning in such situations possible. In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of a new given speaker. Controlled experiments show that when the network is exposed to familiar instructions but containing novel words, the model adapts very efficiently to the new vocabulary. Moreover, even for human speakers whose language usage can depart significantly from our artificial training language, our network can still make use of its automatically acquired inductive bias to learn to follow instructions more effectively.","Gothenburg, Sweden","Leonandya, Rezka  and
Hupkes, Dieuwke  and
Bruni, Elia  and
Kruszewski, Germ{\'a}n",Proceedings of the 13th International Conference on Computational Semantics - Long Papers,10.18653/v1/W19-0419,May,223--234,Association for Computational Linguistics,The Fast and the Flexible: Training Neural Networks to Learn to Follow Instructions from Small Data,https://aclanthology.org/W19-0419,2019,,,,,
698,inproceedings,lane-bird-2019-towards,"Kunwinjku is an indigenous Australian language spoken in northern Australia which exhibits agglutinative and polysynthetic properties. Members of the community have expressed interest in co-developing language applications that promote their values and priorities. Modeling the morphology of the Kunwinjku language is an important step towards accomplishing the community{'}s goals. Finite State Transducers have long been the go-to method for modeling morphologically rich languages, and in this paper we discuss some of the distinct modeling challenges present in the morphosyntax of verbs in Kunwinjku. We show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. Continuing challenges include robustness in the face of variation and unseen vocabulary, as well as how to handle complex reduplicative processes. Our future work will build off the baseline and challenges presented here.","Sydney, Australia","Lane, William  and
Bird, Steven",Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,,4--6 December,1--9,Australasian Language Technology Association,Towards A Robust Morphological Analyzer for Kunwinjku,https://aclanthology.org/U19-1001,2019,,,,,
699,inproceedings,papadopoulou-etal-2019-brenda,"In the effort to tackle the challenge of Hyperpartisan News Detection, i.e., the task of deciding whether a news article is biased towards one party, faction, cause, or person, we experimented with two systems: i) a standard supervised learning approach using superficial text and bag-of-words features from the article title and body, and ii) a deep learning system comprising a four-layer convolutional neural network and max-pooling layers after the embedding layer, feeding the consolidated features to a bi-directional recurrent neural network. We achieved an F-score of 0.712 with our best approach, which corresponds to the mid-range of performance levels in the leaderboard.","Minneapolis, Minnesota, USA","Papadopoulou, Olga  and
Kordopatis-Zilos, Giorgos  and
Zampoglou, Markos  and
Papadopoulos, Symeon  and
Kompatsiaris, Yiannis",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2157,June,924--928,Association for Computational Linguistics,Brenda Starr at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection,https://aclanthology.org/S19-2157,2019,,,,,
700,inproceedings,gupta-etal-2019-clark,"In this paper, we present a news bias prediction system, which we developed as part of a SemEval 2019 task. We developed an XGBoost based system which uses character and word level n-gram features represented using TF-IDF, count vector based correlation matrix, and predicts if an input news article is a hyperpartisan news article. Our model was able to achieve a precision of 68.3{\%} on the test set provided by the contest organizers. We also run our model on the BuzzFeed corpus and find XGBoost with simple character level N-Gram embeddings to be performing well with an accuracy of around 96{\%}.","Minneapolis, Minnesota, USA","Gupta, Viresh  and
Kaur Jolly, Baani Leen  and
Kaur, Ramneek  and
Chakraborty, Tanmoy",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2159,June,934--938,Association for Computational Linguistics,{C}lark {K}ent at {S}em{E}val-2019 Task 4: Stylometric Insights into Hyperpartisan News Detection,https://aclanthology.org/S19-2159,2019,,,,,
701,inproceedings,sengupta-pedersen-2019-duluth,"This paper describes the Pioquinto Manterola Hyperpartisan News Detector, which participated in SemEval-2019 Task 4. Hyperpartisan news is highly polarized and takes a very biased or one{--}sided view of a particular story. We developed two variants of our system, the more successful was a Logistic Regression classifier based on unigram features. This was our official entry in the task, and it placed 23rd of 42 participating teams. Our second variant was a Convolutional Neural Network that did not perform as well.","Minneapolis, Minnesota, USA","Sengupta, Saptarshi  and
Pedersen, Ted",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2162,June,949--953,Association for Computational Linguistics,{D}uluth at {S}em{E}val-2019 Task 4: The Pioquinto Manterola Hyperpartisan News Detector,https://aclanthology.org/S19-2162,2019,,,,,
702,inproceedings,palic-etal-2019-takelab,"In this paper, we demonstrate the system built to solve the SemEval-2019 task 4: Hyperpartisan News Detection (Kiesel et al., 2019), the task of automatically determining whether an article is heavily biased towards one side of the political spectrum. Our system receives an article in its raw, textual form, analyzes it, and predicts with moderate accuracy whether the article is hyperpartisan. The learning model used was primarily trained on a manually prelabeled dataset containing news articles. The system relies on the previously constructed SVM model, available in the Python Scikit-Learn library. We ranked 6th in the competition of 42 teams with an accuracy of 79.1{\%} (the winning team had 82.2{\%}).","Minneapolis, Minnesota, USA","Pali{\'c}, Niko  and
Vladika, Juraj  and
{\v{C}}ubeli{\'c}, Dominik  and
Lovren{\v{c}}i{\'c}, Ivan  and
Buljan, Maja  and
{\v{S}}najder, Jan",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2172,June,995--998,Association for Computational Linguistics,{T}ake{L}ab at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection,https://aclanthology.org/S19-2172,2019,,,,,
703,inproceedings,anthonio-kloppenburg-2019-team,"In this paper we describe our participation in the SemEval 2019 shared task on hyperpartisan news detection. We present the system that we submitted for final evaluation and the three approaches that we used: sentiment, bias-laden words and filtered n-gram features. Our submitted model is a Linear SVM that solely relies on the negative sentiment of a document. We achieved an accuracy of 0.621 and a f1 score of 0.694 in the competition, revealing the predictive power of negative sentiment for this task. There was no major improvement by adding or substituting the features of the other two approaches that we tried.","Minneapolis, Minnesota, USA","Anthonio, Talita  and
Kloppenburg, Lennart",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2177,June,1016--1020,Association for Computational Linguistics,Team Kermit-the-frog at {S}em{E}val-2019 Task 4: Bias Detection Through Sentiment Analysis and Simple Linguistic Features,https://aclanthology.org/S19-2177,2019,,,,,
704,inproceedings,cramerus-scheffler-2019-team,"This paper describes the approach of team Kit Kittredge to SemEval-2019 Task 4: Hyperpartisan News Detection. The goal was binary classification of news articles into the categories of {``}biased{''} or {``}unbiased{''}. We had two software submissions: one a simple bag-of-words model, and the second an LSTM (Long Short Term Memory) neural network, which was trained on a subset of the original dataset selected by a voting system of other LSTMs. This method did not prove much more successful than the baseline, however, due to the models{'} tendency to learn publisher-specific traits instead of general bias.","Minneapolis, Minnesota, USA","Cramerus, Rebekah  and
Scheffler, Tatjana",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2178,June,1021--1025,Association for Computational Linguistics,Team Kit Kittredge at {S}em{E}val-2019 Task 4: {LSTM} Voting System,https://aclanthology.org/S19-2178,2019,,,,,
705,inproceedings,farber-etal-2019-team,"In this paper, we present an approach for classifying news articles as biased (i.e., hyperpartisan) or unbiased, based on a convolutional neural network. We experiment with various embedding methods (pretrained and trained on the training dataset) and variations of the convolutional neural network architecture and compare the results. When evaluating our best performing approach on the actual test data set of the SemEval 2019 Task 4, we obtained relatively low precision and accuracy values, while gaining the highest recall rate among all 42 participating teams.","Minneapolis, Minnesota, USA","F{\""a}rber, Michael  and
Qurdina, Agon  and
Ahmedi, Lule",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2180,June,1032--1036,Association for Computational Linguistics,Team Peter Brinkmann at {S}em{E}val-2019 Task 4: Detecting Biased News Articles Using Convolutional Neural Networks,https://aclanthology.org/S19-2180,2019,,,,,
706,inproceedings,ning-etal-2019-team,"This paper describes the team peter-parker{'}s participation in Hyperpartisan News Detection task (SemEval-2019 Task 4), which requires to classify whether a given news article is bias or not. We decided to use JAVA to do the article parsing tool and the BERT-Based model to do the bias prediction. Furthermore, we will show experiment results with analysis.","Minneapolis, Minnesota, USA","Ning, Zhiyuan  and
Lin, Yuanzhen  and
Zhong, Ruichao",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2181,June,1037--1040,Association for Computational Linguistics,Team Peter-Parker at {S}em{E}val-2019 Task 4: {BERT}-Based Method in Hyperpartisan News Detection,https://aclanthology.org/S19-2181,2019,,,,,
707,inproceedings,saleh-etal-2019-team,"We describe our submission to SemEval-2019 Task 4 on Hyperpartisan News Detection. We rely on a variety of engineered features originally used to detect propaganda. This is based on the assumption that biased messages are propagandistic and promote a particular political cause or viewpoint. In particular, we trained a logistic regression model with features ranging from simple bag of words to vocabulary richness and text readability. Our system achieved 72.9{\%} accuracy on the manually annotated testset, and 60.8{\%} on the test data that was obtained with distant supervision. Additional experiments showed that significant performance gains can be achieved with better feature pre-processing.","Minneapolis, Minnesota, USA","Saleh, Abdelrhman  and
Baly, Ramy  and
Barr{\'o}n-Cede{\~n}o, Alberto  and
Da San Martino, Giovanni  and
Mohtarami, Mitra  and
Nakov, Preslav  and
Glass, James",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2182,June,1041--1046,Association for Computational Linguistics,Team {QCRI}-{MIT} at {S}em{E}val-2019 Task 4: Propaganda Analysis Meets Hyperpartisan News Detection,https://aclanthology.org/S19-2182,2019,,,,,
708,inproceedings,bestgen-2019-tintin,"Tintin, the system proposed by the CECL for the Hyperpartisan News Detection task of SemEval 2019, is exclusively based on the tokens that make up the documents and a standard supervised learning procedure. It obtained very contrasting results: poor on the main task, but much more effective at distinguishing documents published by hyperpartisan media outlets from unbiased ones, as it ranked first. An analysis of the most important features highlighted the positive aspects, but also some potential limitations of the approach.","Minneapolis, Minnesota, USA","Bestgen, Yves",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2186,June,1062--1066,Association for Computational Linguistics,Tintin at {S}em{E}val-2019 Task 4: Detecting Hyperpartisan News Article with only Simple Tokens,https://aclanthology.org/S19-2186,2019,,,,,
709,inproceedings,yeh-etal-2019-tom,"In this paper, we describe our attempt to learn bias from news articles. From our experiments, it seems that although there is a correlation between publisher bias and article bias, it is challenging to learn bias directly from the publisher labels. On the other hand, using few manually-labeled samples can increase the accuracy metric from around 60{\%} to near 80{\%}. Our system is computationally inexpensive and uses several standard document representations in NLP to train an SVM or LR classifier. The system ranked 4th in the SemEval-2019 task. The code is released for reproducibility.","Minneapolis, Minnesota, USA","Yeh, Chia-Lun  and
Loni, Babak  and
Schuth, Anne",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2187,June,1067--1071,Association for Computational Linguistics,Tom Jumbo-Grumbo at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection with {G}lo{V}e vectors and {SVM},https://aclanthology.org/S19-2187,2019,,,,,
710,inproceedings,srivastava-etal-2019-vernon,"In this paper, we present our submission for SemEval-2019 Task 4: Hyperpartisan News Detection. Hyperpartisan news articles are sharply polarized and extremely biased (onesided). It shows blind beliefs, opinions and unreasonable adherence to a party, idea, faction or a person. Through this task, we aim to develop an automated system that can be used to detect hyperpartisan news and serve as a prescreening technique for fake news detection. The proposed system jointly uses a rich set of handcrafted textual and semantic features. Our system achieved 2nd rank on the primary metric (82.0{\%} accuracy) and 1st rank on the secondary metric (82.1{\%} F1-score), among all participating teams. Comparison with the best performing system on the leaderboard shows that our system is behind by only 0.2{\%} absolute difference in accuracy.","Minneapolis, Minnesota, USA","Srivastava, Vertika  and
Gupta, Ankita  and
Prakash, Divya  and
Sahoo, Sudeep Kumar  and
R.R, Rohit  and
Kim, Yeon Hyang",Proceedings of the 13th International Workshop on Semantic Evaluation,10.18653/v1/S19-2189,June,1078--1082,Association for Computational Linguistics,Vernon-fenwick at {S}em{E}val-2019 Task 4: Hyperpartisan News Detection using Lexical and Semantic Features,https://aclanthology.org/S19-2189,2019,,,,,
711,inproceedings,lauscher-glavas-2019-consistently,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Minneapolis, Minnesota","Lauscher, Anne  and
Glava{\v{s}}, Goran",Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),10.18653/v1/S19-1010,June,85--91,Association for Computational Linguistics,Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors,https://aclanthology.org/S19-1010,2019,,,,,
712,inproceedings,nastase-kotnis-2019-abstract,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs {--} e.g. entity embeddings, relation representations or patterns {--} will be affected by the imbalance in the information captured in the graph {--} by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Minneapolis, Minnesota","Nastase, Vivi  and
Kotnis, Bhushan",Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),10.18653/v1/S19-1016,June,147--157,Association for Computational Linguistics,Abstract Graphs and Abstract Paths for Knowledge Graph Completion,https://aclanthology.org/S19-1016,2019,,,,,
713,inproceedings,agarwal-etal-2019-word,"Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person{'}s Big Five personality traits formed solely from information about the occupation, nationality or a common noun description of a hypothetical person. Analysis of the data reveals a large number of statistically significant stereotypes in people. We then demonstrate the bias captured in lexical representations is statistically significantly correlated with the documented human bias. Our results, showing bias for a large set of person descriptors for such nuanced traits put in doubt the feasibility of broadly and fairly applying debiasing methods and call for the development of new methods for auditing language technology systems and resources.","Minneapolis, Minnesota","Agarwal, Oshin  and
Durup{\i}nar, Funda  and
Badler, Norman I.  and
Nenkova, Ani",Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),10.18653/v1/S19-1023,June,205--211,Association for Computational Linguistics,Word Embeddings (Also) Encode Human Personality Stereotypes,https://aclanthology.org/S19-1023,2019,,,,,
714,inproceedings,belinkov-etal-2019-adversarial,"Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.","Minneapolis, Minnesota","Belinkov, Yonatan  and
Poliak, Adam  and
Shieber, Stuart  and
Van Durme, Benjamin  and
Rush, Alexander",Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),10.18653/v1/S19-1028,June,256--262,Association for Computational Linguistics,On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference,https://aclanthology.org/S19-1028,2019,,,,,
715,inproceedings,aleksandrova-etal-2019-multilingual,"We propose a multilingual method for the extraction of biased sentences from Wikipedia, and use it to create corpora in Bulgarian, French and English. Sifting through the revision history of the articles that at some point had been considered biased and later corrected, we retrieve the last tagged and the first untagged revisions as the before/after snapshots of what was deemed a violation of Wikipedia{'}s neutral point of view policy. We extract the sentences that were removed or rewritten in that edit. The approach yields sufficient data even in the case of relatively small Wikipedias, such as the Bulgarian one, where 62k articles produced 5k biased sentences. We evaluate our method by manually annotating 520 sentences for Bulgarian and French, and 744 for English. We assess the level of noise and analyze its sources. Finally, we exploit the data with well-known classification methods to detect biased sentences. Code and datasets are hosted at https://github.com/crim-ca/wiki-bias.","Varna, Bulgaria","Aleksandrova, Desislava  and
Lareau, Fran{\c{c}}ois  and
M{\'e}nard, Pierre Andr{\'e}",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),10.26615/978-954-452-056-4_006,September,42--51,INCOMA Ltd.,Multilingual sentence-level bias detection in {W}ikipedia,https://aclanthology.org/R19-1006,2019,,,,,
716,inproceedings,arikan-etal-2019-detecting,"For the spell correction task, vocabulary based methods have been replaced with methods that take morphological and grammar rules into account. However, such tools are fairly immature, and, worse, non-existent for many low resource languages. Checking only if a word is well-formed with respect to the morphological rules of a language may produce false negatives due to the ambiguity resulting from the presence of numerous homophonic words. In this work, we propose an approach to detect and correct the {``}de/da{''} clitic errors in Turkish text. Our model is a neural sequence tagger trained with a synthetically constructed dataset consisting of positive and negative samples. The model{'}s performance with this dataset is presented according to different word embedding configurations. The model achieved an F1 score of 86.67{\%} on a synthetically constructed dataset. We also compared the model{'}s performance on a manually curated dataset of challenging samples that proved superior to other spelling correctors with 71{\%} accuracy compared to the second-best (Google Docs) with and accuracy of 34{\%}.","Varna, Bulgaria","Arikan, Ugurcan  and
Gungor, Onur  and
Uskudarli, Suzan",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),10.26615/978-954-452-056-4_009,September,71--76,INCOMA Ltd.,Detecting Clitics Related Orthographic Errors in {T}urkish,https://aclanthology.org/R19-1009,2019,,,,,
717,inproceedings,beloucif-etal-2019-naive,"Neural machine translation models have little inductive bias, which can be a disadvantage in low-resource scenarios. Neural models have to be trained on large amounts of data and have been shown to perform poorly when only limited data is available. We show that using naive regularization methods, based on sentence length, punctuation and word frequencies, to penalize translations that are very different from the input sentences, consistently improves the translation quality across multiple low-resource languages. We experiment with 12 language pairs, varying the training data size between 17k to 230k sentence pairs. Our best regularizer achieves an average increase of 1.5 BLEU score and 1.0 TER score across all the language pairs. For example, we achieve a BLEU score of 26.70 on the IWSLT15 English{--}Vietnamese translation task simply by using relative differences in punctuation as a regularizer.","Varna, Bulgaria","Beloucif, Meriem  and
Gonzalez, Ana Valeria  and
Bollmann, Marcel  and
S{\o}gaard, Anders",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),10.26615/978-954-452-056-4_013,September,102--111,INCOMA Ltd.,Naive Regularizers for Low-Resource Neural Machine Translation,https://aclanthology.org/R19-1013,2019,,,,,
718,inproceedings,shoeb-etal-2019-emotag,"Despite being a fairly recent phenomenon, emojis have quickly become ubiquitous. Besides their extensive use in social media, they are now also invoked in customer surveys and feedback forms. Hence, there is a need for techniques to understand their sentiment and emotion. In this work, we provide a method to quantify the emotional association of basic emotions such as anger, fear, joy, and sadness for a set of emojis. We collect and process a unique corpus of 20 million emoji-centric tweets, such that we can capture rich emoji semantics using a comparably small dataset. We evaluate the induced emotion profiles of emojis with regard to their ability to predict word affect intensities as well as sentiment scores.","Varna, Bulgaria","Shoeb, Abu Awal Md  and
Raji, Shahab  and
de Melo, Gerard",Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),10.26615/978-954-452-056-4_126,September,1094--1103,INCOMA Ltd.,{E}mo{T}ag {--} Towards an Emotion-Based Analysis of Emojis,https://aclanthology.org/R19-1126,2019,,,,,
719,inproceedings,martins-etal-2019-latent,"Latent structure models are a powerful tool for modeling compositional data, discovering linguistic structure, and building NLP pipelines. They are appealing for two main reasons: they allow incorporating structural bias during training, leading to more accurate models; and they allow discovering hidden linguistic structure, which provides better interpretability. This tutorial will cover recent advances in discrete latent structure models. We discuss their motivation, potential, and limitations, then explore in detail three strategies for designing such models: gradient approximation, reinforcement learning, and end-to-end differentiable methods. We highlight connections among all these methods, enumerating their strengths and weaknesses. The models we present and analyze have been applied to a wide variety of NLP tasks, including sentiment analysis, natural language inference, language modeling, machine translation, and semantic parsing. Examples and evaluation will be covered throughout. After attending the tutorial, a practitioner will be better informed about which method is best suited for their problem.","Florence, Italy","Martins, Andr{\'e} F. T.  and
Mihaylova, Tsvetomila  and
Nangia, Nikita  and
Niculae, Vlad",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,10.18653/v1/P19-4001,July,1--5,Association for Computational Linguistics,Latent Structure Models for Natural Language Processing,https://aclanthology.org/P19-4001,2019,,,,,
720,inproceedings,vig-2019-multiscale,"The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.","Florence, Italy","Vig, Jesse",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,10.18653/v1/P19-3007,July,37--42,Association for Computational Linguistics,A Multiscale Visualization of Attention in the Transformer Model,https://aclanthology.org/P19-3007,2019,,,,,
721,inproceedings,tay-2019-reviews,"Consumers read online reviews for insights which help them to make decisions. Given the large volumes of reviews, succinct review summaries are important for many applications. Existing research has focused on mining for opinions from only review texts and largely ignores the reviewers. However, reviewers have biases and may write lenient or harsh reviews; they may also have preferences towards some topics over others. Therefore, not all reviews are equal. Ignoring the biases in reviews can generate misleading summaries. We aim for summarization of reviews to include balanced opinions from reviewers of different biases and preferences. We propose to model reviewer biases from their review texts and rating distributions, and learn a bias-aware opinion representation. We further devise an approach for balanced opinion summarization of reviews using our bias-aware opinion representation.","Florence, Italy","Tay, Wenyi",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/P19-2005,July,34--42,Association for Computational Linguistics,Not All Reviews Are Equal: Towards Addressing Reviewer Biases for Opinion Summarization,https://aclanthology.org/P19-2005,2019,,,,,
722,inproceedings,bommasani-2019-long,"Neural models at the sentence level often operate on the constituent words/tokens in a way that encodes the inductive bias of processing the input in a similar fashion to how humans do. However, there is no guarantee that the standard ordering of words is computationally efficient or optimal. To help mitigate this, we consider a dependency parse as a proxy for the inter-word dependencies in a sentence and simplify the sentence with respect to combinatorial objectives imposed on the sentence-parse pair. The associated optimization results in permuted sentences that are provably (approximately) optimal with respect to minimizing dependency parse lengths and that are demonstrably simpler. We evaluate our general-purpose permutations within a fine-tuning schema for the downstream task of subjectivity analysis. Our fine-tuned baselines reflect a new state of the art for the SUBJ dataset and the permutations we introduce lead to further improvements with a 2.0{\%} increase in classification accuracy (absolute) and a 45{\%} reduction in classification error (relative) over the previous state of the art.","Florence, Italy","Bommasani, Rishi",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/P19-2012,July,89--99,Association for Computational Linguistics,Long-Distance Dependencies Don{'}t Have to Be Long: Simplifying through Provably (Approximately) Optimal Permutations,https://aclanthology.org/P19-2012,2019,,,,,
723,inproceedings,qian-etal-2019-reducing,"Gender bias exists in natural language datasets, which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach and show that it outperforms existing strategies in all bias evaluation metrics.","Florence, Italy","Qian, Yusu  and
Muaz, Urwa  and
Zhang, Ben  and
Hyun, Jae Won",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/P19-2031,July,223--228,Association for Computational Linguistics,Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function,https://aclanthology.org/P19-2031,2019,,,,,
724,inproceedings,mihaylova-martins-2019-scheduled,"Scheduled sampling is a technique for avoiding one of the known problems in sequence-to-sequence generation: exposure bias. It consists of feeding the model a mix of the teacher forced embeddings and the model predictions from the previous step in training time. The technique has been used for improving model performance with recurrent neural networks (RNN). In the Transformer model, unlike the RNN, the generation of a new word attends to the full sentence generated so far, not only to the last word, and it is not straightforward to apply the scheduled sampling technique. We propose some structural changes to allow scheduled sampling to be applied to Transformer architectures, via a two-pass decoding strategy. Experiments on two language pairs achieve performance close to a teacher-forcing baseline and show that this technique is promising for further exploration.","Florence, Italy","Mihaylova, Tsvetomila  and
Martins, Andr{\'e} F. T.",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/P19-2049,July,351--356,Association for Computational Linguistics,Scheduled Sampling for Transformers,https://aclanthology.org/P19-2049,2019,,,,,
725,inproceedings,dubossarsky-etal-2019-time,"State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.","Florence, Italy","Dubossarsky, Haim  and
Hengchen, Simon  and
Tahmasebi, Nina  and
Schlechtweg, Dominik",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1044,July,457--470,Association for Computational Linguistics,Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change,https://aclanthology.org/P19-1044,2019,,,,,
726,inproceedings,wu-etal-2019-errudite,"Though error analysis is crucial to understanding and improving NLP models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents Errudite, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; Errudite supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; Errudite enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; Errudite supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that Errudite (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.","Florence, Italy","Wu, Tongshuang  and
Ribeiro, Marco Tulio  and
Heer, Jeffrey  and
Weld, Daniel",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1073,July,747--763,Association for Computational Linguistics,"{E}rrudite: Scalable, Reproducible, and Testable Error Analysis",https://aclanthology.org/P19-1073,2019,,,,,
727,inproceedings,belinkov-etal-2019-dont,"Natural Language Inference (NLI) datasets often contain hypothesis-only biases{---}artifacts that allow models to achieve non-trivial performance without learning whether a premise entails a hypothesis. We propose two probabilistic methods to build models that are more robust to such biases and better transfer across datasets. In contrast to standard approaches to NLI, our methods predict the probability of a premise given a hypothesis and NLI label, discouraging models from ignoring the premise. We evaluate our methods on synthetic and existing NLI datasets by training on datasets containing biases and testing on datasets containing no (or different) hypothesis-only biases. Our results indicate that these methods can make NLI models more robust to dataset-specific artifacts, transferring better than a baseline architecture in 9 out of 12 NLI datasets. Additionally, we provide an extensive analysis of the interplay of our methods with known biases in NLI datasets, as well as the effects of encouraging models to ignore biases and fine-tuning on target datasets.","Florence, Italy","Belinkov, Yonatan  and
Poliak, Adam  and
Shieber, Stuart  and
Van Durme, Benjamin  and
Rush, Alexander",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1084,July,877--891,Association for Computational Linguistics,Don{'}t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference,https://aclanthology.org/P19-1084,2019,,,,,
728,inproceedings,alt-etal-2019-fine,"Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of {``}common-sense{''} knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.","Florence, Italy","Alt, Christoph  and
H{\""u}bner, Marc  and
Hennig, Leonhard",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1134,July,1388--1398,Association for Computational Linguistics,Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction,https://aclanthology.org/P19-1134,2019,,,,,
729,inproceedings,brahma-2019-improved,"Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method improves perplexity on the Penn Treebank dataset by up to 1.8 points and by up to 2.3 points on the WikiText-2 dataset, over strong regularized baselines using a single softmax. With a mixture-of-softmax model, we show gains of up to 1.0 perplexity points on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling.","Florence, Italy","Brahma, Siddhartha",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1142,July,1468--1476,Association for Computational Linguistics,Improved Language Modeling by Decoding the Past,https://aclanthology.org/P19-1142,2019,,,,,
730,inproceedings,wu-cotterell-2019-exact,"Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.","Florence, Italy","Wu, Shijie  and
Cotterell, Ryan",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1148,July,1530--1537,Association for Computational Linguistics,Exact Hard Monotonic Attention for Character-Level Transduction,https://aclanthology.org/P19-1148,2019,,,,,
731,inproceedings,sun-etal-2019-mitigating,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.","Florence, Italy","Sun, Tony  and
Gaut, Andrew  and
Tang, Shirlyn  and
Huang, Yuxin  and
ElSherief, Mai  and
Zhao, Jieyu  and
Mirza, Diba  and
Belding, Elizabeth  and
Chang, Kai-Wei  and
Wang, William Yang",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1159,July,1630--1640,Association for Computational Linguistics,Mitigating Gender Bias in Natural Language Processing: Literature Review,https://aclanthology.org/P19-1159,2019,,,,,
732,inproceedings,kaneko-bollegala-2019-gender,"Word embeddings learnt from massive text collections have demonstrated significant levels of discriminative biases such as gender, racial or ethnic biases, which in turn bias the down-stream NLP applications that use those word embeddings. Taking gender-bias as a working example, we propose a debiasing method that preserves non-discriminative gender-related information, while removing stereotypical discriminative gender biases from pre-trained word embeddings. Specifically, we consider four types of information: \textit{feminine}, \textit{masculine}, \textit{gender-neutral} and \textit{stereotypical}, which represent the relationship between gender vs. bias, and propose a debiasing method that (a) preserves the gender-related information in feminine and masculine words, (b) preserves the neutrality in gender-neutral words, and (c) removes the biases from stereotypical words. Experimental results on several previously proposed benchmark datasets show that our proposed method can debias pre-trained word embeddings better than existing SoTA methods proposed for debiasing word embeddings while preserving gender-related but non-discriminative information.","Florence, Italy","Kaneko, Masahiro  and
Bollegala, Danushka",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1160,July,1641--1650,Association for Computational Linguistics,Gender-preserving Debiasing for Pre-trained Word Embeddings,https://aclanthology.org/P19-1160,2019,,,,,
733,inproceedings,sweeney-najafian-2019-transparent,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Florence, Italy","Sweeney, Chris  and
Najafian, Maryam",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1162,July,1662--1667,Association for Computational Linguistics,A Transparent Framework for Evaluating Unintended Demographic Bias in Word Embeddings,https://aclanthology.org/P19-1162,2019,,,,,
734,inproceedings,stanovsky-etal-2019-evaluating,"We present the first challenge set and evaluation protocol for the analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed of English sentences which cast participants into non-stereotypical gender roles (e.g., {``}The doctor asked the nurse to help her in the operation{''}). We devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word {``}doctor{''}). Our analyses show that four popular industrial MT systems and two recent state-of-the-art academic MT models are significantly prone to gender-biased translation errors for all tested target languages. Our data and code are publicly available at https://github.com/gabrielStanovsky/mt{\_}gender.","Florence, Italy","Stanovsky, Gabriel  and
Smith, Noah A.  and
Zettlemoyer, Luke",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1164,July,1679--1684,Association for Computational Linguistics,Evaluating Gender Bias in Machine Translation,https://aclanthology.org/P19-1164,2019,,,,,
735,inproceedings,ethayarajh-etal-2019-understanding,"Word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes. However, methods for measuring and removing such biases remain poorly understood. We show that for any embedding model that implicitly does matrix factorization, debiasing vectors post hoc using subspace projection (Bolukbasi et al., 2016) is, under certain conditions, equivalent to training on an unbiased corpus. We also prove that WEAT, the most common association test for word embeddings, systematically overestimates bias. Given that the subspace projection method is provably effective, we use it to derive a new measure of association called the relational inner product association (RIPA). Experiments with RIPA reveal that, on average, skipgram with negative sampling (SGNS) does not make most words any more gendered than they are in the training corpus. However, for gender-stereotyped words, SGNS actually amplifies the gender association in the corpus.","Florence, Italy","Ethayarajh, Kawin  and
Duvenaud, David  and
Hirst, Graeme",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1166,July,1696--1705,Association for Computational Linguistics,Understanding Undesirable Word Embedding Associations,https://aclanthology.org/P19-1166,2019,,,,,
736,inproceedings,you-etal-2019-improving,"Comprehensive document encoding and salient information selection are two major difficulties for generating summaries with adequate salient information. To tackle the above difficulties, we propose a Transformer-based encoder-decoder framework with two novel extensions for abstractive document summarization. Specifically, (1) to encode the documents comprehensively, we design a focus-attention mechanism and incorporate it into the encoder. This mechanism models a Gaussian focal bias on attention scores to enhance the perception of local context, which contributes to producing salient and informative summaries. (2) To distinguish salient information precisely, we design an independent saliency-selection network which manages the information flow from encoder to decoder. This network effectively reduces the influences of secondary information on the generated summaries. Experimental results on the popular CNN/Daily Mail benchmark demonstrate that our model outperforms other state-of-the-art baselines on the ROUGE metrics.","Florence, Italy","You, Yongjian  and
Jia, Weijia  and
Liu, Tianyi  and
Yang, Wenmian",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1205,July,2132--2141,Association for Computational Linguistics,Improving Abstractive Document Summarization with Salient Information Modeling,https://aclanthology.org/P19-1205,2019,,,,,
737,inproceedings,peng-etal-2019-distantly,"In this work, we explore the way to perform named entity recognition (NER) using only unlabeled data and named entity dictionaries. To this end, we formulate the task as a positive-unlabeled (PU) learning problem and accordingly propose a novel PU learning algorithm to perform the task. We prove that the proposed algorithm can unbiasedly and consistently estimate the task loss as if there is fully labeled data. A key feature of the proposed method is that it does not require the dictionaries to label every entity within a sentence, and it even does not require the dictionaries to label all of the words constituting an entity. This greatly reduces the requirement on the quality of the dictionaries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at \url{https://github.com/v-mipeng/LexiconNER}.","Florence, Italy","Peng, Minlong  and
Xing, Xiaoyu  and
Zhang, Qi  and
Fu, Jinlan  and
Huang, Xuanjing",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1231,July,2409--2419,Association for Computational Linguistics,Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning,https://aclanthology.org/P19-1231,2019,,,,,
738,inproceedings,field-tsvetkov-2019-entity,"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.","Florence, Italy","Field, Anjalie  and
Tsvetkov, Yulia",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1243,July,2550--2560,Association for Computational Linguistics,Entity-Centric Contextual Affective Analysis,https://aclanthology.org/P19-1243,2019,,,,,
739,inproceedings,schulz-etal-2019-analysis,"Many complex discourse-level tasks can aid domain experts in their work but require costly expert annotations for data creation. To speed up and ease annotations, we investigate the viability of automatically generated annotation suggestions for such tasks. As an example, we choose a task that is particularly hard for both humans and machines: the segmentation and classification of epistemic activities in diagnostic reasoning texts. We create and publish a new dataset covering two domains and carefully analyse the suggested annotations. We find that suggestions have positive effects on annotation speed and performance, while not introducing noteworthy biases. Envisioning suggestion models that improve with newly annotated texts, we contrast methods for continuous model adjustment and suggest the most effective setup for suggestions in future expert tasks.","Florence, Italy","Schulz, Claudia  and
Meyer, Christian M.  and
Kiesewetter, Jan  and
Sailer, Michael  and
Bauer, Elisabeth  and
Fischer, Martin R.  and
Fischer, Frank  and
Gurevych, Iryna",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1265,July,2761--2772,Association for Computational Linguistics,Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains,https://aclanthology.org/P19-1265,2019,,,,,
740,inproceedings,kuncoro-etal-2019-scalable,"Prior work has shown that, on small amounts of training data, syntactic neural language models learn structurally sensitive generalisations more successfully than sequential language models. However, their computational complexity renders scaling difficult, and it remains an open question whether structural biases are still necessary when sequential models have access to ever larger amounts of training data. To answer this question, we introduce an efficient knowledge distillation (KD) technique that transfers knowledge from a syntactic language model trained on a small corpus to an LSTM language model, hence enabling the LSTM to develop a more structurally sensitive representation of the larger training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data.","Florence, Italy","Kuncoro, Adhiguna  and
Dyer, Chris  and
Rimell, Laura  and
Clark, Stephen  and
Blunsom, Phil",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1337,July,3472--3484,Association for Computational Linguistics,Scalable Syntax-Aware Language Models Using Knowledge Distillation,https://aclanthology.org/P19-1337,2019,,,,,
741,inproceedings,chen-etal-2019-semantically,"Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.","Florence, Italy","Chen, Wenhu  and
Chen, Jianshu  and
Qin, Pengda  and
Yan, Xifeng  and
Wang, William Yang",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1360,July,3696--3709,Association for Computational Linguistics,Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention,https://aclanthology.org/P19-1360,2019,,,,,
742,inproceedings,zhang-etal-2019-recosa,"In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named ReCoSa, to tackle this problem. Firstly, a word level LSTM encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both Chinese customer services dataset and English Ubuntu dialogue dataset show that ReCoSa significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by ReCoSa are highly coherent with human{'}s understanding, validating the correctness and interpretability of ReCoSa.","Florence, Italy","Zhang, Hainan  and
Lan, Yanyan  and
Pang, Liang  and
Guo, Jiafeng  and
Cheng, Xueqi",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1362,July,3721--3730,Association for Computational Linguistics,{R}e{C}o{S}a: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation,https://aclanthology.org/P19-1362,2019,,,,,
743,inproceedings,blasi-etal-2019-distribution,"Embedding a clause inside another ({``}the girl [who likes cars [that run fast]] has arrived{''}) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.","Florence, Italy","Blasi, Damian  and
Cotterell, Ryan  and
Wolf-Sonkin, Lawrence  and
Stoll, Sabine  and
Bickel, Balthasar  and
Baroni, Marco",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1384,July,3938--3943,Association for Computational Linguistics,On the Distribution of Deep Clausal Embeddings: A Large Cross-linguistic Study,https://aclanthology.org/P19-1384,2019,,,,,
744,inproceedings,chalkidis-etal-2019-neural,"Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case{'}s facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT{'}s length limitation.","Florence, Italy","Chalkidis, Ilias  and
Androutsopoulos, Ion  and
Aletras, Nikolaos",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1424,July,4317--4323,Association for Computational Linguistics,Neural Legal Judgment Prediction in {E}nglish,https://aclanthology.org/P19-1424,2019,,,,,
745,inproceedings,zhang-etal-2019-selection,"Natural Language Sentence Matching (NLSM) has gained substantial attention from both academics and the industry, and rich public datasets contribute a lot to this process. However, biased datasets can also hurt the generalization performance of trained models and give untrustworthy evaluation results. For many NLSM datasets, the providers select some pairs of sentences into the datasets, and this sampling procedure can easily bring unintended pattern, i.e., selection bias. One example is the QuoraQP dataset, where some content-independent naive features are unreasonably predictive. Such features are the reflection of the selection bias and termed as the {``}leakage features.{''} In this paper, we investigate the problem of selection bias on six NLSM datasets and find that four out of them are significantly biased. We further propose a training and evaluation framework to alleviate the bias. Experimental results on QuoraQP suggest that the proposed framework can improve the generalization ability of trained models, and give more trustworthy evaluation results for real-world adoptions.","Florence, Italy","Zhang, Guanhua  and
Bai, Bing  and
Liang, Jian  and
Bai, Kun  and
Chang, Shiyu  and
Yu, Mo  and
Zhu, Conghui  and
Zhao, Tiejun",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1435,July,4418--4429,Association for Computational Linguistics,Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets,https://aclanthology.org/P19-1435,2019,,,,,
746,inproceedings,mielke-etal-2019-kind,"How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that {``}translationese{''} is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.","Florence, Italy","Mielke, Sabrina J.  and
Cotterell, Ryan  and
Gorman, Kyle  and
Roark, Brian  and
Eisner, Jason",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1491,July,4975--4989,Association for Computational Linguistics,What Kind of Language Is Hard to Language-Model?,https://aclanthology.org/P19-1491,2019,,,,,
747,inproceedings,chaabouni-etal-2019-word,"Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to {``}natural{''} word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of {``}effort{''} into neural networks, as a possible way to make their linguistic behavior more human-like.","Florence, Italy","Chaabouni, Rahma  and
Kharitonov, Eugene  and
Lazaric, Alessandro  and
Dupoux, Emmanuel  and
Baroni, Marco",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1509,July,5166--5175,Association for Computational Linguistics,Word-order Biases in Deep-agent Emergent Communication,https://aclanthology.org/P19-1509,2019,,,,,
748,inproceedings,liu-avci-2019-incorporating,"Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings.","Florence, Italy","Liu, Frederick  and
Avci, Besim",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1631,July,6274--6283,Association for Computational Linguistics,Incorporating Priors with Feature Attribution on Text Classification,https://aclanthology.org/P19-1631,2019,,,,,
749,inproceedings,bevendorff-etal-2019-bias,"The PAN series of shared tasks is well known for its continuous and high quality research in the field of digital text forensics. Among others, PAN contributions include original corpora, tailored benchmarks, and standardized experimentation platforms. In this paper we review, theoretically and practically, the authorship verification task and conclude that the underlying experiment design cannot guarantee pushing forward the state of the art{---}in fact, it allows for top benchmarking with a surprisingly straightforward approach. In this regard, we present a {``}Basic and Fairly Flawed{''} (BAFF) authorship verifier that is on a par with the best approaches submitted so far, and that illustrates sources of bias that should be eliminated. We pinpoint these sources in the evaluation chain and present a refined authorship corpus as effective countermeasure.","Florence, Italy","Bevendorff, Janek  and
Hagen, Matthias  and
Stein, Benno  and
Potthast, Martin",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1634,July,6301--6306,Association for Computational Linguistics,Bias Analysis and Mitigation in the Evaluation of Authorship Verification,https://aclanthology.org/P19-1634,2019,,,,,
750,inproceedings,chrupala-2019-symbolic,"A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this.","Florence, Italy","Chrupa{\l}a, Grzegorz",Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,10.18653/v1/P19-1647,July,6452--6462,Association for Computational Linguistics,Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language,https://aclanthology.org/P19-1647,2019,,,,,
751,inproceedings,nematzadeh-etal-2019-language,"The goal of this tutorial is to bring the fields of computational linguistics and computational cognitive science closer: we will introduce different stages of language acquisition and their parallel problems in NLP. As an example, one of the early challenges children face is mapping the meaning of word labels (such as {``}cat{''}) to their referents (the furry animal in the living room). Word learning is similar to the word alignment problem in machine translation. We explain the current computational models of language acquisition, their limitations, and how the insights from these models can be incorporated into NLP applications. Moreover, we discuss how we can take advantage of the cognitive science of language in computational linguistics: for example, by designing cognitively-motivated evaluations task or buildings language-learning inductive biases into our models.","Minneapolis, Minnesota","Nematzadeh, Aida  and
Futrell, Richard  and
Levy, Roger",Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials,10.18653/v1/N19-5005,June,19--21,Association for Computational Linguistics,Language Learning and Processing in People and Machines,https://aclanthology.org/N19-5005,2019,,,,,
752,inproceedings,ott-etal-2019-fairseq,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","Minneapolis, Minnesota","Ott, Myle  and
Edunov, Sergey  and
Baevski, Alexei  and
Fan, Angela  and
Gross, Sam  and
Ng, Nathan  and
Grangier, David  and
Auli, Michael",Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),10.18653/v1/N19-4009,June,48--53,Association for Computational Linguistics,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",https://aclanthology.org/N19-4009,2019,,,,,
753,inproceedings,bordia-bowman-2019-identifying,"Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora{---}Penn Treebank, WikiText-2, and CNN/Daily Mail{---}resulting in similar conclusions.","Minneapolis, Minnesota","Bordia, Shikha  and
Bowman, Samuel R.",Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/N19-3002,June,7--15,Association for Computational Linguistics,Identifying and Reducing Gender Bias in Word-Level Language Models,https://aclanthology.org/N19-3002,2019,,,,,
754,inproceedings,asaadi-etal-2019-big,"Bigrams (two-word sequences) hold a special place in semantic composition research since they are the smallest unit formed by composing words. A semantic relatedness dataset that includes bigrams will thus be useful in the development of automatic methods of semantic composition. However, existing relatedness datasets only include pairs of unigrams (single words). Further, existing datasets were created using rating scales and thus suffer from limitations such as in consistent annotations and scale region bias. In this paper, we describe how we created a large, fine-grained, bigram relatedness dataset (BiRD), using a comparative annotation technique called Best{--}Worst Scaling. Each of BiRD{'}s 3,345 English term pairs involves at least one bigram. We show that the relatedness scores obtained are highly reliable (split-half reliability r= 0.937). We analyze the data to obtain insights into bigram semantic relatedness. Finally, we present benchmark experiments on using the relatedness dataset as a testbed to evaluate simple unsupervised measures of semantic composition. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.","Minneapolis, Minnesota","Asaadi, Shima  and
Mohammad, Saif  and
Kiritchenko, Svetlana","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1050,June,505--516,Association for Computational Linguistics,"Big {B}i{RD}: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition",https://aclanthology.org/N19-1050,2019,,,,,
755,inproceedings,chen-etal-2019-seeing,"One key consequence of the information revolution is a significant increase and a contamination of our information supply. The practice of fact checking won{'}t suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as {``}animals should have lawful rights{''}, and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a natural language understanding task, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset. We use crowd-sourcing to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding challenges, and show that human baselines across multiple subtasks far outperform ma-chine baselines built upon state-of-the-art NLP techniques. This poses a challenge and opportunity for the NLP community to address.","Minneapolis, Minnesota","Chen, Sihao  and
Khashabi, Daniel  and
Yin, Wenpeng  and
Callison-Burch, Chris  and
Roth, Dan","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1053,June,542--557,Association for Computational Linguistics,Seeing Things from a Different Angle:Discovering Diverse Perspectives about Claims,https://aclanthology.org/N19-1053,2019,,,,,
756,inproceedings,wiegand-etal-2019-detection,We discuss the impact of data bias on abusive language detection. We show that classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion.,"Minneapolis, Minnesota","Wiegand, Michael  and
Ruppenhofer, Josef  and
Kleinbauer, Thomas","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1060,June,602--608,Association for Computational Linguistics,{D}etection of {A}busive {L}anguage: the {P}roblem of {B}iased {D}atasets,https://aclanthology.org/N19-1060,2019,,,,,
757,inproceedings,gonen-goldberg-2019-lipstick,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between {``}gender-neutralized{''} words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Minneapolis, Minnesota","Gonen, Hila  and
Goldberg, Yoav","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1061,June,609--614,Association for Computational Linguistics,Lipstick on a Pig: {D}ebiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them,https://aclanthology.org/N19-1061,2019,,,,,
758,inproceedings,manzini-etal-2019-black,"Online texts - across genres, registers, domains, and styles - are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate and amplify these stereotypes, and propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method to debias word embeddings in multiclass settings such as race and religion, extending the work of (Bolukbasi et al., 2016) from the binary setting, such as binary gender. Next, we propose a novel methodology for the evaluation of multiclass debiasing. We demonstrate that our multiclass debiasing is robust and maintains the efficacy in standard NLP tasks.","Minneapolis, Minnesota","Manzini, Thomas  and
Yao Chong, Lim  and
Black, Alan W  and
Tsvetkov, Yulia","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1062,June,615--621,Association for Computational Linguistics,Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings,https://aclanthology.org/N19-1062,2019,,,,,
759,inproceedings,may-etal-2019-measuring,"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test{'}s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.","Minneapolis, Minnesota","May, Chandler  and
Wang, Alex  and
Bordia, Shikha  and
Bowman, Samuel R.  and
Rudinger, Rachel","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1063,June,622--628,Association for Computational Linguistics,On Measuring Social Biases in Sentence Encoders,https://aclanthology.org/N19-1063,2019,,,,,
760,inproceedings,zhao-etal-2019-gender,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo{'}s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Minneapolis, Minnesota","Zhao, Jieyu  and
Wang, Tianlu  and
Yatskar, Mark  and
Cotterell, Ryan  and
Ordonez, Vicente  and
Chang, Kai-Wei","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1064,June,629--634,Association for Computational Linguistics,Gender Bias in Contextualized Word Embeddings,https://aclanthology.org/N19-1064,2019,,,,,
761,inproceedings,zhang-etal-2019-openki,"In this paper, we consider advancing web-scale knowledge extraction and alignment by integrating OpenIE extractions in the form of (subject, predicate, object) triples with Knowledge Bases (KB). Traditional techniques from universal schema and from schema mapping fall in two extremes: either they perform instance-level inference relying on embedding for (subject, object) pairs, thus cannot handle pairs absent in any existing triples; or they perform predicate-level mapping and completely ignore background evidence from individual entities, thus cannot achieve satisfying quality. We propose \textit{OpenKI} to handle sparsity of OpenIE extractions by performing instance-level inference: for each entity, we encode the rich information in its neighborhood in both KB and OpenIE extractions, and leverage this information in relation inference by exploring different methods of aggregation and attention. In order to handle unseen entities, our model is designed without creating entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional OpenIE extractions like ReVerb, but also boosts the performance on OpenIE from semi-structured data, where new entity pairs are abundant and data are fairly sparse.","Minneapolis, Minnesota","Zhang, Dongxu  and
Mukherjee, Subhabrata  and
Lockard, Colin  and
Dong, Luna  and
McCallum, Andrew","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1083,June,762--772,Association for Computational Linguistics,{O}pen{KI}: {I}ntegrating {O}pen {I}nformation {E}xtraction and {K}nowledge {B}ases with {R}elation {I}nference,https://aclanthology.org/N19-1083,2019,,,,,
762,inproceedings,xiong-etal-2019-imposing,"Existing entity typing systems usually exploit the type hierarchy provided by knowledge base (KB) schema to model label correlations and thus improve the overall performance. Such techniques, however, are not directly applicable to more open and practical scenarios where the type set is not restricted by KB schema and includes a vast number of free-form types. To model the underlying label correlations without access to manually annotated label structures, we introduce a novel label-relational inductive bias, represented by a graph propagation layer that effectively encodes both global label co-occurrence statistics and word-level similarities. On a large dataset with over 10,000 free-form types, the graph-enhanced model equipped with an attention-based matching module is able to achieve a much higher recall score while maintaining a high-level precision. Specifically, it achieves a 15.3{\%} relative F1 improvement and also less inconsistency in the outputs. We further show that a simple modification of our proposed graph layer can also improve the performance on a conventional and widely-tested dataset that only includes KB-schema types.","Minneapolis, Minnesota","Xiong, Wenhan  and
Wu, Jiawei  and
Lei, Deren  and
Yu, Mo  and
Chang, Shiyu  and
Guo, Xiaoxiao  and
Wang, William Yang","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1084,June,773--784,Association for Computational Linguistics,Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing,https://aclanthology.org/N19-1084,2019,,,,,
763,inproceedings,wang-etal-2019-adversarial-training,"Modern weakly supervised methods for event detection (ED) avoid time-consuming human annotation and achieve promising results by learning from auto-labeled data. However, these methods typically rely on sophisticated pre-defined rules as well as existing instances in knowledge bases for automatic annotation and thus suffer from low coverage, topic bias, and data noise. To address these issues, we build a large event-related candidate set with good coverage and then apply an adversarial training mechanism to iteratively identify those informative instances from the candidate set and filter out those noisy ones. The experiments on two real-world datasets show that our candidate selection and adversarial training can cooperate together to obtain more diverse and accurate training data for ED, and significantly outperform the state-of-the-art methods in various weakly supervised scenarios. The datasets and source code can be obtained from https://github.com/thunlp/Adv-ED.","Minneapolis, Minnesota","Wang, Xiaozhi  and
Han, Xu  and
Liu, Zhiyuan  and
Sun, Maosong  and
Li, Peng","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1105,June,998--1008,Association for Computational Linguistics,Adversarial Training for Weakly Supervised Event Detection,https://aclanthology.org/N19-1105,2019,,,,,
764,inproceedings,gao-etal-2019-rebuttal,"Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP. However, only few studies have evaluated its properties empirically. Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018. We quantitatively and qualitatively assess the corpus. This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses. We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a reviewer{'}s final score is largely determined by her initial score and the distance to the other reviewers{'} initial scores. In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research. We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences.","Minneapolis, Minnesota","Gao, Yang  and
Eger, Steffen  and
Kuznetsov, Ilia  and
Gurevych, Iryna  and
Miyao, Yusuke","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1129,June,1274--1290,Association for Computational Linguistics,Does My Rebuttal Matter? Insights from a Major {NLP} Conference,https://aclanthology.org/N19-1129,2019,,,,,
765,inproceedings,goyal-etal-2019-empirical,"Globally normalized neural sequence models are considered superior to their locally normalized equivalents because they may ameliorate the effects of label bias. However, when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing. Thus, the practical advantage of global normalization in the context of modern neural methods remains unclear. In this paper, we attempt to shed light on this problem through an empirical study. We extend an approach for search-aware training via a continuous relaxation of beam search (Goyal et al., 2017b) in order to enable training of globally normalized recurrent sequence models through simple backpropagation. We then use this technique to conduct an empirical study of the interaction between global normalization, high-capacity encoders, and search-aware optimization. We observe that in the context of inexact search, globally normalized neural models are still more effective than their locally normalized counterparts. Further, since our training approach is sensitive to warm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pre-training globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training.","Minneapolis, Minnesota","Goyal, Kartik  and
Dyer, Chris  and
Berg-Kirkpatrick, Taylor","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1171,June,1724--1733,Association for Computational Linguistics,{A}n Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search,https://aclanthology.org/N19-1171,2019,,,,,
766,inproceedings,thomason-etal-2019-shifting,"We demonstrate the surprising strength of unimodal baselines in multimodal domains, and make concrete recommendations for best practices in future research. Where existing work often compares against random or majority class baselines, we argue that unimodal approaches better capture and reflect dataset biases and therefore provide an important comparison when assessing the performance of multimodal techniques. We present unimodal ablations on three recent datasets in visual navigation and QA, seeing an up to 29{\%} absolute gain in performance over published baselines.","Minneapolis, Minnesota","Thomason, Jesse  and
Gordon, Daniel  and
Bisk, Yonatan","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1197,June,1977--1983,Association for Computational Linguistics,Shifting the Baseline: Single Modality Performance on Visual Navigation {\&} {QA},https://aclanthology.org/N19-1197,2019,,,,,
767,inproceedings,xu-etal-2019-differentiable,"Despite some empirical success at correcting exposure bias in machine translation, scheduled sampling algorithms suffer from a major drawback: they incorrectly assume that words in the reference translations and in sampled sequences are aligned at each time step. Our new differentiable sampling algorithm addresses this issue by optimizing the probability that the reference can be aligned with the sampled output, based on a soft alignment predicted by the model itself. As a result, the output distribution at each time step is evaluated with respect to the whole predicted sequence. Experiments on IWSLT translation tasks show that our approach improves BLEU compared to maximum likelihood and scheduled sampling baselines. In addition, our approach is simpler to train with no need for sampling schedule and yields models that achieve larger improvements with smaller beam sizes.","Minneapolis, Minnesota","Xu, Weijia  and
Niu, Xing  and
Carpuat, Marine","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1207,June,2047--2053,Association for Computational Linguistics,Differentiable Sampling with Flexible Reference Word Order for Neural Machine Translation,https://aclanthology.org/N19-1207,2019,,,,,
768,inproceedings,oba-etal-2019-modeling,"There exist biases in individual{'}s language use; the same word (e.g., cool) is used for expressing different meanings (e.g., temperature range) or different words (e.g., cloudy, hazy) are used for describing the same meaning. In this study, we propose a method of modeling such personal biases in word meanings (hereafter, semantic variations) with personalized word embeddings obtained by solving a task on subjective text while regarding words used by different individuals as different words. To prevent personalized word embeddings from being contaminated by other irrelevant biases, we solve a task of identifying a review-target (objective output) from a given review. To stabilize the training of this extreme multi-class classification, we perform a multi-task learning with metadata identification. Experimental results with reviews retrieved from RateBeer confirmed that the obtained personalized word embeddings improved the accuracy of sentiment analysis as well as the target task. Analysis of the obtained personalized word embeddings revealed trends in semantic variations related to frequent and adjective words.","Minneapolis, Minnesota","Oba, Daisuke  and
Yoshinaga, Naoki  and
Sato, Shoetsu  and
Akasaki, Satoshi  and
Toyoda, Masashi","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1215,June,2102--2108,Association for Computational Linguistics,Modeling Personal Biases in Language Use by Inducing Personalized Word Embeddings,https://aclanthology.org/N19-1215,2019,,,,,
769,inproceedings,baly-etal-2019-multi,"In the context of fake news, bias, and propaganda, we study two important but relatively under-explored problems: (i) trustworthiness estimation (on a 3-point scale) and (ii) political ideology detection (left/right bias on a 7-point scale) of entire news outlets, as opposed to evaluating individual articles. In particular, we propose a multi-task ordinal regression framework that models the two problems jointly. This is motivated by the observation that hyper-partisanship is often linked to low trustworthiness, e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and trustworthy. We further use several auxiliary tasks, modeling centrality, hyper-partisanship, as well as left-vs.-right bias on a coarse-grained scale. The evaluation results show sizable performance gains by the joint models over models that target the problems in isolation.","Minneapolis, Minnesota","Baly, Ramy  and
Karadzhov, Georgi  and
Saleh, Abdelrhman  and
Glass, James  and
Nakov, Preslav","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1216,June,2109--2116,Association for Computational Linguistics,Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness and the Leading Political Ideology of News Media,https://aclanthology.org/N19-1216,2019,,,,,
770,inproceedings,tevet-etal-2019-evaluating,"Generative Adversarial Networks (GANs) are a promising approach for text generation that, unlike traditional language models (LM), does not suffer from the problem of {``}exposure bias{''}. However, A major hurdle for understanding the potential of GANs for text generation is the lack of a clear evaluation metric. In this work, we propose to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than state-of-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation.","Minneapolis, Minnesota","Tevet, Guy  and
Habib, Gavriel  and
Shwartz, Vered  and
Berant, Jonathan","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1233,June,2241--2247,Association for Computational Linguistics,Evaluating Text {GAN}s as Language Models,https://aclanthology.org/N19-1233,2019,,,,,
771,inproceedings,hosking-riedel-2019-evaluating,"Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.","Minneapolis, Minnesota","Hosking, Tom  and
Riedel, Sebastian","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1237,June,2278--2283,Association for Computational Linguistics,Evaluating Rewards for Question Generation Models,https://aclanthology.org/N19-1237,2019,,,,,
772,inproceedings,du-etal-2019-consistent,"Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.","Minneapolis, Minnesota","Du, Xinya  and
Dalvi, Bhavana  and
Tandon, Niket  and
Bosselut, Antoine  and
Yih, Wen-tau  and
Clark, Peter  and
Cardie, Claire","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1244,June,2347--2356,Association for Computational Linguistics,Be Consistent! Improving Procedural Text Comprehension using Label Consistency,https://aclanthology.org/N19-1244,2019,,,,,
773,inproceedings,kim-etal-2019-abstractive,"We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually located at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the state-of-the-art summarization models.","Minneapolis, Minnesota","Kim, Byeongchang  and
Kim, Hyunwoo  and
Kim, Gunhee","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1260,June,2519--2531,Association for Computational Linguistics,Abstractive Summarization of {R}eddit Posts with Multi-level Memory Networks,https://aclanthology.org/N19-1260,2019,,,,,
774,inproceedings,zhang-etal-2019-posterior,"This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.","Minneapolis, Minnesota","Zhang, Qi  and
Tang, Siliang  and
Ren, Xiang  and
Wu, Fei  and
Pu, Shiliang  and
Zhuang, Yueting","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1290,June,2831--2835,Association for Computational Linguistics,Posterior-regularized {REINFORCE} for Instance Selection in Distant Supervision,https://aclanthology.org/N19-1290,2019,,,,,
775,inproceedings,chen-etal-2019-improving,"Recently, distant supervision has gained great success on Fine-grained Entity Typing (FET). Despite its efficiency in reducing manual labeling efforts, it also brings the challenge of dealing with false entity type labels, as distant supervision assigns labels in a context-agnostic manner. Existing works alleviated this issue with partial-label loss, but usually suffer from confirmation bias, which means the classifier fit a pseudo data distribution given by itself. In this work, we propose to regularize distantly supervised models with Compact Latent Space Clustering (CLSC) to bypass this problem and effectively utilize noisy data yet. Our proposed method first dynamically constructs a similarity graph of different entity mentions; infer the labels of noisy instances via label propagation. Based on the inferred labels, mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space, thus leading to better classification performance. Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state-of-the-art distantly supervised entity typing systems by a significant margin.","Minneapolis, Minnesota","Chen, Bo  and
Gu, Xiaotao  and
Hu, Yufeng  and
Tang, Siliang  and
Hu, Guoping  and
Zhuang, Yueting  and
Ren, Xiang","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1294,June,2862--2872,Association for Computational Linguistics,Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering,https://aclanthology.org/N19-1294,2019,,,,,
776,inproceedings,ananya-etal-2019-genderquant,"Language is gendered if the context surrounding a mention is suggestive of a particular binary gender for that mention. Detecting the different ways in which language is gendered is an important task since gendered language can bias NLP models (such as for coreference resolution). This task is challenging since genderedness is often expressed in subtle ways. Existing approaches need considerable annotation efforts for each language, domain, and author, and often require handcrafted lexicons and features. Additionally, these approaches do not provide a quantifiable measure of how gendered the text is, nor are they applicable at the fine-grained mention level. In this paper, we use existing NLP pipelines to automatically annotate gender of mentions in the text. On corpora labeled using this method, we train a supervised classifier to predict the gender of any mention from its context and evaluate it on unseen text. The model confidence for a mention{'}s gender can be used as a proxy to indicate the level of genderedness of the context. We test this gendered language detector on movie summaries, movie reviews, news articles, and fiction novels, achieving an AUC-ROC of up to 0.71, and observe that the model predictions agree with human judgments collected for this task. We also provide examples of detected gendered sentences from aforementioned domains.","Minneapolis, Minnesota","{Ananya}  and
Parthasarthi, Nitya  and
Singh, Sameer","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1303,June,2959--2969,Association for Computational Linguistics,{G}ender{Q}uant: Quantifying Mention-Level Genderedness,https://aclanthology.org/N19-1303,2019,,,,,
777,inproceedings,guo-etal-2019-autosem,"Multi-task learning (MTL) has achieved success over a wide range of problems, where the goal is to improve the performance of a primary task using a set of relevant auxiliary tasks. However, when the usefulness of the auxiliary tasks w.r.t. the primary task is not known a priori, the success of MTL models depends on the correct choice of these auxiliary tasks and also a balanced mixing ratio of these tasks during alternate training. These two problems could be resolved via manual intuition or hyper-parameter tuning over all combinatorial task choices, but this introduces inductive bias or is not scalable when the number of candidate auxiliary tasks is very large. To address these issues, we present AutoSeM, a two-stage MTL pipeline, where the first stage automatically selects the most useful auxiliary tasks via a Beta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage learns the training mixing ratio of these selected auxiliary tasks via a Gaussian Process based Bayesian optimization framework. We conduct several MTL experiments on the GLUE language understanding tasks, and show that our AutoSeM framework can successfully find relevant auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on several primary tasks. Finally, we present ablations for each stage of AutoSeM and analyze the learned auxiliary task choices.","Minneapolis, Minnesota","Guo, Han  and
Pasunuru, Ramakanth  and
Bansal, Mohit","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1355,June,3520--3531,Association for Computational Linguistics,{A}uto{S}e{M}: Automatic Task Selection and Mixing in Multi-Task Learning,https://aclanthology.org/N19-1355,2019,,,,,
778,inproceedings,ravfogel-etal-2019-studying,"How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs{'} syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order.","Minneapolis, Minnesota","Ravfogel, Shauli  and
Goldberg, Yoav  and
Linzen, Tal","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1356,June,3532--3542,Association for Computational Linguistics,Studying the Inductive Biases of {RNN}s with Synthetic Variations of Natural Languages,https://aclanthology.org/N19-1356,2019,,,,,
779,inproceedings,aina-etal-2019-entity,"Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed.","Minneapolis, Minnesota","Aina, Laura  and
Silberer, Carina  and
Sorodoc, Ionut-Teodor  and
Westera, Matthijs  and
Boleda, Gemma","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1378,June,3772--3783,Association for Computational Linguistics,What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue,https://aclanthology.org/N19-1378,2019,,,,,
780,inproceedings,dziri-etal-2019-evaluating,"Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses.","Minneapolis, Minnesota","Dziri, Nouha  and
Kamalloo, Ehsan  and
Mathewson, Kory  and
Zaiane, Osmar","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1381,June,3806--3812,Association for Computational Linguistics,Evaluating Coherence in Dialogue Systems using Entailment,https://aclanthology.org/N19-1381,2019,,,,,
781,inproceedings,romanov-etal-2019-whats,"There is a growing body of work that proposes methods for mitigating bias in machine learning systems. These methods typically rely on access to protected attributes such as race, gender, or age. However, this raises two significant challenges: (1) protected attributes may not be available or it may not be legal to use them, and (2) it is often desirable to simultaneously consider multiple protected attributes, as well as their intersections. In the context of mitigating bias in occupation classification, we propose a method for discouraging correlation between the predicted probability of an individual{'}s true occupation and a word embedding of their name. This method leverages the societal biases that are encoded in word embeddings, eliminating the need for access to protected attributes. Crucially, it only requires access to individuals{'} names at training time and not at deployment time. We evaluate two variations of our proposed method using a large-scale dataset of online biographies. We find that both variations simultaneously reduce race and gender biases, with almost no reduction in the classifier{'}s overall true positive rate.","Minneapolis, Minnesota","Romanov, Alexey  and
De-Arteaga, Maria  and
Wallach, Hanna  and
Chayes, Jennifer  and
Borgs, Christian  and
Chouldechova, Alexandra  and
Geyik, Sahin  and
Kenthapadi, Krishnaram  and
Rumshisky, Anna  and
Kalai, Adam","Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",10.18653/v1/N19-1424,June,4187--4195,Association for Computational Linguistics,What{'}s in a Name? {R}educing Bias in Bios without Access to Protected Attributes,https://aclanthology.org/N19-1424,2019,,,,,
782,inproceedings,jumelet-etal-2019-analysing,"Extensive research has recently shown that recurrent neural language models are able to process a wide range of grammatical phenomena. How these models are able to perform these remarkable feats so well, however, is still an open question. To gain more insight into what information LSTMs base their decisions on, we propose a generalisation of Contextual Decomposition (GCD). In particular, this setup enables us to accurately distil which part of a prediction stems from semantic heuristics, which part truly emanates from syntactic cues and which part arise from the model biases themselves instead. We investigate this technique on tasks pertaining to syntactic agreement and co-reference resolution and discover that the model strongly relies on a default reasoning effect to perform these tasks.","Hong Kong, China","Jumelet, Jaap  and
Zuidema, Willem  and
Hupkes, Dieuwke",Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),10.18653/v1/K19-1001,November,1--11,Association for Computational Linguistics,Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment,https://aclanthology.org/K19-1001,2019,,,,,
783,inproceedings,chen-etal-2019-federated,"We propose algorithms to train production-quality n-gram language models using federated learning. Federated learning is a distributed computation platform that can be used to train global models for portable devices such as smart phones. Federated learning is especially relevant for applications handling privacy-sensitive data, such as virtual keyboards, because training is performed without the users{'} data ever leaving their devices. While the principles of federated learning are fairly generic, its methodology assumes that the underlying models are neural networks. However, virtual keyboards are typically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference. Our technical contributions include ways of handling large vocabularies, algorithms to correct capitalization errors in user data, and efficient finite state transducer algorithms to convert word language models to word-piece language models and vice versa. The n-gram language models trained with federated learning are compared to n-grams trained with traditional server-based algorithms using A/B tests on tens of millions of users of a virtual keyboard. Results are presented for two languages, American English and Brazilian Portuguese. This work demonstrates that high-quality n-gram language models can be trained directly on client mobile devices without sensitive training data ever leaving the devices.","Hong Kong, China","Chen, Mingqing  and
Suresh, Ananda Theertha  and
Mathews, Rajiv  and
Wong, Adeline  and
Allauzen, Cyril  and
Beaufays, Fran{\c{c}}oise  and
Riley, Michael",Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),10.18653/v1/K19-1012,November,121--130,Association for Computational Linguistics,Federated Learning of N-Gram Language Models,https://aclanthology.org/K19-1012,2019,,,,,
784,inproceedings,choshen-abend-2019-automatically,"We show that the state-of-the-art Transformer MT model is not biased towards monotonic reordering (unlike previous recurrent neural network models), but that nevertheless, long-distance dependencies remain a challenge for the model. Since most dependencies are short-distance, common evaluation metrics will be little influenced by how well systems perform on them. We therefore propose an automatic approach for extracting challenge sets rich with long-distance dependencies, and argue that evaluation using this methodology provides a complementary perspective on system performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.","Hong Kong, China","Choshen, Leshem  and
Abend, Omri",Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),10.18653/v1/K19-1028,November,291--303,Association for Computational Linguistics,Automatically Extracting Challenge Sets for Non-Local Phenomena in Neural Machine Translation,https://aclanthology.org/K19-1028,2019,,,,,
785,inproceedings,amac-etal-2019-procedural,"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.","Hong Kong, China","Amac, Mustafa Sercan  and
Yagcioglu, Semih  and
Erdem, Aykut  and
Erdem, Erkut",Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),10.18653/v1/K19-1041,November,441--451,Association for Computational Linguistics,Procedural Reasoning Networks for Understanding Multimodal Procedures,https://aclanthology.org/K19-1041,2019,,,,,
786,inproceedings,gonen-etal-2019-grammatical,"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun{'}s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While {``}embedding debiasing{''} methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words{'} context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.","Hong Kong, China","Gonen, Hila  and
Kementchedjhieva, Yova  and
Goldberg, Yoav",Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),10.18653/v1/K19-1043,November,463--471,Association for Computational Linguistics,How Does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?,https://aclanthology.org/K19-1043,2019,,,,,
787,inproceedings,sergeeva-etal-2019-neural,"Since the introduction of context-aware token representation techniques such as Embeddings from Language Models (ELMo) and Bidirectional Encoder Representations from Transformers (BERT), there has been numerous reports on improved performance on a variety of natural language tasks. Nevertheless, the degree to which the resulting context-aware representations encode information about morpho-syntactic properties of the word/token in a sentence remains unclear. In this paper, we investigate the application and impact of state-of-the-art neural token representations for automatic cue-conditional speculation and negation scope detection coupled with the independently computed morpho-syntactic information. Through this work, We establish a new state-of-the-art for the BioScope and NegPar corpus. More importantly, we provide a thorough analysis of neural representations and additional features interactions, cue-representation for conditioning, discuss model behavior on different datasets and address the annotation-induced biases in the learned representations.",Hong Kong,"Sergeeva, Elena  and
Zhu, Henghui  and
Tahmasebi, Amir  and
Szolovits, Peter",Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),10.18653/v1/D19-6221,November,178--187,Association for Computational Linguistics,Neural Token Representations and Negation and Speculation Scope Detection in Biomedical and General Domain Text,https://aclanthology.org/D19-6221,2019,,,,,
788,inproceedings,singh-etal-2019-bert,"Multilingual transfer learning can benefit both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic con- tent while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and word-level tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github.com/salesforce/xnli{\_}extension to assist further research into multilingual representations.","Hong Kong, China","Singh, Jasdeep  and
McCann, Bryan  and
Socher, Richard  and
Xiong, Caiming",Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),10.18653/v1/D19-6106,November,47--55,Association for Computational Linguistics,{BERT} is Not an Interlingua and the Bias of Tokenization,https://aclanthology.org/D19-6106,2019,,,,,
789,inproceedings,zeng-etal-2019-empirical,"Active learning (AL) for machine translation (MT) has been well-studied for the phrase-based MT paradigm. Several AL algorithms for data sampling have been proposed over the years. However, given the rapid advancement in neural methods, these algorithms have not been thoroughly investigated in the context of neural MT (NMT). In this work, we address this missing aspect by conducting a systematic comparison of different AL methods in a simulated AL framework. Our experimental setup to compare different AL methods uses: i) State-of-the-art NMT architecture to achieve realistic results; and ii) the same dataset (WMT{'}13 English-Spanish) to have fair comparison across different methods. We then demonstrate how recent advancements in unsupervised pre-training and paraphrastic embedding can be used to improve existing AL methods. Finally, we propose a neural extension for an AL sampling method used in the context of phrase-based MT - Round Trip Translation Likelihood (RTTL). RTTL uses a bidirectional translation model to estimate the loss of information during translation and outperforms previous methods.","Hong Kong, China","Zeng, Xiangkai  and
Garg, Sarthak  and
Chatterjee, Rajen  and
Nallasamy, Udhyakumar  and
Paulik, Matthias",Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),10.18653/v1/D19-6110,November,84--93,Association for Computational Linguistics,Empirical Evaluation of Active Learning Techniques for Neural {MT},https://aclanthology.org/D19-6110,2019,,,,,
790,inproceedings,he-etal-2019-unlearn,"Statistical natural language inference (NLI) models are susceptible to learning dataset bias: superficial cues that happen to associate with the label on a particular dataset, but are not useful in general, e.g., negation words indicate contradiction. As exposed by several recent challenge datasets, these models perform poorly when such association is absent, e.g., predicting that {``}I love dogs.{''} contradicts {``}I don{'}t love cats.{''}. Our goal is to design learning algorithms that guard against known dataset bias. We formalize the concept of dataset bias under the framework of distribution shift and present a simple debiasing algorithm based on residual fitting, which we call DRiFt. We first learn a biased model that only uses features that are known to relate to dataset bias. Then, we train a debiased model that fits to the residual of the biased model, focusing on examples that cannot be predicted well by biased features only. We use DRiFt to train three high-performing NLI models on two benchmark datasets, SNLI and MNLI. Our debiased models achieve significant gains over baseline models on two challenge test sets, while maintaining reasonable performance on the original test sets.","Hong Kong, China","He, He  and
Zha, Sheng  and
Wang, Haohan",Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),10.18653/v1/D19-6115,November,132--142,Association for Computational Linguistics,Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual,https://aclanthology.org/D19-6115,2019,,,,,
791,inproceedings,desai-etal-2019-evaluating,"The Lottery Ticket Hypothesis suggests large, over-parameterized neural networks consist of small, sparse subnetworks that can be trained in isolation to reach a similar (or better) test accuracy. However, the initialization and generalizability of the obtained sparse subnetworks have been recently called into question. Our work focuses on evaluating the initialization of sparse subnetworks under distributional shifts. Specifically, we investigate the extent to which a sparse subnetwork obtained in a source domain can be re-trained in isolation in a dissimilar, target domain. In addition, we examine the effects of different initialization strategies at transfer-time. Our experiments show that sparse subnetworks obtained through lottery ticket training do not simply overfit to particular domains, but rather reflect an inductive bias of deep neural networks that can be exploited in multiple domains.","Hong Kong, China","Desai, Shrey  and
Zhan, Hongyuan  and
Aly, Ahmed",Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),10.18653/v1/D19-6117,November,153--162,Association for Computational Linguistics,Evaluating Lottery Tickets Under Distributional Shifts,https://aclanthology.org/D19-6117,2019,,,,,
792,inproceedings,abdou-etal-2019-x,"Although the vast majority of knowledge bases (KBs) are heavily biased towards English, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset (X-WikiRE), framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves (zero-shot) relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.","Hong Kong, China","Abdou, Mostafa  and
Sas, Cezar  and
Aralikatte, Rahul  and
Augenstein, Isabelle  and
S{\o}gaard, Anders",Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),10.18653/v1/D19-6130,November,265--274,Association for Computational Linguistics,"{X}-{W}iki{RE}: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension",https://aclanthology.org/D19-6130,2019,,,,,
793,inproceedings,kiyomaru-etal-2019-diversity,"Typical event sequences are an important class of commonsense knowledge. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision.","Hong Kong, China","Kiyomaru, Hirokazu  and
Omura, Kazumasa  and
Murawaki, Yugo  and
Kawahara, Daisuke  and
Kurohashi, Sadao",Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,10.18653/v1/D19-6014,November,113--122,Association for Computational Linguistics,Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction,https://aclanthology.org/D19-6014,2019,,,,,
794,inproceedings,gardner-etal-2019-making,"Machine reading comprehension, the task of evaluating a machine{'}s ability to comprehend a passage of text, has seen a surge in popularity in recent years. There are many datasets that are targeted at reading comprehension, and many systems that perform as well as humans on some of these datasets. Despite all of this interest, there is no work that systematically defines what reading comprehension is. In this work, we justify a question answering approach to reading comprehension and describe the various kinds of questions one might use to more fully test a system{'}s comprehension of a passage, moving beyond questions that only probe local predicate-argument structures. The main pitfall of this approach is that questions can easily have surface cues or other biases that allow a model to shortcut the intended reasoning process. We discuss ways proposed in current literature to mitigate these shortcuts, and we conclude with recommendations for future dataset collection efforts.","Hong Kong, China","Gardner, Matt  and
Berant, Jonathan  and
Hajishirzi, Hannaneh  and
Talmor, Alon  and
Min, Sewon",Proceedings of the 2nd Workshop on Machine Reading for Question Answering,10.18653/v1/D19-5815,November,105--112,Association for Computational Linguistics,On Making Reading Comprehension More Comprehensive,https://aclanthology.org/D19-5815,2019,,,,,
795,inproceedings,schmidt-2019-generalization,"Exposure bias refers to the train-test discrepancy that seemingly arises when an autoregressive generative model uses only ground-truth contexts at training time but generated ones at test time. We separate the contribution of the learning framework and the model to clarify the debate on consequences and review proposed counter-measures. In this light, we argue that generalization is the underlying property to address and propose unconditional generation as its fundamental benchmark. Finally, we combine latent variable modeling with a recent formulation of exploration in reinforcement learning to obtain a rigorous handling of true and generated contexts. Results on language modeling and variational sentence auto-encoding confirm the model{'}s generalization capability.",Hong Kong,"Schmidt, Florian",Proceedings of the 3rd Workshop on Neural Generation and Translation,10.18653/v1/D19-5616,November,157--167,Association for Computational Linguistics,Generalization in Generation: A closer look at Exposure Bias,https://aclanthology.org/D19-5616,2019,,,,,
796,inproceedings,zaremoodi-haffari-2019-adaptively,"Neural Machine Translation (NMT), a data-hungry technology, suffers from the lack of bilingual data in low-resource scenarios. Multitask learning (MTL) can alleviate this issue by injecting inductive biases into NMT, using auxiliary syntactic and semantic tasks. However, an effective \textit{training schedule} is required to balance the importance of tasks to get the best use of the training signal. The role of training schedule becomes even more crucial in \textit{biased-MTL} where the goal is to improve one (or a subset) of tasks the most, e.g. translation quality. Current approaches for biased-MTL are based on brittle \textit{hand-engineered} heuristics that require trial and error, and should be (re-)designed for each learning scenario. To the best of our knowledge, ours is the first work on \textit{adaptively} and \textit{dynamically} changing the training schedule in biased-MTL. We propose a rigorous approach for automatically reweighing the training data of the main and auxiliary tasks throughout the training process based on their contributions to the generalisability of the main NMT task. Our experiments on translating from English to Vietnamese/Turkish/Spanish show improvements of up to +1.2 BLEU points, compared to strong baselines. Additionally, our analyses shed light on the dynamic of needs throughout the training of NMT: from syntax to semantic.",Hong Kong,"Zaremoodi, Poorya  and
Haffari, Gholamreza",Proceedings of the 3rd Workshop on Neural Generation and Translation,10.18653/v1/D19-5618,November,177--186,Association for Computational Linguistics,Adaptively Scheduled Multitask Learning: The Case of Low-Resource Neural Machine Translation,https://aclanthology.org/D19-5618,2019,,,,,
797,inproceedings,fornaciari-hovy-2019-dense,"Prior research has shown that geolocation can be substantially improved by including user network information. While effective, it suffers from the curse of dimensionality, since networks are usually represented as sparse adjacency matrices of connections, which grow exponentially with the number of users. In order to incorporate this information, we therefore need to limit the network size, in turn limiting performance and risking sample bias. In this paper, we address these limitations by instead using dense network representations. We explore two methods to learn continuous node representations from either 1) the network structure with node2vec (Grover and Leskovec, 2016), or 2) textual user mentions via doc2vec (Le and Mikolov, 2014). We combine both methods with input from social media posts in an attention-based convolutional neural network and evaluate the contribution of each component on geolocation performance. Our method enables us to incorporate arbitrarily large networks in a fixed-length vector, without limiting the network size. Our models achieve competitive results with similar state-of-the-art methods, but with much fewer model parameters, while being applicable to networks of virtually any size.","Hong Kong, China","Fornaciari, Tommaso  and
Hovy, Dirk",Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),10.18653/v1/D19-5529,November,224--230,Association for Computational Linguistics,Dense Node Representation for Geolocation,https://aclanthology.org/D19-5529,2019,,,,,
798,inproceedings,han-etal-2019-fallacy,"This study analyzes the political slants of user comments on Korean partisan media. We built a BERT-based classifier to detect political leaning of short comments via the use of semi-unsupervised deep learning methods that produced an F1 score of 0.83. As a result of classifying 21.6K comments, we found the high presence of conservative bias on both conservative and liberal news outlets. Moreover, this study discloses an asymmetry across the partisan spectrum in that more liberals (48.0{\%}) than conservatives (23.6{\%}) comment not only on news stories resonating with their political perspectives but also on those challenging their viewpoints. These findings advance the current understanding of online echo chambers.","Hong Kong, China","Han, Jiyoung  and
Lee, Youngin  and
Lee, Junbum  and
Cha, Meeyoung",Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),10.18653/v1/D19-5548,November,370--374,Association for Computational Linguistics,The Fallacy of Echo Chambers: Analyzing the Political Slants of User-Generated News Comments in {K}orean Media,https://aclanthology.org/D19-5548,2019,,,,,
799,inproceedings,nasrin-etal-2019-many,"Social media has reportedly been (ab)used by Russian troll farms to promote political agendas. Specifically, state-affiliated actors disguise themselves as native citizens of the United States to promote discord and promote their political motives. Therefore, developing methods to automatically detect Russian trolls can ensure fair elections and possibly reduce political extremism by stopping trolls that produce discord. While data exists for some troll organizations (e.g., Internet Research Agency), it is challenging to collect ground-truth accounts for new troll farms in a timely fashion. In this paper, we study the impact the number of labeled troll accounts has on detection performance. We analyze the use of self-supervision with less than 100 troll accounts as training data. We improve classification performance by nearly 4{\%} F1. Furthermore, in combination with self-supervision, we also explore novel features for troll detection grounded in stylometry. Intuitively, we assume that the writing style is consistent across troll accounts because a single troll organization employee may control multiple user accounts. Overall, we improve on models based on words features by {\textasciitilde}9{\%} F1.","Hong Kong, China","Nasrin, Nayeema  and
Raymond Choo, Kim-Kwang  and
Ko, Myung  and
Rios, Anthony","Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda",10.18653/v1/D19-5003,November,20--30,Association for Computational Linguistics,How Many Users Are Enough? Exploring Semi-Supervision and Stylometric Features to Uncover a {R}ussian Troll Farm,https://aclanthology.org/D19-5003,2019,,,,,
800,inproceedings,park-etal-2019-generating,"Considering diverse aspects of an argumentative issue is an essential step for mitigating a biased opinion and making reasonable decisions. A related generation model can produce flexible results that cover a wide range of topics, compared to the retrieval-based method that may show unstable performance for unseen data. In this paper, we study the problem of generating sentential arguments from multiple perspectives, and propose a neural method to address this problem. Our model, ArgDiver (Argument generation model from diverse perspectives), in a way a conversational system, successfully generates high-quality sentential arguments. At the same time, the automatically generated arguments by our model show a higher diversity than those generated by any other baseline models. We believe that our work provides evidence for the potential of a good generation model in providing diverse perspectives on a controversial topic.","Hong Kong, China","Park, ChaeHun  and
Yang, Wonsuk  and
Park, Jong","Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda",10.18653/v1/D19-5007,November,56--65,Association for Computational Linguistics,Generating Sentential Arguments from Diverse Perspectives on Controversial Topic,https://aclanthology.org/D19-5007,2019,,,,,
801,inproceedings,al-omari-etal-2019-justdeep,"The internet and the high use of social media have enabled the modern-day journalism to publish, share and spread news that is difficult to distinguish if it is true or fake. Defining {``}fake news{''} is not well established yet, however, it can be categorized under several labels: false, biased, or framed to mislead the readers that are characterized as propaganda. Digital content production technologies with logical fallacies and emotional language can be used as propaganda techniques to gain more readers or mislead the audience. Recently, several researchers have proposed deep learning (DL) models to address this issue. This research paper provides an ensemble deep learning model using BiLSTM, XGBoost, and BERT to detect propaganda. The proposed model has been applied on the dataset provided by the challenge NLP4IF 2019, Task 1 Sentence Level Classification (SLC) and it shows a significant performance over the baseline model.","Hong Kong, China","Al-Omari, Hani  and
Abdullah, Malak  and
AlTiti, Ola  and
Shaikh, Samira","Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda",10.18653/v1/D19-5016,November,113--118,Association for Computational Linguistics,{JUSTD}eep at {NLP}4{IF} 2019 Task 1: Propaganda Detection using Ensemble Deep Learning Models,https://aclanthology.org/D19-5016,2019,,,,,
802,inproceedings,xu-etal-2019-alter,"In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation.","Hong Kong, China","Xu, Qiongkai  and
Xu, Chenchen  and
Qu, Lizhen",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,10.18653/v1/D19-3003,November,13--18,Association for Computational Linguistics,{ALTER}: Auxiliary Text Rewriting Tool for Natural Language Generation,https://aclanthology.org/D19-3003,2019,,,,,
803,inproceedings,chang-etal-2019-bias,"Recent advances in data-driven machine learning techniques (e.g., deep neural networks) have revolutionized many natural language processing applications. These approaches automatically learn how to make decisions based on the statistics and diagnostic information from large amounts of training data. Despite the remarkable accuracy of machine learning in various applications, learning algorithms run the risk of relying on societal biases encoded in the training data to make predictions. This often occurs even when gender and ethnicity information is not explicitly provided to the system because learning algorithms are able to discover implicit associations between individuals and their demographic information based on other variables such as names, titles, home addresses, etc. Therefore, machine learning algorithms risk potentially encouraging unfair and discriminatory decision making and raise serious privacy concerns. Without properly quantifying and reducing the reliance on such correlations, broad adoption of these models might have the undesirable effect of magnifying harmful stereotypes or implicit biases that rely on sensitive demographic attributes.In this tutorial, we will review the history of bias and fairness studies in machine learning and language processing and present recent community effort in quantifying and mitigating bias in natural language processing models for a wide spectrum of tasks, including word embeddings, co-reference resolution, machine translation, and vision-and-language tasks. In particular, we will focus on the following topics:+ Definitions of fairness and bias.+ Data, algorithms, and models that propagate and even amplify social bias to NLP applications and metrics to quantify these biases.+ Algorithmic solutions; learning objective; design principles to prevent social bias in NLP systems and their potential drawbacks.The tutorial will bring researchers and practitioners to be aware of this issue, and encourage the research community to propose innovative solutions to promote fairness in NLP.","Hong Kong, China","Chang, Kai-Wei  and
Prabhakaran, Vinodkumar  and
Ordonez, Vicente",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts,,November,,Association for Computational Linguistics,Bias and Fairness in Natural Language Processing,https://aclanthology.org/D19-2004,2019,,,,,
804,inproceedings,kryscinski-etal-2019-neural,"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.","Hong Kong, China","Kryscinski, Wojciech  and
Keskar, Nitish Shirish  and
McCann, Bryan  and
Xiong, Caiming  and
Socher, Richard",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1051,November,540--551,Association for Computational Linguistics,Neural Text Summarization: A Critical Evaluation,https://aclanthology.org/D19-1051,2019,,,,,
805,inproceedings,liu-etal-2019-hierarchical,"Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.","Hong Kong, China","Liu, Linlin  and
Lin, Xiang  and
Joty, Shafiq  and
Han, Simeng  and
Bing, Lidong",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1093,November,1007--1017,Association for Computational Linguistics,Hierarchical Pointer Net Parsing,https://aclanthology.org/D19-1093,2019,,,,,
806,inproceedings,geva-etal-2019-modeling,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators.","Hong Kong, China","Geva, Mor  and
Goldberg, Yoav  and
Berant, Jonathan",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1107,November,1161--1166,Association for Computational Linguistics,Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets,https://aclanthology.org/D19-1107,2019,,,,,
807,inproceedings,davison-etal-2019-commonsense,"Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple{'}s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.","Hong Kong, China","Davison, Joe  and
Feldman, Joshua  and
Rush, Alexander",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1109,November,1173--1178,Association for Computational Linguistics,Commonsense Knowledge Mining from Pretrained Models,https://aclanthology.org/D19-1109,2019,,,,,
808,inproceedings,dufter-schutze-2019-analytical,"Word embeddings are useful for a wide variety of tasks, but they lack interpretability. By rotating word spaces, interpretable dimensions can be identified while preserving the information contained in the embeddings without any loss. In this work, we investigate three methods for making word spaces interpretable by rotation: Densifier (Rothe et al., 2016), linear SVMs and DensRay, a new method we propose. In contrast to Densifier, DensRay can be computed in closed form, is hyperparameter-free and thus more robust than Densifier. We evaluate the three methods on lexicon induction and set-based word analogy. In addition we provide qualitative insights as to how interpretable word spaces can be used for removing gender bias from embeddings.","Hong Kong, China","Dufter, Philipp  and
Sch{\""u}tze, Hinrich",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1111,November,1185--1191,Association for Computational Linguistics,Analytical Methods for Interpretable Ultradense Word Embeddings,https://aclanthology.org/D19-1111,2019,,,,,
809,inproceedings,aralikatte-etal-2019-rewarding,"Unresolved coreference is a bottleneck for relation extraction, and high-quality coreference resolvers may produce an output that makes it a lot easier to extract knowledge triples. We show how to improve coreference resolvers by forwarding their input to a relation extraction system and reward the resolvers for producing triples that are found in knowledge bases. Since relation extraction systems can rely on different forms of supervision and be biased in different ways, we obtain the best performance, improving over the state of the art, using multi-task reinforcement learning.","Hong Kong, China","Aralikatte, Rahul  and
Lent, Heather  and
Gonzalez, Ana Valeria  and
Herschcovich, Daniel  and
Qiu, Chen  and
Sandholm, Anders  and
Ringaard, Michael  and
S{\o}gaard, Anders",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1118,November,1229--1235,Association for Computational Linguistics,Rewarding Coreference Resolvers for Being Consistent with World Knowledge,https://aclanthology.org/D19-1118,2019,,,,,
810,inproceedings,niu-bansal-2019-automatically,"Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image{'}s semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy{'}s required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.","Hong Kong, China","Niu, Tong  and
Bansal, Mohit",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1132,November,1317--1323,Association for Computational Linguistics,Automatically Learning Data Augmentation Policies for Dialogue Tasks,https://aclanthology.org/D19-1132,2019,,,,,
811,inproceedings,hao-etal-2019-towards,"Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs {--} Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual architectures and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.","Hong Kong, China","Hao, Jie  and
Wang, Xing  and
Shi, Shuming  and
Zhang, Jinfeng  and
Tu, Zhaopeng",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1135,November,1336--1341,Association for Computational Linguistics,Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons,https://aclanthology.org/D19-1135,2019,,,,,
812,inproceedings,breitfeller-etal-2019-finding,"Microaggressions are subtle, often veiled, manifestations of human biases. These uncivil interactions can have a powerful negative impact on people by marginalizing minorities and disadvantaged groups. The linguistic subtlety of microaggressions in communication has made it difficult for researchers to analyze their exact nature, and to quantify and extract microaggressions automatically. Specifically, the lack of a corpus of real-world microaggressions and objective criteria for annotating them have prevented researchers from addressing these problems at scale. In this paper, we devise a general but nuanced, computationally operationalizable typology of microaggressions based on a small subset of data that we have. We then create two datasets: one with examples of diverse types of microaggressions recollected by their targets, and another with gender-based microaggressions in public conversations on social media. We introduce a new, more objective, criterion for annotation and an active-learning based procedure that increases the likelihood of surfacing posts containing microaggressions. Finally, we analyze the trends that emerge from these new datasets.","Hong Kong, China","Breitfeller, Luke  and
Ahn, Emily  and
Jurgens, David  and
Tsvetkov, Yulia",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1176,November,1664--1674,Association for Computational Linguistics,Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts,https://aclanthology.org/D19-1176,2019,,,,,
813,inproceedings,chen-etal-2019-towards,"In this paper, we propose a novel end-to-end framework called KBRD, which stands for Knowledge-Based Recommender Dialog System. It integrates the recommender system and the dialog generation system. The dialog generation system can enhance the performance of the recommendation system by introducing information about users{'} preferences, and the recommender system can improve that of the dialog generation system by providing recommendation-aware vocabulary bias. Experimental results demonstrate that our proposed model has significant advantages over the baselines in both the evaluation of dialog generation and recommendation. A series of analyses show that the two systems can bring mutual benefits to each other, and the introduced knowledge contributes to both their performances.","Hong Kong, China","Chen, Qibin  and
Lin, Junyang  and
Zhang, Yichang  and
Ding, Ming  and
Cen, Yukuo  and
Yang, Hongxia  and
Tang, Jie",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1189,November,1803--1813,Association for Computational Linguistics,Towards Knowledge-Based Recommender Dialog System,https://aclanthology.org/D19-1189,2019,,,,,
814,inproceedings,shi-etal-2019-build,"User simulators are essential for training reinforcement learning (RL) based dialog models. The performance of the simulator directly impacts the RL policy. However, building a good user simulator that models real user behaviors is challenging. We propose a method of standardizing user simulator building that can be used by the community to compare dialog system quality using the same set of user simulators fairly. We present implementations of six user simulators trained with different dialog planning and generation methods. We then calculate a set of automatic metrics to evaluate the quality of these simulators both directly and indirectly. We also ask human users to assess the simulators directly and indirectly by rating the simulated dialogs and interacting with the trained systems. This paper presents a comprehensive evaluation framework for user simulator study and provides a better understanding of the pros and cons of different user simulators, as well as their impacts on the trained systems.","Hong Kong, China","Shi, Weiyan  and
Qian, Kun  and
Wang, Xuewei  and
Yu, Zhou",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1206,November,1990--2000,Association for Computational Linguistics,How to Build User Simulators to Train {RL}-based Dialog Systems,https://aclanthology.org/D19-1206,2019,,,,,
815,inproceedings,jiang-etal-2019-tiger,"This paper presents a new metric called TIGEr for the automatic evaluation of image captioning systems. Popular metrics, such as BLEU and CIDEr, are based solely on text matching between reference captions and machine-generated captions, potentially leading to biased evaluations because references may not fully cover the image content and natural language is inherently ambiguous. Building upon a machine-learned text-image grounding model, TIGEr allows to evaluate caption quality not only based on how well a caption represents image content, but also on how well machine-generated captions match human-generated captions. Our empirical tests show that TIGEr has a higher consistency with human judgments than alternative existing metrics. We also comprehensively assess the metric{'}s effectiveness in caption evaluation by measuring the correlation between human judgments and metric scores.","Hong Kong, China","Jiang, Ming  and
Huang, Qiuyuan  and
Zhang, Lei  and
Wang, Xin  and
Zhang, Pengchuan  and
Gan, Zhe  and
Diesner, Jana  and
Gao, Jianfeng",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1220,November,2141--2152,Association for Computational Linguistics,{TIGE}r: Text-to-Image Grounding for Image Caption Evaluation,https://aclanthology.org/D19-1220,2019,,,,,
816,inproceedings,wallace-etal-2019-universal,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94{\%} to 0.55{\%}, 72{\%} of {``}why{''} questions in SQuAD to be answered {``}to kill american people{''}, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.","Hong Kong, China","Wallace, Eric  and
Feng, Shi  and
Kandpal, Nikhil  and
Gardner, Matt  and
Singh, Sameer",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1221,November,2153--2162,Association for Computational Linguistics,Universal Adversarial Triggers for Attacking and Analyzing {NLP},https://aclanthology.org/D19-1221,2019,,,,,
817,inproceedings,ross-pavlick-2019-well,"In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical{--}a bias which is amplified in BERT. We further show that, encouragingly, BERT{'}s inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.","Hong Kong, China","Ross, Alexis  and
Pavlick, Ellie",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1228,November,2230--2240,Association for Computational Linguistics,How well do {NLI} models capture verb veridicality?,https://aclanthology.org/D19-1228,2019,,,,,
818,inproceedings,mabona-etal-2019-neural,"Rhetorical structure trees have been shown to be useful for several document-level tasks including summarization and document classification. Previous approaches to RST parsing have used discriminative models; however, these are less sample efficient than generative models, and RST parsing datasets are typically small. In this paper, we present the first generative model for RST parsing. Our model is a document-level RNN grammar (RNNG) with a bottom-up traversal order. We show that, for our parser{'}s traversal order, previous beam search algorithms for RNNGs have a left-branching bias which is ill-suited for RST parsing.We develop a novel beam search algorithm that keeps track of both structure-and word-generating actions without exhibit-ing this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data","Hong Kong, China","Mabona, Amandla  and
Rimell, Laura  and
Clark, Stephen  and
Vlachos, Andreas",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1233,November,2284--2295,Association for Computational Linguistics,Neural Generative Rhetorical Structure Parsing,https://aclanthology.org/D19-1233,2019,,,,,
819,inproceedings,ponti-etal-2019-towards,"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace{'}s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world{'}s languages, we hope that these insights will broaden the scope of applications for language technology.","Hong Kong, China","Ponti, Edoardo Maria  and
Vuli{\'c}, Ivan  and
Cotterell, Ryan  and
Reichart, Roi  and
Korhonen, Anna",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1288,November,2900--2910,Association for Computational Linguistics,Towards Zero-shot Language Modeling,https://aclanthology.org/D19-1288,2019,,,,,
820,inproceedings,yang-etal-2019-end,"Generating high-quality paraphrases is a fundamental yet challenging natural language processing task. Despite the effectiveness of previous work based on generative models, there remain problems with exposure bias in recurrent neural networks, and often a failure to generate realistic sentences. To overcome these challenges, we propose the first end-to-end conditional generative architecture for generating paraphrases via adversarial training, which does not depend on extra linguistic information. Extensive experiments on four public datasets demonstrate the proposed method achieves state-of-the-art results, outperforming previous generative architectures on both automatic metrics (BLEU, METEOR, and TER) and human evaluations.","Hong Kong, China","Yang, Qian  and
Huo, Zhouyuan  and
Shen, Dinghan  and
Cheng, Yong  and
Wang, Wenlin  and
Wang, Guoyin  and
Carin, Lawrence",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1309,November,3132--3142,Association for Computational Linguistics,An End-to-End Generative Architecture for Paraphrase Generation,https://aclanthology.org/D19-1309,2019,,,,,
821,inproceedings,scialom-etal-2019-answers,"Abstractive summarization approaches based on Reinforcement Learning (RL) have recently been proposed to overcome classical likelihood maximization. RL enables to consider complex, possibly non differentiable, metrics that globally assess the quality and relevance of the generated outputs. ROUGE, the most used summarization metric, is known to suffer from bias towards lexical similarity as well as from sub-optimal accounting for fluency and readability of the generated abstracts. We thus explore and propose alternative evaluation measures: the reported human-evaluation analysis shows that the proposed metrics, based on Question Answering, favorably compare to ROUGE {--} with the additional property of not requiring reference summaries. Training a RL-based model on these metrics leads to improvements (both in terms of human or automated metrics) over current approaches that use ROUGE as reward.","Hong Kong, China","Scialom, Thomas  and
Lamprier, Sylvain  and
Piwowarski, Benjamin  and
Staiano, Jacopo",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1320,November,3246--3256,Association for Computational Linguistics,Answers Unite! Unsupervised Metrics for Reinforced Summarization Models,https://aclanthology.org/D19-1320,2019,,,,,
822,inproceedings,jung-etal-2019-earlier,"Despite the recent developments on neural summarization systems, the underlying logic behind the improvements from the systems and its corpus-dependency remains largely unexplored. Position of sentences in the original text, for example, is a well known bias for news summarization. Following in the spirit of the claim that summarization is a combination of sub-functions, we define three sub-aspects of summarization: position, importance, and diversity and conduct an extensive analysis of the biases of each sub-aspect with respect to the domain of nine different summarization corpora (e.g., news, academic papers, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system.","Hong Kong, China","Jung, Taehee  and
Kang, Dongyeop  and
Mentch, Lucas  and
Hovy, Eduard",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1327,November,3324--3335,Association for Computational Linguistics,Earlier Isn{'}t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization,https://aclanthology.org/D19-1327,2019,,,,,
823,inproceedings,stahlberg-byrne-2019-nmt,"We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50{\%} of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.","Hong Kong, China","Stahlberg, Felix  and
Byrne, Bill",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1331,November,3356--3362,Association for Computational Linguistics,On {NMT} Search Errors and Model Errors: Cat Got Your Tongue?,https://aclanthology.org/D19-1331,2019,,,,,
824,inproceedings,schmidt-etal-2019-autoregressive,"Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of autoregressive models.","Hong Kong, China","Schmidt, Florian  and
Mandt, Stephan  and
Hofmann, Thomas",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1338,November,3400--3406,Association for Computational Linguistics,Autoregressive Text Generation Beyond Feedback Loops,https://aclanthology.org/D19-1338,2019,,,,,
825,inproceedings,sheng-etal-2019-woman,"We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset.","Hong Kong, China","Sheng, Emily  and
Chang, Kai-Wei  and
Natarajan, Premkumar  and
Peng, Nanyun",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1339,November,3407--3412,Association for Computational Linguistics,The Woman Worked as a Babysitter: On Biases in Language Generation,https://aclanthology.org/D19-1339,2019,,,,,
826,inproceedings,schuster-etal-2019-towards,"Fact verification requires validating a claim in the context of evidence. We show, however, that in the popular FEVER dataset this might not necessarily be the case. Claim-only classifiers perform competitively with top evidence-aware models. In this paper, we investigate the cause of this phenomenon, identifying strong cues for predicting labels solely based on the claim, without considering any evidence. We create an evaluation set that avoids those idiosyncrasies. The performance of FEVER-trained models significantly drops when evaluated on this test set. Therefore, we introduce a regularization method which alleviates the effect of bias in the training data, obtaining improvements on the newly created test set. This work is a step towards a more sound evaluation of reasoning capabilities in fact verification models.","Hong Kong, China","Schuster, Tal  and
Shah, Darsh  and
Yeo, Yun Jie Serene  and
Roberto Filizzola Ortiz, Daniel  and
Santus, Enrico  and
Barzilay, Regina",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1341,November,3419--3425,Association for Computational Linguistics,Towards Debiasing Fact Verification Models,https://aclanthology.org/D19-1341,2019,,,,,
827,inproceedings,wang-etal-2019-investigating,"Deep neural network models such as long short-term memory (LSTM) and tree-LSTM have been proven to be effective for sentiment analysis. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the parse tree will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement.","Hong Kong, China","Wang, Jin  and
Yu, Liang-Chih  and
Lai, K. Robert  and
Zhang, Xuejie",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1343,November,3432--3437,Association for Computational Linguistics,Investigating Dynamic Routing in Tree-Structured {LSTM} for Sentiment Analysis,https://aclanthology.org/D19-1343,2019,,,,,
828,inproceedings,shen-etal-2019-improving,"Pointer Generators have been the de facto standard for modern summarization systems. However, this architecture faces two major drawbacks: Firstly, the pointer is limited to copying the exact words while ignoring possible inflections or abstractions, which restricts its power of capturing richer latent alignment. Secondly, the copy mechanism results in a strong bias towards extractive generations, where most sentences are produced by simply copying from the source text. In this paper, we address these problems by allowing the model to {``}edit{''} pointed tokens instead of always hard copying them. The editing is performed by transforming the pointed word vector into a target space with a learned relation embedding. On three large-scale summarization dataset, we show the model is able to (1) capture more latent alignment relations than exact word matches, (2) improve word alignment accuracy, allowing for better model interpretation and controlling, (3) generate higher-quality summaries validated by both qualitative and quantitative evaluations and (4) bring more abstraction to the generated summaries.","Hong Kong, China","Shen, Xiaoyu  and
Zhao, Yang  and
Su, Hui  and
Klakow, Dietrich",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1390,November,3762--3773,Association for Computational Linguistics,Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator,https://aclanthology.org/D19-1390,2019,,,,,
829,inproceedings,wang-etal-2019-learning,"Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial {``}abstract program{''} and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WikiTableQuestions and WikiSQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.","Hong Kong, China","Wang, Bailin  and
Titov, Ivan  and
Lapata, Mirella",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1391,November,3774--3785,Association for Computational Linguistics,Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs,https://aclanthology.org/D19-1391,2019,,,,,
830,inproceedings,ye-etal-2019-looking,"In recent years there is a surge of interest in applying distant supervision (DS) to automatically generate training data for relation extraction (RE). In this paper, we study the problem what limits the performance of DS-trained neural models, conduct thorough analyses, and identify a factor that can influence the performance greatly, shifted label distribution. Specifically, we found this problem commonly exists in real-world DS datasets, and without special handing, typical DS-RE models cannot automatically adapt to this shift, thus achieving deteriorated performance. To further validate our intuition, we develop a simple yet effective adaptation method for DS-trained models, bias adjustment, which updates models learned over the source domain (i.e., DS training set) with a label distribution estimated on the target domain (i.e., test set). Experiments demonstrate that bias adjustment achieves consistent performance gains on DS-trained models, especially on neural models, with an up to 23{\%} relative F1 improvement, which verifies our assumptions. Our code and data can be found at https://github.com/INK-USC/shifted-label-distribution.","Hong Kong, China","Ye, Qinyuan  and
Liu, Liyuan  and
Zhang, Maosen  and
Ren, Xiang",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1397,November,3841--3850,Association for Computational Linguistics,Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction,https://aclanthology.org/D19-1397,2019,,,,,
831,inproceedings,prabhu-etal-2019-sampling,"The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble networks). Finally, we show the effectiveness of the selected samples by creating tiny high-quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state of the art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios. Code and models are available at: https://github.com/drimpossible/Sampling-Bias-Active-Learning.","Hong Kong, China","Prabhu, Ameya  and
Dognin, Charles  and
Singh, Maneesh",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1417,November,4058--4068,Association for Computational Linguistics,Sampling Bias in Deep Active Classification: An Empirical Study,https://aclanthology.org/D19-1417,2019,,,,,
832,inproceedings,clark-etal-2019-dont,"State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.","Hong Kong, China","Clark, Christopher  and
Yatskar, Mark  and
Zettlemoyer, Luke",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1418,November,4069--4082,Association for Computational Linguistics,Don{'}t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases,https://aclanthology.org/D19-1418,2019,,,,,
833,inproceedings,geiger-etal-2019-posing,"Deep learning models for semantics are generally evaluated using naturalistic corpora. Adversarial testing methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that good performance at these naturalistic tasks can hide serious shortcomings. However, we should insist that these evaluations be fair {--} that the models are given data sufficient to support the requisite kinds of generalization. In this paper, we define and motivate a formal notion of fairness in this sense. We then apply these ideas to natural language inference by constructing very challenging but provably fair artificial datasets and showing that standard neural models fail to generalize in the required ways; only task-specific models that jointly compose the premise and hypothesis are able to achieve high performance, and even these models do not solve the task perfectly.","Hong Kong, China","Geiger, Atticus  and
Cases, Ignacio  and
Karttunen, Lauri  and
Potts, Christopher",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1456,November,4485--4495,Association for Computational Linguistics,Posing Fair Generalization Tasks for Natural Language Inference,https://aclanthology.org/D19-1456,2019,,,,,
834,inproceedings,dalvi-etal-2019-everything,"Our goal is to better comprehend procedural text, e.g., a paragraph about photosynthesis, by not only predicting what happens, but *why* some actions need to happen before others. Our approach builds on a prior process comprehension framework for predicting actions{'} effects, to also identify subsequent steps that those effects enable. We present our new model (XPAD) that biases effect predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. We also extend an existing benchmark dataset for procedural text comprehension, ProPara, by adding the new task of explaining actions by predicting their dependencies. We find that XPAD significantly outperforms prior systems on this task, while maintaining the performance on the original task in ProPara. The dataset is available at http://data.allenai.org/propara","Hong Kong, China","Dalvi, Bhavana  and
Tandon, Niket  and
Bosselut, Antoine  and
Yih, Wen-tau  and
Clark, Peter",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1457,November,4496--4505,Association for Computational Linguistics,Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text,https://aclanthology.org/D19-1457,2019,,,,,
835,inproceedings,peskov-etal-2019-multi,"The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as virtual assistants become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, linguistic diversity, domain coverage, or annotation granularity. In this paper, we present strategies toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the {``}customer{''}) is paired with a trained annotator (the {``}agent{''}). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our strategies on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain.","Hong Kong, China","Peskov, Denis  and
Clarke, Nancy  and
Krone, Jason  and
Fodor, Brigi  and
Zhang, Yi  and
Youssef, Adel  and
Diab, Mona",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1460,November,4526--4536,Association for Computational Linguistics,Multi-Domain Goal-Oriented Dialogues ({M}ulti{D}o{GO}): Strategies toward Curating and Annotating Large Scale Dialogue Data,https://aclanthology.org/D19-1460,2019,,,,,
836,inproceedings,xie-etal-2019-text,"We present a text-based framework for investigating moral sentiment change of the public via longitudinal corpora. Our framework is based on the premise that language use can inform people{'}s moral perception toward right or wrong, and we build our methodology by exploring moral biases learned from diachronic word embeddings. We demonstrate how a parameter-free model supports inference of historical shifts in moral sentiment toward concepts such as slavery and democracy over centuries at three incremental levels: moral relevance, moral polarity, and fine-grained moral dimensions. We apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale. Our work offers opportunities for applying natural language processing toward characterizing moral sentiment change in society.","Hong Kong, China","Xie, Jing Yi  and
Ferreira Pinto Junior, Renato  and
Hirst, Graeme  and
Xu, Yang",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1472,November,4654--4663,Association for Computational Linguistics,Text-based inference of moral sentiment change,https://aclanthology.org/D19-1472,2019,,,,,
837,inproceedings,zhong-etal-2019-detecting,"Gang-involved youth in cities such as Chicago sometimes post on social media to express their aggression towards rival gangs and previous research has demonstrated that a deep learning approach can predict aggression and loss in posts. To address the possibility of bias in this sensitive application, we developed an approach to systematically interpret the state of the art model. We found, surprisingly, that it frequently bases its predictions on stop words such as {``}a{''} or {``}on{''}, an approach that could harm social media users who have no aggressive intentions. To tackle this bias, domain experts annotated the rationales, highlighting words that explain why a tweet is labeled as {``}aggression{''}. These new annotations enable us to quantitatively measure how justified the model predictions are, and build models that drastically reduce bias. Our study shows that in high stake scenarios, accuracy alone cannot guarantee a good system and we need new evaluation methods.","Hong Kong, China","Zhong, Ruiqi  and
Chen, Yanda  and
Patton, Desmond  and
Selous, Charlotte  and
McKeown, Kathy",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1483,November,4765--4775,Association for Computational Linguistics,Detecting and Reducing Bias in a High Stakes Domain,https://aclanthology.org/D19-1483,2019,,,,,
838,inproceedings,ma-etal-2019-news2vec,"With the development of NLP technologies, news can be automatically categorized and labeled according to a variety of characteristics, at the same time be represented as low dimensional embeddings. However, it lacks a systematic approach that effectively integrates the inherited features and inter-textual knowledge of news to represent the collective information with a dense vector. With the aim of filling this gap, the News2vec model is proposed to allow the distributed representation of news taking into account its associated features. To describe the cross-document linkages between news, a network consisting of news and its attributes is constructed. Moreover, the News2vec model treats the news node as a bag of features by developing the Subnode model. Based on the biased random walk and the skip-gram model, each news feature is mapped to a vector, and the news is thus represented as the sum of its features. This approach offers an easy solution to create embeddings for unseen news nodes based on its attributes. To evaluate our model, dimension reduction plots and correlation heat-maps are created to visualize the news vectors, together with the application of two downstream tasks, the stock movement prediction and news recommendation. By comparing with other established text/sentence embedding models, we show that News2vec achieves state-of-the-art performance on these news-related tasks.","Hong Kong, China","Ma, Ye  and
Zong, Lu  and
Yang, Yikang  and
Su, Jionglong",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1490,November,4843--4852,Association for Computational Linguistics,{N}ews2vec: News Network Embedding with Subnode Information,https://aclanthology.org/D19-1490,2019,,,,,
839,inproceedings,hall-maudslay-etal-2019-name,"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19{\%} across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49{\%}), thus improving on the state-of-the-art for bias mitigation.","Hong Kong, China","Hall Maudslay, Rowan  and
Gonen, Hila  and
Cotterell, Ryan  and
Teufel, Simone",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1530,November,5267--5275,Association for Computational Linguistics,It{'}s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution,https://aclanthology.org/D19-1530,2019,,,,,
840,inproceedings,zhou-etal-2019-examining,"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.","Hong Kong, China","Zhou, Pei  and
Shi, Weijia  and
Zhao, Jieyu  and
Huang, Kuan-Hao  and
Chen, Muhao  and
Cotterell, Ryan  and
Chang, Kai-Wei",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1531,November,5276--5284,Association for Computational Linguistics,Examining Gender Bias in Languages with Grammatical Gender,https://aclanthology.org/D19-1531,2019,,,,,
841,inproceedings,amplayo-2019-rethinking,"Text attributes, such as user and product information in product reviews, have been used to improve the performance of sentiment classification models. The de facto standard method is to incorporate them as additional biases in the attention mechanism, and more performance gains are achieved by extending the model architecture. In this paper, we show that the above method is the least effective way to represent and inject attributes. To demonstrate this hypothesis, unlike previous models with complicated architectures, we limit our base model to a simple BiLSTM with attention classifier, and instead focus on how and where the attributes should be incorporated in the model. We propose to represent attributes as chunk-wise importance weight matrices and consider four locations in the model (i.e., embedding, encoding, attention, classifier) to inject attributes. Experiments show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also outperform the state-of-the-art despite our use of a simple base model. Finally, we show that these representations transfer well to other tasks. Model implementation and datasets are released here: https://github.com/rktamplayo/CHIM.","Hong Kong, China","Amplayo, Reinald Kim",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1562,November,5602--5613,Association for Computational Linguistics,Rethinking Attribute Representation and Injection for Sentiment Classification,https://aclanthology.org/D19-1562,2019,,,,,
842,inproceedings,wang-etal-2019-hmeae,"Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a Hierarchical Modular Event Argument Extraction (HMEAE) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that HMEAE can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from https://github.com/thunlp/HMEAE.","Hong Kong, China","Wang, Xiaozhi  and
Wang, Ziqi  and
Han, Xu  and
Liu, Zhiyuan  and
Li, Juanzi  and
Li, Peng  and
Sun, Maosong  and
Zhou, Jie  and
Ren, Xiang",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1584,November,5777--5783,Association for Computational Linguistics,{HMEAE}: Hierarchical Modular Event Argument Extraction,https://aclanthology.org/D19-1584,2019,,,,,
843,inproceedings,le-etal-2019-revisiting,"Theory of mind, i.e., the ability to reason about intents and beliefs of agents is an important task in artificial intelligence and central to resolving ambiguous references in natural language dialogue. In this work, we revisit the evaluation of theory of mind through question answering. We show that current evaluation methods are flawed and that existing benchmark tasks can be solved without theory of mind due to dataset biases. Based on prior work, we propose an improved evaluation protocol and dataset in which we explicitly control for data regularities via a careful examination of the answer space. We show that state-of-the-art methods which are successful on existing benchmarks fail to solve theory-of-mind tasks in our proposed approach.","Hong Kong, China","Le, Matthew  and
Boureau, Y-Lan  and
Nickel, Maximilian",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1598,November,5872--5877,Association for Computational Linguistics,Revisiting the Evaluation of Theory of Mind through Question Answering,https://aclanthology.org/D19-1598,2019,,,,,
844,inproceedings,du-ji-2019-empirical,"Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.","Hong Kong, China","Du, Wanyu  and
Ji, Yangfeng",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1619,November,6012--6018,Association for Computational Linguistics,An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation,https://aclanthology.org/D19-1619,2019,,,,,
845,inproceedings,grenander-etal-2019-countering,"Sentence position is a strong feature for news summarization, since the lead often (but not always) summarizes the key points of the article. In this paper, we show that recent neural systems excessively exploit this trend, which although powerful for many inputs, is also detrimental when summarizing documents where important content should be extracted from later parts of the article. We propose two techniques to make systems sensitive to the importance of content in different parts of the article. The first technique employs {`}unbiased{'} data; i.e., randomly shuffled sentences of the source document, to pretrain the model. The second technique uses an auxiliary ROUGE-based loss that encourages the model to distribute importance scores throughout a document by mimicking sentence-level ROUGE scores on the training data. We show that these techniques significantly improve the performance of a competitive reinforcement learning based extractive system, with the auxiliary loss being more powerful than pretraining.","Hong Kong, China","Grenander, Matt  and
Dong, Yue  and
Cheung, Jackie Chi Kit  and
Louis, Annie",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1620,November,6019--6024,Association for Computational Linguistics,Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses,https://aclanthology.org/D19-1620,2019,,,,,
846,inproceedings,li-etal-2019-deep,"Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.","Hong Kong, China","Li, Siyao  and
Lei, Deren  and
Qin, Pengda  and
Wang, William Yang",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1623,November,6038--6044,Association for Computational Linguistics,Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization,https://aclanthology.org/D19-1623,2019,,,,,
847,inproceedings,du-etal-2019-exploring,"Word embeddings have been widely used to study gender stereotypes in texts. One key problem regarding existing bias scores is to evaluate their validities: do they really reflect true bias levels? For a small set of words (e.g. occupations), we can rely on human annotations or external data. However, for most words, evaluating the correctness of them is still an open problem. In this work, we utilize word association test, which contains rich types of word connections annotated by human participants, to explore how gender stereotypes spread within our minds. Specifically, we use random walk on word association graph to derive bias scores for a large amount of words. Experiments show that these bias scores correlate well with bias in the real world. More importantly, comparing with word-embedding-based bias scores, it provides a different perspective on gender stereotypes in words.","Hong Kong, China","Du, Yupei  and
Wu, Yuanbin  and
Lan, Man",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1635,November,6133--6143,Association for Computational Linguistics,Exploring Human Gender Stereotypes with Word Association Test,https://aclanthology.org/D19-1635,2019,,,,,
848,inproceedings,barrett-etal-2019-adversarial,"Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models.","Hong Kong, China","Barrett, Maria  and
Kementchedjhieva, Yova  and
Elazar, Yanai  and
Elliott, Desmond  and
S{\o}gaard, Anders",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1662,November,6330--6335,Association for Computational Linguistics,Adversarial Removal of Demographic Attributes Revisited,https://aclanthology.org/D19-1662,2019,,,,,
849,inproceedings,fan-etal-2019-plain,"The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions.","Hong Kong, China","Fan, Lisa  and
White, Marshall  and
Sharma, Eva  and
Su, Ruisi  and
Choubey, Prafulla Kumar  and
Huang, Ruihong  and
Wang, Lu",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1664,November,6343--6349,Association for Computational Linguistics,In Plain Sight: Media Bias Through the Lens of Factual Reporting,https://aclanthology.org/D19-1664,2019,,,,,
850,inproceedings,merullo-etal-2019-investigating,"Sports broadcasters inject drama into play-by-play commentary by building team and player narratives through subjective analyses and anecdotes. Prior studies based on small datasets and manual coding show that such theatrics evince commentator bias in sports broadcasts. To examine this phenomenon, we assemble FOOTBALL, which contains 1,455 broadcast transcripts from American football games across six decades that are automatically annotated with 250K player mentions and linked with racial metadata. We identify major confounding factors for researchers examining racial bias in FOOTBALL, and perform a computational analysis that supports conclusions from prior social science studies.","Hong Kong, China","Merullo, Jack  and
Yeh, Luke  and
Handler, Abram  and
Grissom II, Alvin  and
O{'}Connor, Brendan  and
Iyyer, Mohit",Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),10.18653/v1/D19-1666,November,6355--6361,Association for Computational Linguistics,Investigating Sports Commentator Bias within a Large Corpus of {A}merican Football Broadcasts,https://aclanthology.org/D19-1666,2019,,,,,
851,inproceedings,lakew-etal-2019-controlling,"The recent advances introduced by neural machine translation (NMT) are rapidly expanding the application fields of machine translation, as well as reshaping the quality level to be targeted. In particular, if translations have to fit some given layout, quality should not only be measured in terms of adequacy and fluency, but also length. Exemplary cases are the translation of document files, subtitles, and scripts for dubbing, where the output length should ideally be as close as possible to the length of the input text. This pa-per addresses for the first time, to the best of our knowledge, the problem of controlling the output length in NMT. We investigate two methods for biasing the output length with a transformer architecture: i) conditioning the output to a given target-source length-ratio class and ii) enriching the transformer positional embedding with length information. Our experiments show that both methods can induce the network to generate shorter translations, as well as acquiring inter- pretable linguistic skills.",Hong Kong,"Lakew, Surafel Melaku  and
Di Gangi, Mattia  and
Federico, Marcello",Proceedings of the 16th International Conference on Spoken Language Translation,,November 2-3,,Association for Computational Linguistics,Controlling the Output Length of Neural Machine Translation,https://aclanthology.org/2019.iwslt-1.31,2019,,,,,
852,inproceedings,das-chatterji-2019-identification,"Often sentences of correct news are either made biased towards a particular person or a group of persons or parties or maybe distorted to add some sentiment or importance in it. Engaged readers often are not able to extract the inherent meaning of such synthetic sentences. In Bengali, the news contents of the synthetic sentences are presented in such a rich way that it usually becomes difficult to identify the synthetic part of it. We have used machine learning algorithms to classify Bengali news sentences into synthetic and legitimate and then used some rule-based postprocessing on each of these models. Finally, we have developed a voting based combination of these models to build a hybrid model for Bengali synthetic sentence identification. This is a new task and therefore we could not compare it with any existing work in the field. Identification of such types of sentences may be used to improve the performance of identifying fake news and satire news. Thus, identifying molecular level biasness in news articles.","International Institute of Information Technology, Hyderabad, India","Das, Soma  and
Chatterji, Sanjay",Proceedings of the 16th International Conference on Natural Language Processing,,December,193--200,NLP Association of India,Identification of Synthetic Sentence in {B}engali News using Hybrid Approach,https://aclanthology.org/2019.icon-1.23,2019,,,,,
853,inproceedings,veres-2019-making,The schema.org initiative was designed to introduce machine readable metadata into the World Wide Web. This paper investigates conceptual biases in the schema through a mapping exercise between schema.org types and WordNet synsets. We create a mapping ontology which establishes the relationship between schema metadata types and the corresponding everyday concepts. This in turn can be used to enhance metadata annotation to include a more complete description of knowledge on the Web of data.,"Wroclaw, Poland","Veres, Csaba",Proceedings of the 10th Global Wordnet Conference,,July,1--9,Global Wordnet Association,Making Sense of schema.org with {W}ord{N}et,https://aclanthology.org/2019.gwc-1.1,2019,,,,,
854,inproceedings,chen-etal-2018-learning,"This paper introduces the task of {``}flipping{''} the bias of news articles: Given an article with a political bias (left or right), generate an article with the same topic but opposite bias. To study this task, we create a corpus with bias-labeled articles from \textit{all-sides.com}. As a first step, we analyze the corpus and discuss intrinsic characteristics of bias. They point to the main challenges of bias flipping, which in turn lead to a specific setting in the generation process. The paper in hand narrows down the general bias flipping task to focus on bias flipping for news article \textit{headlines}. A manual annotation of headlines from each side reveals that they are self-informative in general and often convey bias. We apply an autoencoder incorporating information from an article{'}s content to learn how to automatically flip the bias. From 200 generated headlines, 73 are classified as understandable by annotators, and 83 maintain the topic while having opposite bias. Insights from our analysis shed light on how to solve the main challenges of bias flipping.","Tilburg University, The Netherlands","Chen, Wei-Fan  and
Wachsmuth, Henning  and
Al-Khatib, Khalid  and
Stein, Benno",Proceedings of the 11th International Conference on Natural Language Generation,10.18653/v1/W18-6509,November,79--88,Association for Computational Linguistics,Learning to Flip the Bias of News Headlines,https://aclanthology.org/W18-6509,2018,,,,,
855,inproceedings,junczys-dowmunt-2018-microsofts,This paper describes the Microsoft submission to the WMT2018 news translation shared task. We participated in one language direction {--} English-German. Our system follows current best-practice and combines state-of-the-art models with new data filtering (dual conditional cross-entropy filtering) and sentence weighting methods. We trained fairly standard Transformer-big models with an updated version of Edinburgh{'}s training scheme for WMT2017 and experimented with different filtering schemes for Paracrawl. According to automatic metrics (BLEU) we reached the highest score for this subtask with a nearly 2 BLEU point margin over the next strongest system. Based on human evaluation we ranked first among constrained systems. We believe this is mostly caused by our data filtering/weighting regime.,"Belgium, Brussels","Junczys-Dowmunt, Marcin",Proceedings of the Third Conference on Machine Translation: Shared Task Papers,10.18653/v1/W18-6415,October,425--430,Association for Computational Linguistics,{M}icrosoft{'}s Submission to the {WMT}2018 News Translation Task: How {I} Learned to Stop Worrying and Love the Data,https://aclanthology.org/W18-6415,2018,,,,,
856,inproceedings,tebbifakhr-etal-2018-multi,"Recent approaches to the Automatic Post-editing (APE) of Machine Translation (MT) have shown that best results are obtained by neural multi-source models that correct the raw MT output by also considering information from the corresponding source sentence. To this aim, we present for the first time a neural multi-source APE model based on the Transformer architecture. Moreover, we employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics used for the task. These are the main features of our submissions to the WMT 2018 APE shared task, where we participated both in the PBSMT subtask (i.e. the correction of MT outputs from a phrase-based system) and in the NMT subtask (i.e. the correction of neural outputs). In the first subtask, our system improves over the baseline up to -5.3 TER and +8.23 BLEU points ranking second out of 11 submitted runs. In the second one, characterized by the higher quality of the initial translations, we report lower but statistically significant gains (up to -0.38 TER and +0.8 BLEU), ranking first out of 10 submissions.","Belgium, Brussels","Tebbifakhr, Amirhossein  and
Agrawal, Ruchit  and
Negri, Matteo  and
Turchi, Marco",Proceedings of the Third Conference on Machine Translation: Shared Task Papers,10.18653/v1/W18-6471,October,846--852,Association for Computational Linguistics,Multi-source transformer with combined losses for automatic post editing,https://aclanthology.org/W18-6471,2018,,,,,
857,inproceedings,littell-etal-2018-measuring,"The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a {``}clean{''} corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the \textit{only} sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task{---}translating the European Medicines Agency corpus (Tiedemann, 2009){---}scored among the best systems even in the 10M-word conditions.","Belgium, Brussels","Littell, Patrick  and
Larkin, Samuel  and
Stewart, Darlene  and
Simard, Michel  and
Goutte, Cyril  and
Lo, Chi-kiu",Proceedings of the Third Conference on Machine Translation: Shared Task Papers,10.18653/v1/W18-6480,October,900--907,Association for Computational Linguistics,Measuring sentence parallelism using Mahalanobis distances: The {NRC} unsupervised submissions to the {WMT}18 Parallel Corpus Filtering shared task,https://aclanthology.org/W18-6480,2018,,,,,
858,inproceedings,murray-chiang-2018-correcting,"We study two problems in neural machine translation (NMT). First, in beam search, whereas a wider beam should in principle help translation, it often hurts NMT. Second, NMT has a tendency to produce translations that are too short. Here, we argue that these problems are closely related and both rooted in label bias. We show that correcting the brevity problem almost eliminates the beam problem; we compare some commonly-used methods for doing this, finding that a simple per-word reward works well; and we introduce a simple and quick way to tune this reward using the perceptron algorithm.","Brussels, Belgium","Murray, Kenton  and
Chiang, David",Proceedings of the Third Conference on Machine Translation: Research Papers,10.18653/v1/W18-6322,October,212--223,Association for Computational Linguistics,Correcting Length Bias in Neural Machine Translation,https://aclanthology.org/W18-6322,2018,,,,,
859,inproceedings,shoemark-etal-2018-inducing,"Sociolinguistics is often concerned with how variants of a linguistic item (e.g., \textit{nothing} vs. \textit{nothin{'}}) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (\textit{football}, \textit{fitba}) along with their linguistic code (\textit{football}â†’British, \textit{fitba}â†’Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70{\%} for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.","Brussels, Belgium","Shoemark, Philippa  and
Kirby, James  and
Goldwater, Sharon",Proceedings of the 2018 {EMNLP} Workshop W-{NUT}: The 4th Workshop on Noisy User-generated Text,10.18653/v1/W18-6101,November,1--6,Association for Computational Linguistics,Inducing a lexicon of sociolinguistic variables from code-mixed text,https://aclanthology.org/W18-6101,2018,,,,,
860,inproceedings,fu-etal-2018-case,"Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. An individual corpus is often small, and the models may often be biased or overfitted to the corpus. We hypothesize that we can learn a better representation by combining multiple relation datasets. We attempt to use a shared encoder to learn the unified feature representation and to augment it with regularization by adversarial training. The additional corpora feeding the encoder can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both datasets.","Brussels, Belgium","Fu, Lisheng  and
Min, Bonan  and
Nguyen, Thien Huu  and
Grishman, Ralph",Proceedings of the 2018 {EMNLP} Workshop W-{NUT}: The 4th Workshop on Noisy User-generated Text,10.18653/v1/W18-6126,November,202--207,Association for Computational Linguistics,A Case Study on Learning a Unified Encoder of Relations,https://aclanthology.org/W18-6126,2018,,,,,
861,inproceedings,cardenas-zeman-2018-morphological,"We present a fairly complete morphological analyzer for Shipibo-Konibo, a low-resourced native language spoken in the Amazonian region of Peru. We resort to the robustness of finite-state systems in order to model the complex morphosyntax of the language. Evaluation over raw corpora shows promising coverage of grammatical phenomena, limited only by the scarce lexicon. We make this tool freely available so as to aid the production of annotated corpora and impulse further research in native languages of Peru.","Brussels, Belgium","Cardenas, Ronald  and
Zeman, Daniel","Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",10.18653/v1/W18-5815,October,131--139,Association for Computational Linguistics,A Morphological Analyzer for {S}hipibo-Konibo,https://aclanthology.org/W18-5815,2018,,,,,
862,inproceedings,petrovski-etal-2018-embedding,"Most of the world{'}s data is stored in relational databases. Accessing these requires specialized knowledge of the Structured Query Language (SQL), putting them out of the reach of many people. A recent research thread in Natural Language Processing (NLP) aims to alleviate this problem by automatically translating natural language questions into SQL queries. While the proposed solutions are a great start, they lack robustness and do not easily generalize: the methods require high quality descriptions of the database table columns, and the most widely used training dataset, WikiSQL, is heavily biased towards using those descriptions as part of the questions. In this work, we propose solutions to both problems: we entirely eliminate the need for column descriptions, by relying solely on their contents, and we augment the WikiSQL dataset by paraphrasing column names to reduce bias. We show that the accuracy of existing methods drops when trained on our augmented, column-agnostic dataset, and that our own method reaches state of the art accuracy, while relying on column contents only.","Brussels, Belgium","Petrovski, Bojan  and
Aguado, Ignacio  and
Hossmann, Andreea  and
Baeriswyl, Michael  and
Musat, Claudiu",Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI},10.18653/v1/W18-5710,October,67--73,Association for Computational Linguistics,Embedding Individual Table Columns for Resilient {SQL} Chatbots,https://aclanthology.org/W18-5710,2018,,,,,
863,inproceedings,wang-etal-2018-glue,"Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.","Brussels, Belgium","Wang, Alex  and
Singh, Amanpreet  and
Michael, Julian  and
Hill, Felix  and
Levy, Omer  and
Bowman, Samuel",Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},10.18653/v1/W18-5446,November,353--355,Association for Computational Linguistics,{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,https://aclanthology.org/W18-5446,2018,,,,,
864,inproceedings,galescu-etal-2018-cogent,"The bulk of current research in dialogue systems is focused on fairly simple task models, primarily state-based. Progress on developing dialogue systems for more complex tasks has been limited by the lack generic toolkits to build from. In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving. We implemented the model in a dialogue system shell (Cogent) that al-lows developers to plug in problem-solving agents to create dialogue systems in new domains. The Cogent shell has now been used by several independent teams of researchers to develop dialogue systems in different domains, with varied lexicons and interaction style, each with their own problem-solving back-end. We believe this to be the first practical demonstration of the feasibility of a CPS-based dialogue system shell.","Melbourne, Australia","Galescu, Lucian  and
Teng, Choh Man  and
Allen, James  and
Perera, Ian",Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,10.18653/v1/W18-5048,July,400--409,Association for Computational Linguistics,{C}ogent: A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model,https://aclanthology.org/W18-5048,2018,,,,,
865,inproceedings,steimel-2018-part,"Luyia is a macrolanguage in central Kenya. The Luyia languages, like other Bantu languages, have a complex morphological system. This system can be leveraged to aid in part of speech tagging. Bag-of-characters taggers trained on a source Luyia language can be applied directly to another Luyia language with some degree of success. In addition, mixing data from the target language with data from the source language does produce more accurate predictive models compared to models trained on just the target language data when the training set size is small. However, for both of these tagging tasks, models involving the more distantly related language, Tiriki, are better at predicting part of speech tags for Wanga data. The models incorporating Bukusu data are not as successful despite the closer relationship between Bukusu and Wanga. Overlapping vocabulary between the Wanga and Tiriki corpora as well as a bias towards open class words help Tiriki outperform Bukusu.","Santa Fe, New Mexico, USA","Steimel, Kenneth","Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",,August,46--54,Association for Computational Linguistics,Part of Speech Tagging in {L}uyia: A {B}antu Macrolanguage,https://aclanthology.org/W18-3905,2018,,,,,
866,inproceedings,baldwin-2018-language,"In this talk, I will first present recent work on domain debiasing in the context of language identification, then discuss a new line of work on language variety analysis in the form of dialect map generation. Finally, I will reflect on the interplay between time and space on language variation, and speculate on how these can be captured in a single model.","Santa Fe, New Mexico, USA","Baldwin, Timothy","Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",,August,76,Association for Computational Linguistics,"Language and the Shifting Sands of Domain, Space and Time (Invited Talk)",https://aclanthology.org/W18-3908,2018,,,,,
867,inproceedings,chandrasekaran-kan-2018-countering,"We systematically confirm that instructors are strongly influenced by the user interface presentation of Massive Online Open Course (MOOC) discussion forums. In a large scale dataset, we conclusively show that instructor interventions exhibit strong position bias, as measured by the position where the thread appeared on the user interface at the time of intervention. We measure and remove this bias, enabling unbiased statistical modelling and evaluation. We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2{\%} in F1 and 24.4{\%} in recall on average.","Melbourne, Australia","Chandrasekaran, Muthu Kumar  and
Kan, Min-Yen",Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,10.18653/v1/W18-3720,July,135--142,Association for Computational Linguistics,Countering Position Bias in Instructor Interventions in {MOOC} Discussion Forums,https://aclanthology.org/W18-3720,2018,,,,,
868,inproceedings,chandu-etal-2018-code,"Code-Mixing (CM) is the phenomenon of alternating between two or more languages which is prevalent in bi- and multi-lingual communities. Most NLP applications today are still designed with the assumption of a single interaction language and are most likely to break given a CM utterance with multiple languages mixed at a morphological, phrase or sentence level. For example, popular commercial search engines do not yet fully understand the intents expressed in CM queries. As a first step towards fostering research which supports CM in NLP applications, we systematically crowd-sourced and curated an evaluation dataset for factoid question answering in three CM languages - Hinglish (Hindi+English), Tenglish (Telugu+English) and Tamlish (Tamil+English) which belong to two language families (Indo-Aryan and Dravidian). We share the details of our data collection process, techniques which were used to avoid inducing lexical bias amongst the crowd workers and other CM specific linguistic properties of the dataset. Our final dataset, which is available freely for research purposes, has 1,694 Hinglish, 2,848 Tamlish and 1,391 Tenglish factoid questions and their answers. We discuss the techniques used by the participants for the first edition of this ongoing challenge.","Melbourne, Australia","Chandu, Khyathi  and
Loginova, Ekaterina  and
Gupta, Vishal  and
van Genabith, Josef  and
Neumann, G{\""u}nter  and
Chinnakotla, Manoj  and
Nyberg, Eric  and
Black, Alan W.",Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,10.18653/v1/W18-3204,July,29--38,Association for Computational Linguistics,Code-Mixed Question Answering Challenge: Crowd-sourcing Data and Techniques,https://aclanthology.org/W18-3204,2018,,,,,
869,inproceedings,geetha-etal-2018-tackling,"Named Entity Recognition plays a major role in several downstream applications in NLP. Though this task has been heavily studied in formal monolingual texts and also noisy texts like Twitter data, it is still an emerging task in code-switched (CS) content on social media. This paper describes our participation in the shared task of NER on code-switched data for Spanglish (Spanish + English) and Arabish (Arabic + English). In this paper we describe models that intuitively developed from the data for the shared task Named Entity Recognition on Code-switched Data. Owing to the sparse and non-linear relationships between words in Twitter data, we explored neural architectures that are capable of non-linearities fairly well. In specific, we trained character level models and word level models based on Bidirectional LSTMs (Bi-LSTMs) to perform sequential tagging. We train multiple models to identify nominal mentions and subsequently use this information to predict the labels of named entity in a sequence. Our best model is a character level model along with word level pre-trained multilingual embeddings that gave an F-score of 56.72 in Spanglish and a word level model that gave an F-score of 65.02 in Arabish on the test data.","Melbourne, Australia","Geetha, Parvathy  and
Chandu, Khyathi  and
Black, Alan W",Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,10.18653/v1/W18-3217,July,126--131,Association for Computational Linguistics,Tackling Code-Switched {NER}: Participation of {CMU},https://aclanthology.org/W18-3217,2018,,,,,
870,inproceedings,rondeau-hazen-2018-systematic,"We analyzed the outputs of multiple question answering (QA) models applied to the Stanford Question Answering Dataset (SQuAD) to identify the core challenges for QA systems on this data set. Through an iterative process, challenging aspects were hypothesized through qualitative analysis of the common error cases. A classifier was then constructed to predict whether SQuAD test examples were likely to be difficult for systems to answer based on features associated with the hypothesized aspects. The classifier{'}s performance was used to accept or reject each aspect as an indicator of difficulty. With this approach, we ensured that our hypotheses were systematically tested and not simply accepted based on our pre-existing biases. Our explanations are not accepted based on human evaluation of individual examples. This process also enabled us to identify the primary QA strategy learned by the models, i.e., systems determined the acceptable answer type for a question and then selected the acceptable answer span of that type containing the highest density of words present in the question within its local vicinity in the passage.","Melbourne, Australia","Rondeau, Marc-Antoine  and
Hazen, T. J.",Proceedings of the Workshop on Machine Reading for Question Answering,10.18653/v1/W18-2602,July,12--20,Association for Computational Linguistics,Systematic Error Analysis of the {S}tanford Question Answering Dataset,https://aclanthology.org/W18-2602,2018,,,,,
871,inproceedings,wadhwa-etal-2018-comparative,"The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.","Melbourne, Australia","Wadhwa, Soumya  and
Chandu, Khyathi  and
Nyberg, Eric",Proceedings of the Workshop on Machine Reading for Question Answering,10.18653/v1/W18-2610,July,89--97,Association for Computational Linguistics,Comparative Analysis of Neural {QA} models on {SQ}u{AD},https://aclanthology.org/W18-2610,2018,,,,,
872,inproceedings,yang-etal-2018-adaptations,"Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU. However, bias may appear when these metrics are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the metrics to better correlate $n$-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.","Melbourne, Australia","Yang, An  and
Liu, Kai  and
Liu, Jing  and
Lyu, Yajuan  and
Li, Sujian",Proceedings of the Workshop on Machine Reading for Question Answering,10.18653/v1/W18-2611,July,98--104,Association for Computational Linguistics,Adaptations of {ROUGE} and {BLEU} to Better Evaluate Machine Reading Comprehension Task,https://aclanthology.org/W18-2611,2018,,,,,
873,inproceedings,dobnik-etal-2018-exploring,"The challenge for computational models of spatial descriptions for situated dialogue systems is the integration of information from different modalities. The semantics of spatial descriptions are grounded in at least two sources of information: (i) a geometric representation of space and (ii) the functional interaction of related objects that. We train several neural language models on descriptions of scenes from a dataset of image captions and examine whether the functional or geometric bias of spatial descriptions reported in the literature is reflected in the estimated perplexity of these models. The results of these experiments have implications for the creation of models of spatial lexical semantics for human-robot dialogue systems. Furthermore, they also provide an insight into the kinds of the semantic knowledge captured by neural language models trained on spatial descriptions, which has implications for image captioning systems.",New Orleans,"Dobnik, Simon  and
Ghanimifard, Mehdi  and
Kelleher, John",Proceedings of the First International Workshop on Spatial Language Understanding,10.18653/v1/W18-1401,June,1--11,Association for Computational Linguistics,Exploring the Functional and Geometric Bias of Spatial Relations Using Neural Language Models,https://aclanthology.org/W18-1401,2018,,,,,
874,inproceedings,hovy-2018-social,"Over the years, natural language processing has increasingly focused on tasks that can be solved by statistical models, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the models, but have narrowed our focus and biased the tools demographically. However, with the increased availability of data sets including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of NLP to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.","New Orleans, Louisiana, USA","Hovy, Dirk","Proceedings of the Second Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media",10.18653/v1/W18-1106,June,42--49,Association for Computational Linguistics,The Social and the Neural Network: How to Make Natural Language Processing about People again,https://aclanthology.org/W18-1106,2018,,,,,
875,inproceedings,cercas-curry-rieser-2018-metoo,"Conversational AI systems, such as Amazon{'}s Alexa, are rapidly developing from purely transactional systems to social chatbots, which can respond to a wide variety of user requests. In this article, we establish how current state-of-the-art conversational systems react to inappropriate requests, such as bullying and sexual harassment on the part of the user, by collecting and analysing the novel {\#}MeTooAlexa corpus. Our results show that commercial systems mainly avoid answering, while rule-based chatbots show a variety of behaviours and often deflect. Data-driven systems, on the other hand, are often non-coherent, but also run the risk of being interpreted as flirtatious and sometimes react with counter-aggression. This includes our own system, trained on {``}clean{''} data, which suggests that inappropriate system behaviour is not caused by data bias.","New Orleans, Louisiana, USA","Cercas Curry, Amanda  and
Rieser, Verena",Proceedings of the Second {ACL} Workshop on Ethics in Natural Language Processing,10.18653/v1/W18-0802,June,7--14,Association for Computational Linguistics,{\#}{M}e{T}oo {A}lexa: How Conversational Systems Respond to Sexual Harassment,https://aclanthology.org/W18-0802,2018,,,,,
876,inproceedings,loaiciga-etal-2018-event,"Anaphora resolution systems require both an enumeration of possible candidate antecedents and an identification process of the antecedent. This paper focuses on (i) the impact of the form of referring expression on entity-vs-event preferences and (ii) how properties of the passage interact with referential form. Two crowd-sourced story-continuation experiments were conducted, using constructed and naturally-occurring passages, to see how participants interpret \textit{It} and \textit{This} pronouns following a context sentence that makes available event and entity referents. Our participants show a strong, but not categorical, bias to use \textit{This} to refer to events and \textit{It} to refer to entities. However, these preferences vary with passage characteristics such as verb class (a proxy in our constructed examples for the number of explicit and implicit entities) and more subtle author intentions regarding subsequent re-mention (the original event-vs-entity re-mention of our corpus items).","New Orleans, Louisiana","Lo{\'a}iciga, Sharid  and
Bevacqua, Luca  and
Rohde, Hannah  and
Hardmeier, Christian","Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference",10.18653/v1/W18-0711,June,97--103,Association for Computational Linguistics,Event versus entity co-reference: Effects of context and form of referring expression,https://aclanthology.org/W18-0711,2018,,,,,
877,inproceedings,loveys-etal-2018-cross,"Depression is a global mental health condition that affects all cultures. Despite this, the way depression is expressed varies by culture. Uptake of machine learning technology for diagnosing mental health conditions means that increasingly more depression classifiers are created from online language data. Yet, culture is rarely considered as a factor affecting online language in this literature. This study explores cultural differences in online language data of users with depression. Written language data from 1,593 users with self-reported depression from the online peer support community 7 Cups of Tea was analyzed using the Linguistic Inquiry and Word Count (LIWC), topic modeling, data visualization, and other techniques. We compared the language of users identifying as White, Black or African American, Hispanic or Latino, and Asian or Pacific Islander. Exploratory analyses revealed cross-cultural differences in depression expression in online language data, particularly in relation to emotion expression, cognition, and functioning. The results have important implications for avoiding depression misclassification from machine-driven assessments when used in a clinical setting, and for avoiding inadvertent cultural biases in this line of research more broadly.","New Orleans, LA","Loveys, Kate  and
Torrez, Jonathan  and
Fine, Alex  and
Moriarty, Glen  and
Coppersmith, Glen",Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,10.18653/v1/W18-0608,June,78--87,Association for Computational Linguistics,Cross-cultural differences in language markers of depression online,https://aclanthology.org/W18-0608,2018,,,,,
878,inproceedings,bryant-briscoe-2018-language,"Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (âˆ¼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.","New Orleans, Louisiana","Bryant, Christopher  and
Briscoe, Ted",Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,10.18653/v1/W18-0529,June,247--253,Association for Computational Linguistics,Language Model Based Grammatical Error Correction without Annotated Training Data,https://aclanthology.org/W18-0529,2018,,,,,
879,inproceedings,kiritchenko-mohammad-2018-examining,"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 {`}Affect in Tweets{'}. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.","New Orleans, Louisiana","Kiritchenko, Svetlana  and
Mohammad, Saif",Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,10.18653/v1/S18-2005,June,43--53,Association for Computational Linguistics,Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems,https://aclanthology.org/S18-2005,2018,,,,,
880,inproceedings,vu-shwartz-2018-integrating,"Supervised distributional methods are applied successfully in lexical entailment, but recent work questioned whether these methods actually learn a relation between two words. Specifically, Levy et al. (2015) claimed that linear classifiers learn only separate properties of each word. We suggest a cheap and easy way to boost the performance of these methods by integrating multiplicative features into commonly used representations. We provide an extensive evaluation with different classifiers and evaluation setups, and suggest a suitable evaluation setup for the task, eliminating biases existing in previous ones.","New Orleans, Louisiana","Vu, Tu  and
Shwartz, Vered",Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,10.18653/v1/S18-2020,June,160--166,Association for Computational Linguistics,Integrating Multiplicative Features into Supervised Distributional Methods for Lexical Entailment,https://aclanthology.org/S18-2020,2018,,,,,
881,inproceedings,mohammad-etal-2018-semeval,"We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring the affectual state of a person from their tweet. For each task, we created labeled data from English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. emotion intensity ordinal classification, 3. valence (sentiment) regression, 4. valence ordinal classification, and 5. emotion classification. Seventy-five teams (about 200 team members) participated in the shared task. We summarize the methods, resources, and tools used by the participating teams, with a focus on the techniques and resources that are particularly useful. We also analyze systems for consistent bias towards a particular race or gender. The data is made freely available to further improve our understanding of how people convey emotions through language.","New Orleans, Louisiana","Mohammad, Saif  and
Bravo-Marquez, Felipe  and
Salameh, Mohammad  and
Kiritchenko, Svetlana",Proceedings of The 12th International Workshop on Semantic Evaluation,10.18653/v1/S18-1001,June,1--17,Association for Computational Linguistics,{S}em{E}val-2018 Task 1: Affect in Tweets,https://aclanthology.org/S18-1001,2018,,,,,
882,article,liu-lapata-2018-learning,"In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.","Cambridge, MA","Liu, Yang  and
Lapata, Mirella",,10.1162/tacl_a_00005,,63--75,MIT Press,Learning Structured Text Representations,https://aclanthology.org/Q18-1005,2018,Transactions of the Association for Computational Linguistics,6,,,
883,article,roy-roth-2018-mapping,"Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, supporting interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.","Cambridge, MA","Roy, Subhro  and
Roth, Dan",,10.1162/tacl_a_00012,,159--172,MIT Press,Mapping to Declarative Knowledge for Word Problem Solving,https://aclanthology.org/Q18-1012,2018,Transactions of the Association for Computational Linguistics,6,,,
884,article,paun-etal-2018-comparing,"The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation.","Cambridge, MA","Paun, Silviu  and
Carpenter, Bob  and
Chamberlain, Jon  and
Hovy, Dirk  and
Kruschwitz, Udo  and
Poesio, Massimo",,10.1162/tacl_a_00040,,571--585,MIT Press,Comparing {B}ayesian Models of Annotation,https://aclanthology.org/Q18-1040,2018,Transactions of the Association for Computational Linguistics,6,,,
885,article,bender-friedman-2018-data,"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.","Cambridge, MA","Bender, Emily M.  and
Friedman, Batya",,10.1162/tacl_a_00041,,587--604,MIT Press,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science,https://aclanthology.org/Q18-1041,2018,Transactions of the Association for Computational Linguistics,6,,,
886,article,webster-etal-2018-mind,"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.","Cambridge, MA","Webster, Kellie  and
Recasens, Marta  and
Axelrod, Vera  and
Baldridge, Jason",,10.1162/tacl_a_00240,,605--617,MIT Press,Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns,https://aclanthology.org/Q18-1042,2018,Transactions of the Association for Computational Linguistics,6,,,
887,inproceedings,qin-etal-2018-automatic,"Comments of online articles provide extended views and improve user engagement. Automatically making comments thus become a valuable functionality for online forums, intelligent chatbots, etc. This paper proposes the new task of automatic article commenting, and introduces a large-scale Chinese dataset with millions of real comments and a human-annotated subset characterizing the comments{'} varying quality. Incorporating the human bias of comment quality, we further develop automatic metrics that generalize a broad set of popular reference-based metrics and exhibit greatly improved correlations with human evaluations.","Melbourne, Australia","Qin, Lianhui  and
Liu, Lemao  and
Bi, Wei  and
Wang, Yan  and
Liu, Xiaojiang  and
Hu, Zhiting  and
Zhao, Hai  and
Shi, Shuming",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2025,July,151--156,Association for Computational Linguistics,Automatic Article Commenting: the Task and Dataset,https://aclanthology.org/P18-2025,2018,,,,,
888,inproceedings,michel-neubig-2018-extreme,"Every person speaks or writes their own flavor of their native language, influenced by a number of factors: the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the system should perform translation, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on TED talks in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.","Melbourne, Australia","Michel, Paul  and
Neubig, Graham",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2050,July,312--318,Association for Computational Linguistics,Extreme Adaptation for Personalized Neural Machine Translation,https://aclanthology.org/P18-2050,2018,,,,,
889,inproceedings,fried-klein-2018-policy,"Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser{'}s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.","Melbourne, Australia","Fried, Daniel  and
Klein, Dan",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2075,July,469--476,Association for Computational Linguistics,Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing,https://aclanthology.org/P18-2075,2018,,,,,
890,inproceedings,zaremoodi-etal-2018-adaptive,"Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is Multi-Task Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the Seq2Seq transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple {``}blocks{''} along with a trainable {``}routing network{''}. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.","Melbourne, Australia","Zaremoodi, Poorya  and
Buntine, Wray  and
Haffari, Gholamreza",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2104,July,656--661,Association for Computational Linguistics,Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation,https://aclanthology.org/P18-2104,2018,,,,,
891,inproceedings,ni-mcauley-2018-personalized,"In this paper, we focus on the problem of building assistive systems that can help users to write reviews. We cast this problem using an encoder-decoder framework that generates personalized reviews by expanding short phrases (e.g. review summaries, product titles) provided as input to the system. We incorporate aspect-level information via an aspect encoder that learns aspect-aware user and item representations. An attention fusion layer is applied to control generation by attending on the outputs of multiple encoders. Experimental results show that our model successfully learns representations capable of generating coherent and diverse reviews. In addition, the learned aspect-aware representations discover those aspects that users are more inclined to discuss and bias the generated text toward their personalized aspect preferences.","Melbourne, Australia","Ni, Jianmo  and
McAuley, Julian",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2112,July,706--711,Association for Computational Linguistics,Personalized Review Generation By Expanding Phrases and Attending on Aspect-Aware Representations,https://aclanthology.org/P18-2112,2018,,,,,
892,inproceedings,sharma-etal-2018-tackling,"The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning. There have been a variety of models tackling the SCT so far. Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset. In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task. Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases. We benchmark a few models on the new dataset and show that the top-performing model on the original SCT dataset fails to keep up its performance. Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.","Melbourne, Australia","Sharma, Rishi  and
Allen, James  and
Bakhshandeh, Omid  and
Mostafazadeh, Nasrin",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P18-2119,July,752--757,Association for Computational Linguistics,Tackling the Story Ending Biases in The Story Cloze Test,https://aclanthology.org/P18-2119,2018,,,,,
893,inproceedings,vilnis-etal-2018-probabilistic,"Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.","Melbourne, Australia","Vilnis, Luke  and
Li, Xiang  and
Murty, Shikhar  and
McCallum, Andrew",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1025,July,263--272,Association for Computational Linguistics,Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures,https://aclanthology.org/P18-1025,2018,,,,,
894,inproceedings,choshen-abend-2018-inherent,"The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.","Melbourne, Australia","Choshen, Leshem  and
Abend, Omri",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1059,July,632--642,Association for Computational Linguistics,Inherent Biases in Reference-based Evaluation for Grammatical Error Correction,https://aclanthology.org/P18-1059,2018,,,,,
895,inproceedings,chaganty-etal-2018-price,"For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13{\%} cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks{---}the automatic metric and the prompt shown to human evaluators{---}both of which need to be improved to obtain greater cost savings.","Melbourne, Australia","Chaganty, Arun  and
Mussmann, Stephen  and
Liang, Percy",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1060,July,643--653,Association for Computational Linguistics,The price of debiasing automatic metrics in natural language evalaution,https://aclanthology.org/P18-1060,2018,,,,,
896,inproceedings,johnson-goldwasser-2018-classification,"Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.","Melbourne, Australia","Johnson, Kristen  and
Goldwasser, Dan",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1067,July,720--730,Association for Computational Linguistics,Classification of Moral Foundations in Microblog Political Discourse,https://aclanthology.org/P18-1067,2018,,,,,
897,inproceedings,le-titov-2018-improving,"Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.","Melbourne, Australia","Le, Phong  and
Titov, Ivan",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1148,July,1595--1604,Association for Computational Linguistics,Improving Entity Linking by Modeling Latent Relations between Mentions,https://aclanthology.org/P18-1148,2018,,,,,
898,inproceedings,kreutzer-etal-2018-reliability,"We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator Î±-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.","Melbourne, Australia","Kreutzer, Julia  and
Uyheng, Joshua  and
Riezler, Stefan",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1165,July,1777--1788,Association for Computational Linguistics,Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning,https://aclanthology.org/P18-1165,2018,,,,,
899,inproceedings,xin-etal-2018-batch,"Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.","Melbourne, Australia","Xin, Xin  and
Yuan, Fajie  and
He, Xiangnan  and
Jose, Joemon M.",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1172,July,1853--1862,Association for Computational Linguistics,Batch {IS} {NOT} Heavy: Learning Word Representations From All Samples,https://aclanthology.org/P18-1172,2018,,,,,
900,inproceedings,shen-etal-2018-nash,"Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled \textit{ad-hoc}. In this paper, we present an \textit{end-to-end} Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as \textit{Bernoulli} latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and \textit{rate-distortion theory}, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both \textit{unsupervised} and \textit{supervised} scenarios.","Melbourne, Australia","Shen, Dinghan  and
Su, Qinliang  and
Chapfuwa, Paidamoyo  and
Wang, Wenlin  and
Wang, Guoyin  and
Henao, Ricardo  and
Carin, Lawrence",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1190,July,2041--2050,Association for Computational Linguistics,{NASH}: Toward End-to-End Neural Architecture for Generative Semantic Hashing,https://aclanthology.org/P18-1190,2018,,,,,
901,inproceedings,elbayad-etal-2018-token,"Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from {'}exposure bias{'}: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.","Melbourne, Australia","Elbayad, Maha  and
Besacier, Laurent  and
Verbeek, Jakob",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1195,July,2094--2103,Association for Computational Linguistics,Token-level and sequence-level loss smoothing for {RNN} language models,https://aclanthology.org/P18-1195,2018,,,,,
902,inproceedings,peng-etal-2018-deep,"Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.","Melbourne, Australia","Peng, Baolin  and
Li, Xiujun  and
Gao, Jianfeng  and
Liu, Jingjing  and
Wong, Kam-Fai",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1203,July,2182--2192,Association for Computational Linguistics,{D}eep {D}yna-{Q}: Integrating Planning for Task-Completion Dialogue Policy Learning,https://aclanthology.org/P18-1203,2018,,,,,
903,inproceedings,rohde-etal-2018-discourse,"Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments {--} some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing.","Melbourne, Australia","Rohde, Hannah  and
Johnson, Alexander  and
Schneider, Nathan  and
Webber, Bonnie",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1210,July,2257--2267,Association for Computational Linguistics,Discourse Coherence: Concurrent Explicit and Implicit Relations,https://aclanthology.org/P18-1210,2018,,,,,
904,inproceedings,lu-etal-2018-object,"We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document, and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL), reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.","Melbourne, Australia","Lu, Zhengdong  and
Liu, Xianggen  and
Cui, Haotian  and
Yan, Yukun  and
Zheng, Daqi",Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P18-1253,July,2717--2726,Association for Computational Linguistics,Object-oriented Neural Programming ({OONP}) for Document Understanding,https://aclanthology.org/P18-1253,2018,,,,,
905,inproceedings,tsvetkov-etal-2018-socially,"As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University.","New Orleans, Louisiana","Tsvetkov, Yulia  and
Prabhakaran, Vinodkumar  and
Voigt, Rob",Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,10.18653/v1/N18-6005,June,24--26,Association for Computational Linguistics,Socially Responsible {NLP},https://aclanthology.org/N18-6005,2018,,,,,
906,inproceedings,ruder-etal-2018-360deg,"The proliferation of fake news and filter bubbles makes it increasingly difficult to form an unbiased, balanced opinion towards a topic. To ameliorate this, we propose 360{\mbox{$^\circ$}} Stance Detection, a tool that aggregates news with multiple perspectives on a topic. It presents them on a spectrum ranging from support to opposition, enabling the user to base their opinion on multiple pieces of diverse evidence.","New Orleans, Louisiana","Ruder, Sebastian  and
Glover, John  and
Mehrabani, Afshin  and
Ghaffari, Parsa",Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,10.18653/v1/N18-5007,June,31--35,Association for Computational Linguistics,360{\mbox{$^\circ$}} Stance Detection,https://aclanthology.org/N18-5007,2018,,,,,
907,inproceedings,shin-doyle-2018-alignment,"Conversation is a joint social process, with participants cooperating to exchange information. This process is helped along through linguistic alignment: participants{'} adoption of each other{'}s word use. This alignment is robust, appearing many settings, and is nearly always positive. We create an alignment model for examining alignment in Twitter conversations across antagonistic groups. This model finds that some word categories, specifically pronouns used to establish group identity and common ground, are negatively aligned. This negative alignment is observed despite other categories, which are less related to the group dynamics, showing the standard positive alignment. This suggests that alignment is strongly biased toward cooperative alignment, but that different linguistic features can show substantially different behaviors.","New Orleans, Louisiana, USA","Shin, Hagyeong  and
Doyle, Gabriel",Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,10.18653/v1/N18-4001,June,1--8,Association for Computational Linguistics,"Alignment, Acceptance, and Rejection of Group Identities in Online Political Discourse",https://aclanthology.org/N18-4001,2018,,,,,
908,inproceedings,rudinger-etal-2018-gender,"We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these {``}Winogender schemas,{''} we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.","New Orleans, Louisiana","Rudinger, Rachel  and
Naradowsky, Jason  and
Leonard, Brian  and
Van Durme, Benjamin","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2002,June,8--14,Association for Computational Linguistics,Gender Bias in Coreference Resolution,https://aclanthology.org/N18-2002,2018,,,,,
909,inproceedings,zhao-etal-2018-gender,"In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.","New Orleans, Louisiana","Zhao, Jieyu  and
Wang, Tianlu  and
Yatskar, Mark  and
Ordonez, Vicente  and
Chang, Kai-Wei","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2003,June,15--20,Association for Computational Linguistics,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,https://aclanthology.org/N18-2003,2018,,,,,
910,inproceedings,dhingra-etal-2018-neural,"Many problems in NLP require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets {--} Wikihop, LAMBADA and the bAbi AI tasks {--} with large gains when training data is scarce.","New Orleans, Louisiana","Dhingra, Bhuwan  and
Jin, Qiao  and
Yang, Zhilin  and
Cohen, William  and
Salakhutdinov, Ruslan","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2007,June,42--48,Association for Computational Linguistics,Neural Models for Reasoning over Multiple Mentions Using Coreference,https://aclanthology.org/N18-2007,2018,,,,,
911,inproceedings,madhyastha-etal-2018-defoiling,"We address the task of detecting foiled image captions, i.e. identifying whether a caption contains a word that has been deliberately replaced by a semantically similar word, thus rendering it inaccurate with respect to the image being described. Solving this problem should in principle require a fine-grained understanding of images to detect subtle perturbations in captions. In such contexts, encoding sufficiently descriptive image information becomes a key challenge. In this paper, we demonstrate that it is possible to solve this task using simple, interpretable yet powerful representations based on explicit object information over multilayer perceptron models. Our models achieve state-of-the-art performance on a recently published dataset, with scores exceeding those achieved by humans on the task. We also measure the upper-bound performance of our models using gold standard annotations. Our study and analysis reveals that the simpler model performs well even without image information, suggesting that the dataset contains strong linguistic bias.","New Orleans, Louisiana","Madhyastha, Pranava Swaroop  and
Wang, Josiah  and
Specia, Lucia","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2069,June,433--438,Association for Computational Linguistics,Defoiling Foiled Image Captions,https://aclanthology.org/N18-2069,2018,,,,,
912,inproceedings,li-etal-2018-whats,"Most real world language problems require learning from heterogenous corpora, raising the problem of learning robust models which generalise well to both similar (\textit{in domain}) and dissimilar (\textit{out of domain}) instances to those seen in training. This requires learning an underlying task, while not learning irrelevant signals and biases specific to individual domains. We propose a novel method to optimise both in- and out-of-domain accuracy based on joint learning of a structured neural model with domain-specific and domain-general components, coupled with adversarial training for domain. Evaluating on multi-domain language identification and multi-domain sentiment analysis, we show substantial improvements over standard domain adaptation techniques, and domain-adversarial training.","New Orleans, Louisiana","Li, Yitong  and
Baldwin, Timothy  and
Cohn, Trevor","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2076,June,474--479,Association for Computational Linguistics,What{'}s in a Domain? Learning Domain-Robust Text Representations using Adversarial Training,https://aclanthology.org/N18-2076,2018,,,,,
913,inproceedings,marcheggiani-etal-2018-exploiting,"Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of machine translation methods. In this work, we are the first to incorporate information about predicate-argument structure of source sentences (namely, semantic-role representations) into neural machine translation. We use Graph Convolutional Networks (GCNs) to inject a semantic bias into sentence encoders and achieve improvements in BLEU scores over the linguistic-agnostic and syntax-aware versions on the English{--}German language pair.","New Orleans, Louisiana","Marcheggiani, Diego  and
Bastings, Jasmijn  and
Titov, Ivan","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2078,June,486--492,Association for Computational Linguistics,Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks,https://aclanthology.org/N18-2078,2018,,,,,
914,inproceedings,cotterell-etal-2018-languages,"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.","New Orleans, Louisiana","Cotterell, Ryan  and
Mielke, Sabrina J.  and
Eisner, Jason  and
Roark, Brian","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2085,June,536--541,Association for Computational Linguistics,Are All Languages Equally Hard to Language-Model?,https://aclanthology.org/N18-2085,2018,,,,,
915,inproceedings,cirik-etal-2018-visual,"We present an empirical analysis of state-of-the-art systems for referring expression recognition {--} the task of identifying the object in an image referred to by a natural language expression {--} with the goal of gaining insight into how these systems reason about language and vision. Surprisingly, we find strong evidence that even sophisticated and linguistically-motivated models for this task may ignore linguistic structure, instead relying on shallow correlations introduced by unintended biases in the data selection and annotation process. For example, we show that a system trained and tested on the input image without the input referring expression can achieve a precision of 71.2{\%} in top-2 predictions. Furthermore, a system that predicts only the object category given the input can achieve a precision of 84.2{\%} in top-2 predictions. These surprisingly positive results for what should be deficient prediction scenarios suggest that careful analysis of what our models are learning {--} and further, how our data is constructed {--} is critical as we seek to make substantive progress on grounded language tasks.","New Orleans, Louisiana","Cirik, Volkan  and
Morency, Louis-Philippe  and
Berg-Kirkpatrick, Taylor","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",10.18653/v1/N18-2123,June,781--787,Association for Computational Linguistics,Visual Referring Expression Recognition: What Do Systems Actually Learn?,https://aclanthology.org/N18-2123,2018,,,,,
916,inproceedings,amorim-etal-2018-automated,"Studies in Social Sciences have revealed that when people evaluate someone else, their evaluations often reflect their biases. As a result, rater bias may introduce highly subjective factors that make their evaluations inaccurate. This may affect automated essay scoring models in many ways, as these models are typically designed to model (potentially biased) essay raters. While there is sizeable literature on rater effects in general settings, it remains unknown how rater bias affects automated essay scoring. To this end, we present a new annotated corpus containing essays and their respective scores. Different from existing corpora, our corpus also contains comments provided by the raters in order to ground their scores. We present features to quantify rater bias based on their comments, and we found that rater bias plays an important role in automated essay scoring. We investigated the extent to which rater bias affects models based on hand-crafted features. Finally, we propose to rectify the training set by removing essays associated with potentially biased scores while learning the scoring model.","New Orleans, Louisiana","Amorim, Evelin  and
Can{\c{c}}ado, Marcia  and
Veloso, Adriano","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",10.18653/v1/N18-1021,June,229--237,Association for Computational Linguistics,Automated Essay Scoring in the Presence of Biased Ratings,https://aclanthology.org/N18-1021,2018,,,,,
917,inproceedings,wu-etal-2018-reinforced,"Co-training is a popular semi-supervised learning framework to utilize a large amount of unlabeled data in addition to a small labeled set. Co-training methods exploit predicted labels on the unlabeled data and select samples based on prediction confidence to augment the training. However, the selection of samples in existing co-training methods is based on a predetermined policy, which ignores the sampling bias between the unlabeled and the labeled subsets, and fails to explore the data space. In this paper, we propose a novel method, Reinforced Co-Training, to select high-quality unlabeled samples to better co-train on. More specifically, our approach uses Q-learning to learn a data selection policy with a small labeled dataset, and then exploits this policy to train the co-training classifiers automatically. Experimental results on clickbait detection and generic text classification tasks demonstrate that our proposed method can obtain more accurate text classification results.","New Orleans, Louisiana","Wu, Jiawei  and
Li, Lei  and
Wang, William Yang","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",10.18653/v1/N18-1113,June,1252--1262,Association for Computational Linguistics,Reinforced Co-Training,https://aclanthology.org/N18-1113,2018,,,,,
918,inproceedings,yang-etal-2018-improving,"This paper proposes an approach for applying GANs to NMT. We build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generator and a discriminator. The generator aims to generate sentences which are hard to be discriminated from human-translated sentences ( i.e., the golden target sentences); And the discriminator makes efforts to discriminate the machine-generated sentences from human-translated ones. The two sub models play a mini-max game and achieve the win-win situation when they reach a Nash Equilibrium. Additionally, the static sentence-level BLEU is utilized as the reinforced objective for the generator, which biases the generation towards high BLEU points. During training, both the dynamic discriminator and the static BLEU objective are employed to evaluate the generated sentences and feedback the evaluations to guide the learning of the generator. Experimental results show that the proposed model consistently outperforms the traditional RNNSearch and the newly emerged state-of-the-art Transformer on English-German and Chinese-English translation tasks.","New Orleans, Louisiana","Yang, Zhen  and
Chen, Wei  and
Wang, Feng  and
Xu, Bo","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",10.18653/v1/N18-1122,June,1346--1355,Association for Computational Linguistics,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,https://aclanthology.org/N18-1122,2018,,,,,
919,inproceedings,miculicich-werlen-etal-2018-self,"Neural sequence-to-sequence networks with attention have achieved remarkable performance for machine translation. One of the reasons for their effectiveness is their ability to capture relevant source-side contextual information at each time-step prediction through an attention mechanism. However, the target-side context is solely based on the sequence model which, in practice, is prone to a recency bias and lacks the ability to capture effectively non-sequential dependencies among words. To address this limitation, we propose a target-side-attentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. The residual learning facilitates the flow of information from the distant past and is able to emphasize any of the previously translated words, hence it gains access to a wider context. The proposed model outperforms a neural MT baseline as well as a memory and self-attention network on three language pairs. The analysis of the attention learned by the decoder confirms that it emphasizes a wider context, and that it captures syntactic-like structures.","New Orleans, Louisiana","Miculicich Werlen, Lesly  and
Pappas, Nikolaos  and
Ram, Dhananjay  and
Popescu-Belis, Andrei","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",10.18653/v1/N18-1124,June,1366--1379,Association for Computational Linguistics,Self-Attentive Residual Decoder for Neural Machine Translation,https://aclanthology.org/N18-1124,2018,,,,,
920,inproceedings,krishna-srinivasan-2018-generating,"Summarizing a document requires identifying the important parts of the document with an objective of providing a quick overview to a reader. However, a long article can span several topics and a single summary cannot do justice to all the topics. Further, the interests of readers can vary and the notion of importance can change across them. Existing summarization algorithms generate a single summary and are not capable of generating multiple summaries tuned to the interests of the readers. In this paper, we propose an attention based RNN framework to generate multiple summaries of a single document tuned to different topics of interest. Our method outperforms existing baselines and our results suggest that the attention of generative networks can be successfully biased to look at sentences relevant to a topic and effectively used to generate topic-tuned summaries.","New Orleans, Louisiana","Krishna, Kundan  and
Srinivasan, Balaji Vasan","Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",10.18653/v1/N18-1153,June,1697--1705,Association for Computational Linguistics,Generating Topic-Oriented Summaries Using Neural Attention,https://aclanthology.org/N18-1153,2018,,,,,
921,inproceedings,zhang-etal-2018-bidirectional,"Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.","Brussels, Belgium","Zhang, Zhirui  and
Liu, Shujie  and
Li, Mu  and
Zhou, Ming  and
Chen, Enhong",Proceedings of the 22nd Conference on Computational Natural Language Learning,10.18653/v1/K18-1019,October,190--199,Association for Computational Linguistics,Bidirectional Generative Adversarial Networks for Neural Machine Translation,https://aclanthology.org/K18-1019,2018,,,,,
922,inproceedings,barrett-etal-2018-sequence,"Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eye-tracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.","Brussels, Belgium","Barrett, Maria  and
Bingel, Joachim  and
Hollenstein, Nora  and
Rei, Marek  and
S{\o}gaard, Anders",Proceedings of the 22nd Conference on Computational Natural Language Learning,10.18653/v1/K18-1030,October,302--312,Association for Computational Linguistics,Sequence Classification with Human Attention,https://aclanthology.org/K18-1030,2018,,,,,
923,inproceedings,cer-etal-2018-universal,"We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. We report the relationship between model complexity, resources, and transfer performance. Comparisons are made with baselines without transfer learning and to baselines that incorporate word-level transfer. Transfer learning using sentence-level embeddings is shown to outperform models without transfer learning and often those that use only word-level transfer. We show good transfer task performance with minimal training data and obtain encouraging results on word embedding association tests (WEAT) of model bias.","Brussels, Belgium","Cer, Daniel  and
Yang, Yinfei  and
Kong, Sheng-yi  and
Hua, Nan  and
Limtiaco, Nicole  and
St. John, Rhomni  and
Constant, Noah  and
Guajardo-Cespedes, Mario  and
Yuan, Steve  and
Tar, Chris  and
Strope, Brian  and
Kurzweil, Ray",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,10.18653/v1/D18-2029,November,169--174,Association for Computational Linguistics,Universal Sentence Encoder for {E}nglish,https://aclanthology.org/D18-2029,2018,,,,,
924,inproceedings,tandon-etal-2018-reasoning,"Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this task, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways: (1) by incorporating global, commonsense constraints (e.g., a non-existent entity cannot be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8{\%} relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.","Brussels, Belgium","Tandon, Niket  and
Dalvi, Bhavana  and
Grus, Joel  and
Yih, Wen-tau  and
Bosselut, Antoine  and
Clark, Peter",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1006,October-November,57--66,Association for Computational Linguistics,Reasoning about Actions and State Changes by Injecting Commonsense Knowledge,https://aclanthology.org/D18-1006,2018,,,,,
925,inproceedings,zellers-etal-2018-swag,"Given a partial description like {``}she opened the hood of the car,{''} humans can reason about the situation and anticipate what might come next ({''}then, she examined the engine{''}). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88{\%}), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","Brussels, Belgium","Zellers, Rowan  and
Bisk, Yonatan  and
Schwartz, Roy  and
Choi, Yejin",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1009,October-November,93--104,Association for Computational Linguistics,{SWAG}: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,https://aclanthology.org/D18-1009,2018,,,,,
926,inproceedings,wang-etal-2018-personalized,"Sentiment expression in microblog posts can be affected by user{'}s personal character, opinion bias, political stance and so on. Most of existing personalized microblog sentiment classification methods suffer from the insufficiency of discriminative tweets for personalization learning. We observed that microblog users have consistent individuality and opinion bias in different languages. Based on this observation, in this paper we propose a novel user-attention-based Convolutional Neural Network (CNN) model with adversarial cross-lingual learning framework. The user attention mechanism is leveraged in CNN model to capture user{'}s language-specific individuality from the posts. Then the attention-based CNN model is incorporated into a novel adversarial cross-lingual learning framework, in which with the help of user properties as bridge between languages, we can extract the language-specific features and language-independent features to enrich the user post representation so as to alleviate the data insufficiency problem. Results on English and Chinese microblog datasets confirm that our method outperforms state-of-the-art baseline algorithms with large margins.","Brussels, Belgium","Wang, Weichao  and
Feng, Shi  and
Gao, Wei  and
Wang, Daling  and
Zhang, Yifei",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1031,October-November,338--348,Association for Computational Linguistics,Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning,https://aclanthology.org/D18-1031,2018,,,,,
927,inproceedings,hartmann-etal-2018-unsupervised,"This paper presents a challenge to the community: Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same algorithm, based on distributional information alone; but fails to do so, for two different embeddings algorithms. Why is that? We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs. This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters. One plausible suggestion based on our initial experiments is that the differences in the inductive biases of the embedding algorithms lead to an optimization landscape that is riddled with local optima, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.","Brussels, Belgium","Hartmann, Mareike  and
Kementchedjhieva, Yova  and
S{\o}gaard, Anders",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1056,October-November,582--586,Association for Computational Linguistics,Why is unsupervised alignment of {E}nglish embeddings from different algorithms so hard?,https://aclanthology.org/D18-1056,2018,,,,,
928,inproceedings,ryu-etal-2018-domain,"The main goal of this paper is to develop out-of-domain (OOD) detection for dialog systems. We propose to use only in-domain (IND) sentences to build a generative adversarial network (GAN) of which the discriminator generates low scores for OOD sentences. To improve basic GANs, we apply feature matching loss in the discriminator, use domain-category analysis as an additional task in the discriminator, and remove the biases in the generator. Thereby, we reduce the huge effort of collecting OOD sentences for training OOD detection. For evaluation, we experimented OOD detection on a multi-domain dialog system. The experimental results showed the proposed method was most accurate compared to the existing methods.","Brussels, Belgium","Ryu, Seonghan  and
Koo, Sangjun  and
Yu, Hwanjo  and
Lee, Gary Geunbae",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1077,October-November,714--718,Association for Computational Linguistics,Out-of-domain Detection based on Generative Adversarial Network,https://aclanthology.org/D18-1077,2018,,,,,
929,inproceedings,guo-etal-2018-improving,"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow.To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.","Brussels, Belgium","Guo, Tszhang  and
Chang, Shiyu  and
Yu, Mo  and
Bai, Kun",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1083,October-November,751--756,Association for Computational Linguistics,Improving Reinforcement Learning Based Image Captioning with Natural Language Prior,https://aclanthology.org/D18-1083,2018,,,,,
930,inproceedings,shafieibavani-etal-2018-graph,"ROUGE is one of the first and most widely used evaluation metrics for text summarization. However, its assessment merely relies on surface similarities between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate summaries including lexical variations and paraphrasing. We propose a graph-based approach adopted into ROUGE to evaluate summaries based on both lexical and semantic similarities. Experiment results over TAC AESOP datasets show that exploiting the lexico-semantic similarity of the words used in summaries would significantly help ROUGE correlate better with human judgments.","Brussels, Belgium","ShafieiBavani, Elaheh  and
Ebrahimi, Mohammad  and
Wong, Raymond  and
Chen, Fang",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1085,October-November,762--767,Association for Computational Linguistics,A Graph-theoretic Summary Evaluation for {ROUGE},https://aclanthology.org/D18-1085,2018,,,,,
931,inproceedings,moorthy-etal-2018-nike,"Are brand names such as Nike female or male? Previous research suggests that the sound of a person{'}s first name is associated with the person{'}s gender, but no research has tried to use this knowledge to assess the gender of brand names. We present a simple computational approach that uses sound symbolism to address this open issue. Consistent with previous research, a model trained on various linguistic features of name endings predicts human gender with high accuracy. Applying this model to a data set of over a thousand commercially-traded brands in 17 product categories, our results reveal an overall bias toward male names, cutting across both male-oriented product categories as well as female-oriented categories. In addition, we find variation within categories, suggesting that firms might be seeking to imbue their brands with differentiating characteristics as part of their competitive strategy.","Brussels, Belgium","Moorthy, Sridhar  and
Pogacar, Ruth  and
Khan, Samin  and
Xu, Yang",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1142,October-November,1128--1132,Association for Computational Linguistics,Is {N}ike female? Exploring the role of sound symbolism in predicting brand name gender,https://aclanthology.org/D18-1142,2018,,,,,
932,inproceedings,chen-etal-2018-collective,"Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.","Brussels, Belgium","Chen, Yubo  and
Yang, Hang  and
Liu, Kang  and
Zhao, Jun  and
Jia, Yantao",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1158,October-November,1267--1276,Association for Computational Linguistics,Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms,https://aclanthology.org/D18-1158,2018,,,,,
933,inproceedings,sun-etal-2018-memory,"Distributional semantic models (DSMs) generally require sufficient examples for a word to learn a high quality representation. This is in stark contrast with human who can guess the meaning of a word from one or a few referents only. In this paper, we propose Mem2Vec, a memory based embedding learning method capable of acquiring high quality word representations from fairly limited context. Our method directly adapts the representations produced by a DSM with a longterm memory to guide its guess of a novel word. Based on a pre-trained embedding space, the proposed method delivers impressive performance on two challenging few-shot word similarity tasks. Embeddings learned with our method also lead to considerable improvements over strong baselines on NER and sentiment classification.","Brussels, Belgium","Sun, Jingyuan  and
Wang, Shaonan  and
Zong, Chengqing",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1173,October-November,1435--1444,Association for Computational Linguistics,"Memory, Show the Way: Memory Based Few Shot Word Representation Learning",https://aclanthology.org/D18-1173,2018,,,,,
934,inproceedings,ailem-etal-2018-probabilistic,"Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely, on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.","Brussels, Belgium","Ailem, Melissa  and
Zhang, Bowen  and
Bellet, Aurelien  and
Denis, Pascal  and
Sha, Fei",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1177,October-November,1478--1487,Association for Computational Linguistics,A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images,https://aclanthology.org/D18-1177,2018,,,,,
935,inproceedings,dubossarsky-etal-2018-coming,"The point of departure of this article is the claim that sense-specific vectors provide an advantage over normal vectors due to the polysemy that they presumably represent. This claim is based on performance gains observed in gold standard evaluation tests such as word similarity tasks. We demonstrate that this claim, at least as it is instantiated in prior art, is unfounded in two ways. Furthermore, we provide empirical data and an analytic discussion that may account for the previously reported improved performance. First, we show that ground-truth polysemy degrades performance in word similarity tasks. Therefore word similarity tasks are not suitable as an evaluation test for polysemy representation. Second, random assignment of words to senses is shown to improve performance in the same task. This and additional results point to the conclusion that performance gains as reported in previous work may be an artifact of random sense assignment, which is equivalent to sub-sampling and multiple estimation of word vector representations. Theoretical analysis shows that this may on its own be beneficial for the estimation of word similarity, by reducing the bias in the estimation of the cosine distance.","Brussels, Belgium","Dubossarsky, Haim  and
Grossman, Eitan  and
Weinshall, Daphna",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1200,October-November,1732--1740,Association for Computational Linguistics,Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research,https://aclanthology.org/D18-1200,2018,,,,,
936,inproceedings,misra-etal-2018-policy,"Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-the-art model that outperforms previous work by 5.0{\%} absolute on exact match accuracy.","Brussels, Belgium","Misra, Dipendra  and
Chang, Ming-Wei  and
He, Xiaodong  and
Yih, Wen-tau",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1266,October-November,2442--2452,Association for Computational Linguistics,Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations,https://aclanthology.org/D18-1266,2018,,,,,
937,inproceedings,park-etal-2018-reducing,"Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, {``}You are a good woman{''} was considered {``}sexist{''} when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98{\%} and can be extended to correct model bias in other scenarios.","Brussels, Belgium","Park, Ji Ho  and
Shin, Jamin  and
Fung, Pascale",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1302,October-November,2799--2804,Association for Computational Linguistics,Reducing Gender Bias in Abusive Language Detection,https://aclanthology.org/D18-1302,2018,,,,,
938,inproceedings,makarov-clematide-2018-imitation,"We employ imitation learning to train a neural transition-based string transducer for morphological tasks such as inflection generation and lemmatization. Previous approaches to training this type of model either rely on an external character aligner for the production of gold action sequences, which results in a suboptimal model due to the unwarranted dependence on a single gold action sequence despite spurious ambiguity, or require warm starting with an MLE model. Our approach only requires a simple expert policy, eliminating the need for a character aligner or warm start. It also addresses familiar MLE training biases and leads to strong and state-of-the-art performance on several benchmarks.","Brussels, Belgium","Makarov, Peter  and
Clematide, Simon",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1314,October-November,2877--2882,Association for Computational Linguistics,Imitation Learning for Neural Morphological String Transduction,https://aclanthology.org/D18-1314,2018,,,,,
939,inproceedings,carton-etal-2018-extractive,"We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate {``}default{''} behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.","Brussels, Belgium","Carton, Samuel  and
Mei, Qiaozhu  and
Resnick, Paul",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1386,October-November,3497--3507,Association for Computational Linguistics,Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts,https://aclanthology.org/D18-1386,2018,,,,,
940,inproceedings,baly-etal-2018-predicting,"We present a study on predicting the factuality of reporting and bias of news media. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire news media. This is an under-studied, but arguably important research problem, both in its own right and as a prior for fact-checking systems. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target news media, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the baseline, and reveal the importance of each feature type.","Brussels, Belgium","Baly, Ramy  and
Karadzhov, Georgi  and
Alexandrov, Dimitar  and
Glass, James  and
Nakov, Preslav",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1389,October-November,3528--3539,Association for Computational Linguistics,Predicting Factuality of Reporting and Bias of News Media Sources,https://aclanthology.org/D18-1389,2018,,,,,
941,inproceedings,zamani-etal-2018-residualized,"Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.","Brussels, Belgium","Zamani, Mohammadzaman  and
Schwartz, H. Andrew  and
Lynn, Veronica  and
Giorgi, Salvatore  and
Balasubramanian, Niranjan",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1392,October-November,3560--3569,Association for Computational Linguistics,Residualized Factor Adaptation for Community Social Media Prediction Tasks,https://aclanthology.org/D18-1392,2018,,,,,
942,inproceedings,wu-etal-2018-beyond,"Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the \textit{accuracy drop} (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.","Brussels, Belgium","Wu, Lijun  and
Tan, Xu  and
He, Di  and
Tian, Fei  and
Qin, Tao  and
Lai, Jianhuang  and
Liu, Tie-Yan",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1396,October-November,3602--3611,Association for Computational Linguistics,Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter,https://aclanthology.org/D18-1396,2018,,,,,
943,inproceedings,yavuz-etal-2018-calcs,"Maximum-likelihood estimation (MLE) is one of the most widely used approaches for training structured prediction models for text-generation based natural language processing applications. However, besides exposure bias, models trained with MLE suffer from wrong objective problem where they are trained to maximize the word-level correct next step prediction, but are evaluated with respect to sequence-level discrete metrics such as ROUGE and BLEU. Several variants of policy-gradient methods address some of these problems by optimizing for final discrete evaluation metrics and showing improvements over MLE training for downstream tasks like text summarization and machine translation. However, policy-gradient methods suffers from high sample variance, making the training process very difficult and unstable. In this paper, we present an alternative direction towards mitigating this problem by introducing a new objective (CaLcs) based on a differentiable surrogate of longest common subsequence (LCS) measure that captures sequence-level structure similarity. Experimental results on abstractive summarization and machine translation validate the effectiveness of the proposed approach.","Brussels, Belgium","Yavuz, Semih  and
Chiu, Chung-Cheng  and
Nguyen, Patrick  and
Wu, Yonghui",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1406,October-November,3708--3718,Association for Computational Linguistics,{C}a{L}cs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization,https://aclanthology.org/D18-1406,2018,,,,,
944,inproceedings,yang-etal-2018-modeling,"Self-attention networks have proven to be of profound value for its strength of capturing global dependencies. In this work, we propose to model localness for self-attention networks, which enhances the ability of capturing useful local context. We cast localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to be paid more attention. The bias is then incorporated into the original attention distribution to form a revised distribution. To maintain the strength of capturing long distance dependencies while enhance the ability of capturing short-range dependencies, we only apply localness modeling to lower layers of self-attention networks. Quantitative and qualitative analyses on Chinese-English and English-German translation tasks demonstrate the effectiveness and universality of the proposed approach.","Brussels, Belgium","Yang, Baosong  and
Tu, Zhaopeng  and
Wong, Derek F.  and
Meng, Fandong  and
Chao, Lidia S.  and
Zhang, Tong",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1475,October-November,4449--4458,Association for Computational Linguistics,Modeling Localness for Self-Attention Networks,https://aclanthology.org/D18-1475,2018,,,,,
945,inproceedings,shao-etal-2018-greedy,"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.","Brussels, Belgium","Shao, Chenze  and
Chen, Xilin  and
Feng, Yang",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1510,October-November,4778--4784,Association for Computational Linguistics,Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,https://aclanthology.org/D18-1510,2018,,,,,
946,inproceedings,htut-etal-2018-grammar,"A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.","Brussels, Belgium","Htut, Phu Mon  and
Cho, Kyunghyun  and
Bowman, Samuel",Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D18-1544,October-November,4998--5003,Association for Computational Linguistics,Grammar Induction with Neural Language Models: An Unusual Replication,https://aclanthology.org/D18-1544,2018,,,,,
947,inproceedings,hu-etal-2018-shot,"Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and charges. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed model achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our model outperforms other baselines by more than 50{\%} in the few-shot scenario. Our codes and datasets can be obtained from https://github.com/thunlp/attribute{\_}charge.","Santa Fe, New Mexico, USA","Hu, Zikun  and
Li, Xiang  and
Tu, Cunchao  and
Liu, Zhiyuan  and
Sun, Maosong",Proceedings of the 27th International Conference on Computational Linguistics,,August,487--498,Association for Computational Linguistics,Few-Shot Charge Prediction with Discriminative Legal Attributes,https://aclanthology.org/C18-1041,2018,,,,,
948,inproceedings,cao-etal-2018-neural,"Entity Linking aims to link entity mentions in texts to knowledge bases, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL apply Graph Convolutional Network to integrate both local contextual features and global coherence information for entity linking. To improve the computation efficiency, we approximately perform graph convolution on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.","Santa Fe, New Mexico, USA","Cao, Yixin  and
Hou, Lei  and
Li, Juanzi  and
Liu, Zhiyuan",Proceedings of the 27th International Conference on Computational Linguistics,,August,675--686,Association for Computational Linguistics,Neural Collective Entity Linking,https://aclanthology.org/C18-1057,2018,,,,,
949,inproceedings,madnani-cahill-2018-automated,"In this position paper, we argue that building operational automated scoring systems is a task that has disciplinary complexity above and beyond standard competitive shared tasks which usually involve applying the latest machine learning techniques to publicly available data in order to obtain the best accuracy. Automated scoring systems warrant significant cross-discipline collaboration of which natural language processing and machine learning are just two of many important components. Such systems have multiple stakeholders with different but valid perspectives that can often times be at odds with each other. Our position is that it is essential for us as NLP researchers to understand and incorporate these perspectives in our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful.","Santa Fe, New Mexico, USA","Madnani, Nitin  and
Cahill, Aoife",Proceedings of the 27th International Conference on Computational Linguistics,,August,1099--1109,Association for Computational Linguistics,Automated Scoring: Beyond Natural Language Processing,https://aclanthology.org/C18-1094,2018,,,,,
950,inproceedings,potthast-etal-2018-crowdsourcing,"Clickbait has become a nuisance on social media. To address the urging task of clickbait detection, we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazon{'}s Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017. Download: https://webis.de/data/webis-clickbait-17.html Challenge: https://clickbait-challenge.org","Santa Fe, New Mexico, USA","Potthast, Martin  and
Gollub, Tim  and
Komlossy, Kristof  and
Schuster, Sebastian  and
Wiegmann, Matti  and
Garces Fernandez, Erika Patricia  and
Hagen, Matthias  and
Stein, Benno",Proceedings of the 27th International Conference on Computational Linguistics,,August,1498--1507,Association for Computational Linguistics,Crowdsourcing a Large Corpus of Clickbait on {T}witter,https://aclanthology.org/C18-1127,2018,,,,,
951,inproceedings,preotiuc-pietro-ungar-2018-user,"User demographic inference from social media text has the potential to improve a range of downstream applications, including real-time passive polling or quantifying demographic bias. This study focuses on developing models for user-level race and ethnicity prediction. We introduce a data set of users who self-report their race/ethnicity through a survey, in contrast to previous approaches that use distantly supervised data or perceived labels. We develop predictive models from text which accurately predict the membership of a user to the four largest racial and ethnic groups with up to .884 AUC and make these available to the research community.","Santa Fe, New Mexico, USA","Preo{\c{t}}iuc-Pietro, Daniel  and
Ungar, Lyle",Proceedings of the 27th International Conference on Computational Linguistics,,August,1534--1545,Association for Computational Linguistics,User-Level Race and Ethnicity Predictors from {T}witter Text,https://aclanthology.org/C18-1130,2018,,,,,
952,inproceedings,ghaddar-langlais-2018-robust,"Neural network approaches to Named-Entity Recognition reduce the need for carefully hand-crafted features. While some features do remain in state-of-the-art systems, lexical features have been mostly discarded, with the exception of gazetteers. In this work, we show that this is unfair: lexical features are actually quite useful. We propose to embed words and entity types into a low-dimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia. From this, we compute {---} offline {---} a feature vector representing each word. When used with a vanilla recurrent neural network model, this representation yields substantial improvements. We establish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003 dataset.","Santa Fe, New Mexico, USA","Ghaddar, Abbas  and
Langlais, Phillippe",Proceedings of the 27th International Conference on Computational Linguistics,,August,1896--1907,Association for Computational Linguistics,Robust Lexical Features for Improved Neural Network Named-Entity Recognition,https://aclanthology.org/C18-1161,2018,,,,,
953,inproceedings,moss-etal-2018-using,"K-fold cross validation (CV) is a popular method for estimating the true performance of machine learning models, allowing model selection and parameter tuning. However, the very process of CV requires random partitioning of the data and so our performance estimates are in fact stochastic, with variability that can be substantial for natural language processing tasks. We demonstrate that these unstable estimates cannot be relied upon for effective parameter tuning. The resulting tuned parameters are highly sensitive to how our data is partitioned, meaning that we often select sub-optimal parameter choices and have serious reproducibility issues. Instead, we propose to use the less variable J-K-fold CV, in which J independent K-fold cross validations are used to assess performance. Our main contributions are extending J-K-fold CV from performance estimation to parameter tuning and investigating how to choose J and K. We argue that variability is more important than bias for effective tuning and so advocate lower choices of K than are typically seen in the NLP literature and instead use the saved computation to increase J. To demonstrate the generality of our recommendations we investigate a wide range of case-studies: sentiment classification (both general and target-specific), part-of-speech tagging and document classification.","Santa Fe, New Mexico, USA","Moss, Henry  and
Leslie, David  and
Rayson, Paul",Proceedings of the 27th International Conference on Computational Linguistics,,August,2978--2989,Association for Computational Linguistics,Using {J}-{K}-fold Cross Validation To Reduce Variance When Tuning {NLP} Models,https://aclanthology.org/C18-1252,2018,,,,,
954,inproceedings,labeau-allauzen-2018-learning,"Noise-Contrastive Estimation (NCE) is a learning criterion that is regularly used to train neural language models in place of Maximum Likelihood Estimation, since it avoids the computational bottleneck caused by the output softmax. In this paper, we analyse and explain some of the weaknesses of this objective function, linked to the mechanism of self-normalization, by closely monitoring comparative experiments. We then explore several remedies and modifications to propose tractable and efficient NCE training strategies. In particular, we propose to make the scaling factor a trainable parameter of the model, and to use the noise distribution to initialize the output bias. These solutions, yet simple, yield stable and competitive performances in either small and large scale language modelling tasks.","Santa Fe, New Mexico, USA","Labeau, Matthieu  and
Allauzen, Alexandre",Proceedings of the 27th International Conference on Computational Linguistics,,August,3090--3101,Association for Computational Linguistics,Learning with Noise-Contrastive Estimation: Easing training by learning to scale,https://aclanthology.org/C18-1261,2018,,,,,
955,inproceedings,wang-etal-2018-prospective,"Generative dialog models usually adopt beam search as the inference method to generate responses. However, small-width beam search only focuses on the limited current optima. This deficiency named as myopic bias ultimately suppresses the diversity and probability of generated responses. Although increasing the beam width mitigates the myopic bias, it also proportionally slows down the inference efficiency. To alleviate the myopic bias in small-width beam search, this paper proposes a Prospective-Performance Network (PPN) to predict the future reward of the given partially-generated response, and the future reward is defined by the expectation of the partial response appearing in the top-ranked responses given by a larger-width beam search. Enhanced by PPN, the decoder can promote the results with great potential during the beam search phase. The experimental results on both Chinese and English corpora show that our method is promising to increase the quality and diversity of generated responses, with inference efficiency well maintained.","Santa Fe, New Mexico, USA","Wang, Zongsheng  and
Bai, Yunzhi  and
Wu, Bowen  and
Xu, Zhen  and
Wang, Zhuoran  and
Wang, Baoxun",Proceedings of the 27th International Conference on Computational Linguistics,,August,3608--3618,Association for Computational Linguistics,A Prospective-Performance Network to Alleviate Myopia in Beam Search for Response Generation,https://aclanthology.org/C18-1306,2018,,,,,
956,inproceedings,bentivogli-etal-2018-machine,"In this paper we present an analysis of the two most prominent methodologies used for the human evaluation of MT quality, namely evaluation based on Post-Editing (PE) and evaluation based on Direct Assessment (DA). To this purpose, we exploit a publicly available large dataset containing both types of evaluations. We first focus on PE and investigate how sensitive TER-based evaluation is to the type and number of references used. Then, we carry out a comparative analysis of PE and DA to investigate the extent to which the evaluation results obtained by methodologies addressing different human perspectives are similar. This comparison sheds light not only on PE but also on the so-called reference bias related to monolingual DA. Also, we analyze if and how the two methodologies can complement each other{'}s weaknesses.",Brussels,"Bentivogli, Luisa  and
Cettolo, Mauro  and
Federico, Marcello  and
Federmann, Christian",Proceedings of the 15th International Conference on Spoken Language Translation,,October 29-30,62--69,International Conference on Spoken Language Translation,Machine Translation Human Evaluation: an investigation of evaluation based on Post-Editing and its relation with Direct Assessment,https://aclanthology.org/2018.iwslt-1.9,2018,,,,,
957,inproceedings,patel-etal-2018-semi,"Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, manual maintenance of such resources is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link wordnets. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60{\%} of all synsets and 70{\%} of noun synsets.","Nanyang Technological University (NTU), Singapore","Patel, Kevin  and
Kanojia, Diptesh  and
Bhattacharyya, Pushpak",Proceedings of the 9th Global Wordnet Conference,,January,266--271,Global Wordnet Association,Semi-automatic {W}ord{N}et Linking using Word Embeddings,https://aclanthology.org/2018.gwc-1.31,2018,,,,,
958,inproceedings,bhat-etal-2017-leveraging,"We investigate the problem of parsing conversational data of morphologically-rich languages such as Hindi where argument scrambling occurs frequently. We evaluate a state-of-the-art non-linear transition-based parsing system on a new dataset containing 506 dependency trees for sentences from Bollywood (Hindi) movie scripts and Twitter posts of Hindi monolingual speakers. We show that a dependency parser trained on a newswire treebank is strongly biased towards the canonical structures and degrades when applied to conversational data. Inspired by Transformational Generative Grammar (Chomsky, 1965), we mitigate the sampling bias by generating all theoretically possible alternative word orders of a clause from the existing (kernel) structures in the treebank. Training our parser on canonical and transformed structures improves performance on conversational data by around 9{\%} LAS over the baseline newswire parser.","Pisa, Italy","Bhat, Riyaz A.  and
Bhat, Irshad  and
Sharma, Dipti",Proceedings of the 15th International Conference on Parsing Technologies,,September,61--66,Association for Computational Linguistics,Leveraging Newswire Treebanks for Parsing Conversational Data with Argument Scrambling,https://aclanthology.org/W17-6309,2017,,,,,
959,inproceedings,wang-etal-2017-group,"For practical chatbots, one of the essential factor for improving user experience is the capability of customizing the talking style of the agents, that is, to make chatbots provide responses meeting users{'} preference on language styles, topics, etc. To address this issue, this paper proposes to incorporate linguistic biases, which implicitly involved in the conversation corpora generated by human groups in the Social Network Services (SNS), into the encoder-decoder based response generator. By attaching a specially designed neural component to dynamically control the impact of linguistic biases in response generation, a Group Linguistic Bias Aware Neural Response Generation (GLBA-NRG) model is eventually presented. The experimental results on the dataset from the Chinese SNS show that the proposed architecture outperforms the current response generating models by producing both meaningful and vivid responses with customized styles.",Taiwan,"Wang, Jianan  and
Wang, Xin  and
Li, Fang  and
Xu, Zhen  and
Wang, Zhuoran  and
Wang, Baoxun",Proceedings of the 9th {SIGHAN} Workshop on {C}hinese Language Processing,,December,1--10,Association for Computational Linguistics,Group Linguistic Bias Aware Neural Response Generation,https://aclanthology.org/W17-6001,2017,,,,,
960,inproceedings,ravichander-etal-2017-say,"Building dialogue interfaces for real-world scenarios often entails training semantic parsers starting from zero examples. How can we build datasets that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a grammar and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a methodology that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named SmartHome, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets.","Saarbr{\""u}cken, Germany","Ravichander, Abhilasha  and
Manzini, Thomas  and
Grabmair, Matthias  and
Neubig, Graham  and
Francis, Jonathan  and
Nyberg, Eric",Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,10.18653/v1/W17-5545,August,374--383,Association for Computational Linguistics,How Would You Say It? Eliciting Lexically Diverse Dialogue for Supervised Semantic Parsing,https://aclanthology.org/W17-5545,2017,,,,,
961,inproceedings,nangia-etal-2017-repeval,"This paper presents the results of the RepEval 2017 Shared Task, which evaluated neural network sentence representation learning models on the Multi-Genre Natural Language Inference corpus (MultiNLI) recently introduced by Williams et al. (2017). All of the five participating teams beat the bidirectional LSTM (BiLSTM) and continuous bag of words baselines reported in Williams et al. The best single model used stacked BiLSTMs with residual connections to extract sentence features and reached 74.5{\%} accuracy on the genre-matched test set. Surprisingly, the results of the competition were fairly consistent across the genre-matched and genre-mismatched test sets, and across subsets of the test data representing a variety of linguistic phenomena, suggesting that all of the submitted systems learned reasonably domain-independent representations for sentence meaning.","Copenhagen, Denmark","Nangia, Nikita  and
Williams, Adina  and
Lazaridou, Angeliki  and
Bowman, Samuel",Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP},10.18653/v1/W17-5301,September,1--10,Association for Computational Linguistics,The {R}ep{E}val 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations,https://aclanthology.org/W17-5301,2017,,,,,
962,inproceedings,ostling-grigonyte-2017-transparent,"We present a very simple model for text quality assessment based on a deep convolutional neural network, where the only supervision required is one corpus of user-generated text of varying quality, and one contrasting text corpus of consistently high quality. Our model is able to provide local quality assessments in different parts of a text, which allows visual feedback about where potentially problematic parts of the text are located, as well as a way to evaluate which textual features are captured by our model. We evaluate our method on two corpora: a large corpus of manually graded student essays and a longitudinal corpus of language learner written production, and find that the text quality metric learned by our model is a fairly strong predictor of both essay grade and learner proficiency level.","Copenhagen, Denmark","{\""O}stling, Robert  and
Grigonyte, Gintare",Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,10.18653/v1/W17-5031,September,282--286,Association for Computational Linguistics,Transparent text quality assessment with convolutional neural networks,https://aclanthology.org/W17-5031,2017,,,,,
963,inproceedings,kulmizev-etal-2017-power,"In this paper, we explore the performance of a linear SVM trained on language independent character features for the NLI Shared Task 2017. Our basic system (GRONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution.","Copenhagen, Denmark","Kulmizev, Artur  and
Blankers, Bo  and
Bjerva, Johannes  and
Nissim, Malvina  and
van Noord, Gertjan  and
Plank, Barbara  and
Wieling, Martijn",Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,10.18653/v1/W17-5043,September,382--389,Association for Computational Linguistics,The Power of Character N-grams in Native Language Identification,https://aclanthology.org/W17-5043,2017,,,,,
964,inproceedings,hitschler-etal-2017-authorship,"We use a convolutional neural network to perform authorship identification on a very homogeneous dataset of scientific publications. In order to investigate the effect of domain biases, we obscure words below a certain frequency threshold, retaining only their POS-tags. This procedure improves test performance due to better generalization on unseen data. Using our method, we are able to predict the authors of scientific publications in the same discipline at levels well above chance.","Copenhagen, Denmark","Hitschler, Julian  and
van den Berg, Esther  and
Rehbein, Ines",Proceedings of the Workshop on Stylistic Variation,10.18653/v1/W17-4907,September,53--58,Association for Computational Linguistics,Authorship Attribution with Convolutional Neural Networks and {POS}-Eliding,https://aclanthology.org/W17-4907,2017,,,,,
965,inproceedings,blodgett-etal-2017-dataset,"While language identification works well on standard texts, it performs much worse on social media language, in particular dialectal language{---}even for English. First, to support work on English language identification, we contribute a new dataset of tweets annotated for English versus non-English, with attention to ambiguity, code-switching, and automatic generation issues. It is randomly sampled from all public messages, avoiding biases towards pre-existing language classifiers. Second, we find that a demographic language model{---}which identifies messages with language similar to that used by several U.S. ethnic populations on Twitter{---}can be used to improve English language identification performance when combined with a traditional supervised language identifier. It increases recall with almost no loss of precision, including, surprisingly, for English messages written by non-U.S. authors. Our dataset and identifier ensemble are available online.","Copenhagen, Denmark","Blodgett, Su Lin  and
Wei, Johnny  and
O{'}Connor, Brendan",Proceedings of the 3rd Workshop on Noisy User-generated Text,10.18653/v1/W17-4408,September,56--61,Association for Computational Linguistics,A Dataset and Classifier for Recognizing Social Media {E}nglish,https://aclanthology.org/W17-4408,2017,,,,,
966,inproceedings,jang-etal-2017-improving,"Technical documents contain a fair amount of unnatural language, such as tables, formulas, and pseudo-code. Unnatural language can bean important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of un-natural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various for-mats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that re-moving unnatural language components gives an absolute improvement in document cluster-ing by up to 15{\%}. Our corpus and tool are publicly available","Copenhagen, Denmark","Jang, Myungha  and
Choi, Jinho D.  and
Allan, James",Proceedings of the 3rd Workshop on Noisy User-generated Text,10.18653/v1/W17-4416,September,122--130,Association for Computational Linguistics,Improving Document Clustering by Removing Unnatural Language,https://aclanthology.org/W17-4416,2017,,,,,
967,inproceedings,potash-etal-2017-tracking,"This paper addresses the task of identifying the bias in news articles published during a political or social conflict. We create a silver-standard corpus based on the actions of users in social media. Specifically, we reconceptualize bias in terms of how likely a given article is to be shared or liked by each of the opposing sides. We apply our methodology to a dataset of links collected in relation to the Russia-Ukraine Maidan crisis from 2013-2014. We show that on the task of predicting which side is likely to prefer a given article, a Naive Bayes classifier can record 90.3{\%} accuracy looking only at domain names of the news sources. The best accuracy of 93.5{\%} is achieved by a feed forward neural network. We also apply our methodology to gold-labeled set of articles annotated for bias, where the aforementioned Naive Bayes classifier records 82.6{\%} accuracy and a feed-forward neural networks records 85.6{\%} accuracy.","Copenhagen, Denmark","Potash, Peter  and
Romanov, Alexey  and
Gronas, Mikhail  and
Rumshisky, Anna  and
Gronas, Mikhail",Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,10.18653/v1/W17-4203,September,13--18,Association for Computational Linguistics,Tracking Bias in News Sources Using Social Media: the Russia-{U}kraine Maidan Crisis of 2013{--}2014,https://aclanthology.org/W17-4203,2017,,,,,
968,inproceedings,jiang-etal-2017-comparing,"News media typically present biased accounts of news stories, and different publications present different angles on the same event. In this research, we investigate how different publications differ in their approach to stories about climate change, by examining the sentiment and topics presented. To understand these attitudes, we find sentiment targets by combining Latent Dirichlet Allocation (LDA) with SentiWordNet, a general sentiment lexicon. Using LDA, we generate topics containing keywords which represent the sentiment targets, and then annotate the data using SentiWordNet before regrouping the articles based on topic similarity. Preliminary analysis identifies clearly different attitudes on the same issue presented in different news sources. Ongoing work is investigating how systematic these attitudes are between different publications, and how these may change over time.","Copenhagen, Denmark","Jiang, Ye  and
Song, Xingyi  and
Harrison, Jackie  and
Quegan, Shaun  and
Maynard, Diana",Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,10.18653/v1/W17-4205,September,25--30,Association for Computational Linguistics,Comparing Attitudes to Climate Change in the Media using sentiment analysis based on {L}atent {D}irichlet {A}llocation,https://aclanthology.org/W17-4205,2017,,,,,
969,inproceedings,pavlopoulos-etal-2017-improved,"Experimenting with a dataset of approximately 1.6M user comments from a Greek news sports portal, we explore how a state of the art RNN-based moderation method can be improved by adding user embeddings, user type embeddings, user biases, or user type biases. We observe improvements in all cases, with user embeddings leading to the biggest performance gains.","Copenhagen, Denmark","Pavlopoulos, John  and
Malakasiotis, Prodromos  and
Bakagianni, Juli  and
Androutsopoulos, Ion",Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,10.18653/v1/W17-4209,September,51--55,Association for Computational Linguistics,Improved Abusive Comment Moderation with User Embeddings,https://aclanthology.org/W17-4209,2017,,,,,
970,inproceedings,madhyastha-espana-bonet-2017-learning,"We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in machine translation. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English{--}Spanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 BLEU points.","Vancouver, Canada","Madhyastha, Pranava Swaroop  and
Espa{\~n}a-Bonet, Cristina",Proceedings of the 2nd Workshop on Representation Learning for {NLP},10.18653/v1/W17-2617,August,139--145,Association for Computational Linguistics,Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation,https://aclanthology.org/W17-2617,2017,,,,,
971,inproceedings,langlais-2017-users,"Despite numerous studies devoted to mining parallel material from bilingual data, we have yet to see the resulting technologies wholeheartedly adopted by professional translators and terminologists alike. I argue that this state of affairs is mainly due to two factors: the emphasis published authors put on models (even though data is as important), and the conspicuous lack of concern for actual end-users.","Vancouver, Canada","Langlais, Phillippe",Proceedings of the 10th Workshop on Building and Using Comparable Corpora,10.18653/v1/W17-2501,August,1--5,Association for Computational Linguistics,Users and Data: The Two Neglected Children of Bilingual Natural Language Processing Research,https://aclanthology.org/W17-2501,2017,,,,,
972,inproceedings,fivez-etal-2017-unsupervised,"We present an unsupervised context-sensitive spelling correction method for clinical free-text that uses word and character n-gram embeddings. Our method generates misspelling replacement candidates and ranks them according to their semantic fit, by calculating a weighted cosine similarity between the vectorized representation of a candidate and the misspelling context. We greatly outperform two baseline off-the-shelf spelling correction tools on a manually annotated MIMIC-III test set, and counter the frequency bias of an optimized noisy channel model, showing that neural embeddings can be successfully exploited to include context-awareness in a spelling correction model.","Vancouver, Canada,","Fivez, Pieter  and
{\v{S}}uster, Simon  and
Daelemans, Walter",{B}io{NLP} 2017,10.18653/v1/W17-2317,August,143--148,Association for Computational Linguistics,Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text with Word and Character N-Gram Embeddings,https://aclanthology.org/W17-2317,2017,,,,,
973,inproceedings,madnani-etal-2017-building,"Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GREÂ® and the TOEFLÂ®. Ethical considerations require that automated scoring algorithms treat all test-takers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution.","Valencia, Spain","Madnani, Nitin  and
Loukina, Anastassia  and
von Davier, Alina  and
Burstein, Jill  and
Cahill, Aoife",Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing,10.18653/v1/W17-1605,April,41--52,Association for Computational Linguistics,Building Better Open-Source Tools to Support Fairness in Automated Scoring,https://aclanthology.org/W17-1605,2017,,,,,
974,inproceedings,rudinger-etal-2017-social,"We analyze the Stanford Natural Language Inference (SNLI) corpus in an investigation of bias and stereotyping in NLP data. The SNLI human-elicitation protocol makes it prone to amplifying bias and stereotypical associations, which we demonstrate statistically (using pointwise mutual information) and with qualitative examples.","Valencia, Spain","Rudinger, Rachel  and
May, Chandler  and
Van Durme, Benjamin",Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing,10.18653/v1/W17-1609,April,74--79,Association for Computational Linguistics,Social Bias in Elicited Natural Language Inferences,https://aclanthology.org/W17-1609,2017,,,,,
975,inproceedings,suster-etal-2017-short,"Clinical NLP has an immense potential in contributing to how clinical practice will be revolutionized by the advent of large scale processing of clinical records. However, this potential has remained largely untapped due to slow progress primarily caused by strict data access policies for researchers. In this paper, we discuss the concern for privacy and the measures it entails. We also suggest sources of less sensitive data. Finally, we draw attention to biases that can compromise the validity of empirical research and lead to socially harmful applications.","Valencia, Spain","{\v{S}}uster, Simon  and
Tulkens, St{\'e}phan  and
Daelemans, Walter",Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing,10.18653/v1/W17-1610,April,80--87,Association for Computational Linguistics,A Short Review of Ethical Challenges in Clinical Natural Language Processing,https://aclanthology.org/W17-1610,2017,,,,,
976,inproceedings,darwish-etal-2017-arabic-pos,"This paper focuses on comparing between using Support Vector Machine based ranking (SVM-Rank) and Bidirectional Long-Short-Term-Memory (bi-LSTM) neural-network based sequence labeling in building a state-of-the-art Arabic part-of-speech tagging system. Using SVM-Rank leads to state-of-the-art results, but with a fair amount of feature engineering. Using bi-LSTM, particularly when combined with word embeddings, may lead to competitive POS-tagging results by automatically deducing latent linguistic features. However, we show that augmenting bi-LSTM sequence labeling with some of the features that we used for the SVM-Rank based tagger yields to further improvements. We also show that gains that realized by using embeddings may not be additive with the gains achieved by the features. We are open-sourcing both the SVM-Rank and the bi-LSTM based systems for free.","Valencia, Spain","Darwish, Kareem  and
Mubarak, Hamdy  and
Abdelali, Ahmed  and
Eldesouki, Mohamed",Proceedings of the Third {A}rabic Natural Language Processing Workshop,10.18653/v1/W17-1316,April,130--137,Association for Computational Linguistics,{A}rabic {POS} Tagging: Don{'}t Abandon Feature Engineering Just Yet,https://aclanthology.org/W17-1316,2017,,,,,
977,inproceedings,chambers-2017-behind,"This paper analyzes the narrative event cloze test and its recent evolution. The test removes one event from a document{'}s chain of events, and systems predict the missing event. Originally proposed to evaluate learned knowledge of event scenarios (e.g., scripts and frames), most recent work now builds ngram-like language models (LM) to beat the test. This paper argues that the test has slowly/unknowingly been altered to accommodate LMs.5 Most notably, tests are auto-generated rather than by hand, and no effort is taken to include core script events. Recent work is not clear on evaluation goals and contains contradictory results. We implement several models, and show that the test{'}s bias to high-frequency events explains the inconsistencies. We conclude with recommendations on how to return to the test{'}s original intent, and offer brief suggestions on a path forward.","Valencia, Spain","Chambers, Nathanael","Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",10.18653/v1/W17-0905,April,41--45,Association for Computational Linguistics,Behind the Scenes of an Evolving Event Cloze Test,https://aclanthology.org/W17-0905,2017,,,,,
978,inproceedings,tatman-2017-oh,"Human listeners are able to quickly and robustly adapt to new accents and do so by using information about speaker{'}s identities. This paper will present experimental evidence that, even considering information about speaker{'}s identities, listeners retain a strong bias towards the acoustics of their own dialect after dialect learning. Participants{'} behaviour was accurately mimicked by a classifier which was trained on more cases from the base dialect and fewer from the target dialect. This suggests that imbalanced training data may result in automatic speech recognition errors consistent with those of speakers from populations over-represented in the training data.","Valencia, Spain","Tatman, Rachael",Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2017),10.18653/v1/W17-0704,April,29--34,Association for Computational Linguistics,"{``}Oh, {I}{'}ve Heard That Before{''}: Modelling Own-Dialect Bias After Perceptual Learning by Weighting Training Data",https://aclanthology.org/W17-0704,2017,,,,,
979,inproceedings,doucette-2017-inherent,"A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary.","Valencia, Spain","Doucette, Amanda",Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2017),10.18653/v1/W17-0705,April,35--40,Association for Computational Linguistics,Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation,https://aclanthology.org/W17-0705,2017,,,,,
980,inproceedings,bastawisy-elmahdy-2017-multi,"In this paper, we implement a multilingual Statistical Machine Translation (SMT) system for Arabic-English Translation. Arabic Text can be categorized into standard and dialectal Arabic. These two forms of Arabic differ significantly. Different mono-lingual and multi-lingual hybrid SMT approaches are compared. Mono-lingual systems do always results in better translation accuracy in one Arabic form and poor accuracy in the other. Multi-lingual SMT models that are trained with pooled parallel MSA/dialectal data result in better accuracy. However, since the available parallel MSA data are much larger compared to dialectal data, multilingual models are biased to MSA. We propose in the work, a multi-lingual combination of different mono-lingual systems using an Arabic form classifier. The outcome of the classier directs the system to use the appropriate mono-lingual models (standard, dialectal, or mixture). Testing the different SMT systems shows that the proposed classifier-based SMT system outperforms mono-lingual and data pooled multi-lingual systems.","Varna, Bulgaria","Bastawisy, Ahmed  and
Elmahdy, Mohamed","Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",10.26615/978-954-452-049-6_013,September,86--89,INCOMA Ltd.,Multi-Lingual Phrase-Based Statistical Machine Translation for {A}rabic-{E}nglish,https://doi.org/10.26615/978-954-452-049-6_013,2017,,,,,
981,inproceedings,steinberger-etal-2017-large,"We work on detecting positive or negative sentiment towards named entities in very large volumes of news articles. The aim is to monitor changes over time, as well as to work towards media bias detection by com-paring differences across news sources and countries. With view to applying the same method to dozens of languages, we use lin-guistically light-weight methods: searching for positive and negative terms in bags of words around entity mentions (also consid-ering negation). Evaluation results are good and better than a third-party baseline sys-tem, but precision is not sufficiently high to display the results publicly in our multilin-gual news analysis system Europe Media Monitor (EMM). In this paper, we focus on describing our effort to improve the English language results by avoiding the biggest sources of errors. We also present new work on using a syntactic parser to identify safe opinion recognition rules, such as predica-tive structures in which sentiment words di-rectly refer to an entity. The precision of this method is good, but recall is very low.","Varna, Bulgaria","Steinberger, Ralf  and
Hegele, Stefanie  and
Tanev, Hristo  and
Della Rocca, Leonida","Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",10.26615/978-954-452-049-6_091,September,707--715,INCOMA Ltd.,Large-scale news entity sentiment analysis,https://doi.org/10.26615/978-954-452-049-6_091,2017,,,,,
982,inproceedings,kazi-thompson-2017-implicitly,"In this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components. The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies. Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging task and a baseline bidirectional network on an additional artificial random biased walk task.","Vancouver, Canada","Kazi, Michaeel  and
Thompson, Brian",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P17-2027,July,172--177,Association for Computational Linguistics,Implicitly-Defined Neural Networks for Sequence Labeling,https://aclanthology.org/P17-2027,2017,,,,,
983,inproceedings,goyal-etal-2017-differentiable,"We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding in sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure{--}a well-known technique for correcting exposure bias{--}we introduce a new training objective that is continuous and differentiable everywhere and can provide informative gradients near points where previous decoding decisions change their value. By using a related approximation, we also demonstrate a similar approach to sampled-based training. We show that our approach outperforms both standard cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.","Vancouver, Canada","Goyal, Kartik  and
Dyer, Chris  and
Berg-Kirkpatrick, Taylor",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),10.18653/v1/P17-2058,July,366--371,Association for Computational Linguistics,Differentiable Scheduled Sampling for Credit Assignment,https://aclanthology.org/P17-2058,2017,,,,,
984,inproceedings,forbes-choi-2017-verb,"Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., {``}My house is bigger than me.{''} However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, {``}Tyler entered his house{''} implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.","Vancouver, Canada","Forbes, Maxwell  and
Choi, Yejin",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P17-1025,July,266--276,Association for Computational Linguistics,Verb Physics: Relative Physical Knowledge of Actions and Objects,https://aclanthology.org/P17-1025,2017,,,,,
985,inproceedings,he-etal-2017-unsupervised,"Aspect extraction is an important and challenging task in aspect-based sentiment analysis. Existing works tend to apply variants of topic models on this task. While fairly successful, these methods usually do not produce highly coherent aspects. In this paper, we present a novel neural approach with the aim of discovering coherent aspects. The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings. Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space. In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects. Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.","Vancouver, Canada","He, Ruidan  and
Lee, Wee Sun  and
Ng, Hwee Tou  and
Dahlmeier, Daniel",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P17-1036,July,388--397,Association for Computational Linguistics,An Unsupervised Neural Attention Model for Aspect Extraction,https://aclanthology.org/P17-1036,2017,,,,,
986,inproceedings,florescu-caragea-2017-positionrank,"The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document{'}s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word{'}s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09{\%}.","Vancouver, Canada","Florescu, Corina  and
Caragea, Cornelia",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P17-1102,July,1105--1115,Association for Computational Linguistics,{P}osition{R}ank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,https://aclanthology.org/P17-1102,2017,,,,,
987,inproceedings,lowe-etal-2017-towards,"Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem.We present an evaluation model (ADEM)that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model{'}s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue mod-els unseen during training, an important step for automatic dialogue evaluation.","Vancouver, Canada","Lowe, Ryan  and
Noseworthy, Michael  and
Serban, Iulian Vlad  and
Angelard-Gontier, Nicolas  and
Bengio, Yoshua  and
Pineau, Joelle",Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),10.18653/v1/P17-1103,July,1116--1126,Association for Computational Linguistics,Towards an Automatic {T}uring Test: Learning to Evaluate Dialogue Responses,https://aclanthology.org/P17-1103,2017,,,,,
988,inproceedings,dyer-2017-neural,"I explore the hypothesis that conventional neural network models (e.g., recurrent neural networks) are incorrectly biased for making linguistically sensible generalizations when learning, and that a better class of models is based on architectures that reflect hierarchical structures for which considerable behavioral evidence exists. I focus on the problem of modeling and representing the meanings of sentences. On the generation front, I introduce recurrent neural network grammars (RNNGs), a joint, generative model of phrase-structure trees and sentences. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire (top-down, left-to-right) syntactic derivation history, thus relaxing context-free independence assumptions, while retaining a bias toward explaining decisions via {``}syntactically local{''} conditioning contexts. Experiments show that RNNGs obtain better results in generating language than models that don{'}t exploit linguistic structure. On the representation front, I explore unsupervised learning of syntactic structures based on distant semantic supervision using a reinforcement-learning algorithm. The learner seeks a syntactic structure that provides a compositional architecture that produces a good representation for a downstream semantic task. Although the inferred structures are quite different from traditional syntactic analyses, the performance on the downstream tasks surpasses that of systems that use sequential RNNs and tree-structured RNNs based on treebank dependencies. This is joint work with Adhi Kuncoro, Dani Yogatama, Miguel Ballesteros, Phil Blunsom, Ed Grefenstette, Wang Ling, and Noah A. Smith.","Vancouver, Canada","Dyer, Chris",Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),10.18653/v1/K17-1001,August,1,Association for Computational Linguistics,Should Neural Network Architecture Reflect Linguistic Structure?,https://aclanthology.org/K17-1001,2017,,,,,
989,inproceedings,cromieres-etal-2017-neural,"Machine Translation (MT) is a sub-field of NLP which has experienced a number of paradigm shifts since its inception. Up until 2014, Phrase Based Statistical Machine Translation (PBSMT) approaches used to be the state of the art. In late 2014, Neural Machine Translation (NMT) was introduced and was proven to outperform all PBSMT approaches by a significant margin. Since then, the NMT approaches have undergone several transformations which have pushed the state of the art even further. This tutorial is primarily aimed at researchers who are either interested in or are fairly new to the world of NMT and want to obtain a deep understanding of NMT fundamentals. Because it will also cover the latest developments in NMT, it should also be useful to attendees with some experience in NMT.","Taipei, Taiwan","Cromieres, Fabien  and
Nakazawa, Toshiaki  and
Dabre, Raj","Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",,November,11--13,Asian Federation of Natural Language Processing,"Neural Machine Translation: Basics, Practical Aspects and Recent Trends",https://aclanthology.org/I17-5004,2017,,,,,
990,inproceedings,yuan-etal-2017-ynu,"A shared task is a typical question answering task that aims to test how accurately the participants can answer the questions in exams. Typically, for each question, there are four candidate answers, and only one of the answers is correct. The existing methods for such a task usually implement a recurrent neural network (RNN) or long short-term memory (LSTM). However, both RNN and LSTM are biased models in which the words in the tail of a sentence are more dominant than the words in the header. In this paper, we propose the use of an attention-based LSTM (AT-LSTM) model for these tasks. By adding an attention mechanism to the standard LSTM, this model can more easily capture long contextual information.","Taipei, Taiwan","Yuan, Hang  and
Zhang, You  and
Wang, Jin  and
Zhang, Xuejie","Proceedings of the {IJCNLP} 2017, Shared Tasks",,December,208--212,Asian Federation of Natural Language Processing,{YNU}-{HPCC} at {IJCNLP}-2017 Task 5: Multi-choice Question Answering in Exams Using an Attention-based {LSTM} Model,https://aclanthology.org/I17-4035,2017,,,,,
991,inproceedings,sakaguchi-etal-2017-grammatical,"We propose a neural encoder-decoder model with reinforcement learning (NRL) for grammatical error correction (GEC). Unlike conventional maximum likelihood estimation (MLE), the model directly optimizes towards an objective that considers a sentence-level, task-specific evaluation metric, avoiding the exposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in human and automated evaluation metrics, achieving the state-of-the-art on a fluency-oriented GEC corpus.","Taipei, Taiwan","Sakaguchi, Keisuke  and
Post, Matt  and
Van Durme, Benjamin",Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),,November,366--372,Asian Federation of Natural Language Processing,Grammatical Error Correction with Neural Reinforcement Learning,https://aclanthology.org/I17-2062,2017,,,,,
992,inproceedings,tran-etal-2017-named,"Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these models. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of deep neural networks. The second innovation is a bias decoding mechanism that allows the trained system to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train/development/test split of the CoNLL 2003 Shared Task NER dataset.","Taipei, Taiwan","Tran, Quan  and
MacKinlay, Andrew  and
Jimeno Yepes, Antonio",Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),,November,566--575,Asian Federation of Natural Language Processing,Named Entity Recognition with Stack Residual {LSTM} and Trainable Bias Decoding,https://aclanthology.org/I17-1057,2017,,,,,
993,inproceedings,aufrant-etal-2017-dont,"This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any computation overhead.","Valencia, Spain","Aufrant, Lauriane  and
Wisniewski, Guillaume  and
Yvon, Fran{\c{c}}ois","Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",,April,318--323,Association for Computational Linguistics,Don{'}t Stop Me Now! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers,https://aclanthology.org/E17-2051,2017,,,,,
994,inproceedings,pezzelle-etal-2017-precise,"People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a {`}fuzzy{'} measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.","Valencia, Spain","Pezzelle, Sandro  and
Marelli, Marco  and
Bernardi, Raffaella","Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",,April,337--342,Association for Computational Linguistics,Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision,https://aclanthology.org/E17-2054,2017,,,,,
995,inproceedings,lapesa-evert-2017-large,"This paper presents a large-scale evaluation study of dependency-based distributional semantic models. We evaluate dependency-filtered and dependency-structured DSMs in a number of standard semantic similarity tasks, systematically exploring their parameter space in order to give them a {``}fair shot{''} against window-based models. Our results show that properly tuned window-based DSMs still outperform the dependency-based models in most tasks. There appears to be little need for the language-dependent resources and computational cost associated with syntactic analysis.","Valencia, Spain","Lapesa, Gabriella  and
Evert, Stefan","Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",,April,394--400,Association for Computational Linguistics,Large-scale evaluation of dependency-based {DSM}s: Are they worth the effort?,https://aclanthology.org/E17-2063,2017,,,,,
996,inproceedings,raganato-etal-2017-word,"Word Sense Disambiguation is a long-standing task in Natural Language Processing, lying at the core of human language understanding. However, the evaluation of automatic systems has been problematic, mainly due to the lack of a reliable evaluation framework. In this paper we develop a unified evaluation framework and analyze the performance of various Word Sense Disambiguation systems in a fair setup. The results show that supervised systems clearly outperform knowledge-based models. Among the supervised systems, a linear classifier trained on conventional local features still proves to be a hard baseline to beat. Nonetheless, recent approaches exploiting neural networks on unlabeled corpora achieve promising results, surpassing this hard baseline in most test sets.","Valencia, Spain","Raganato, Alessandro  and
Camacho-Collados, Jose  and
Navigli, Roberto","Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",,April,99--110,Association for Computational Linguistics,Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison,https://aclanthology.org/E17-1010,2017,,,,,
997,inproceedings,poddar-etal-2017-author,"User generated content about products and services in the form of reviews are often diverse and even contradictory. This makes it difficult for users to know if an opinion in a review is prevalent or biased. We study the problem of searching for supporting opinions in the context of reviews. We propose a framework called SURF, that first identifies opinions expressed in a review, and then finds similar opinions from other reviews. We design a novel probabilistic graphical model that captures opinions as a combination of aspect, topic and sentiment dimensions, takes into account the preferences of individual authors, as well as the quality of the entity under review, and encodes the flow of thoughts in a review by constraining the aspect distribution dynamically among successive review segments. We derive a similarity measure that considers both lexical and semantic similarity to find supporting opinions. Experiments on TripAdvisor hotel reviews and Yelp restaurant reviews show that our model outperforms existing methods for modeling opinions, and the proposed framework is effective in finding supporting opinions.","Copenhagen, Denmark","Poddar, Lahari  and
Hsu, Wynne  and
Lee, Mong Li",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1049,September,472--481,Association for Computational Linguistics,Author-aware Aspect Topic Sentiment Model to Retrieve Supporting Opinions from Reviews,https://aclanthology.org/D17-1049,2017,,,,,
998,inproceedings,mahendru-etal-2017-promise,"In this paper, we make a simple observation that questions about images often contain premises {--} objects and relationships implied by the question {--} and that reasoning about premises can help Visual Question Answering (VQA) models respond more intelligently to irrelevant or previously unseen questions. When presented with a question that is irrelevant to an image, state-of-the-art VQA models will still answer purely based on learned language biases, resulting in non-sensical or even misleading answers. We note that a visual question is irrelevant to an image if at least one of its premises is false (i.e. not depicted in the image). We leverage this observation to construct a dataset for Question Relevance Prediction and Explanation (QRPE) by searching for false premises. We train novel question relevance detection models and show that models that reason about premises consistently outperform models that do not. We also find that forcing standard VQA models to reason about premises during training can lead to improvements on tasks requiring compositional reasoning.","Copenhagen, Denmark","Mahendru, Aroma  and
Prabhu, Viraj  and
Mohapatra, Akrit  and
Batra, Dhruv  and
Lee, Stefan",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1097,September,926--935,Association for Computational Linguistics,The Promise of Premise: Harnessing Question Premises in Visual Question Answering,https://aclanthology.org/D17-1097,2017,,,,,
999,inproceedings,chaganty-etal-2017-importance,"Knowledge base population (KBP) systems take in a large document corpus and extract entities and their relations. Thus far, KBP evaluation has relied on judgements on the pooled predictions of existing systems. We show that this evaluation is problematic: when a new system predicts a previously unseen relation, it is penalized even if it is correct. This leads to significant bias against new systems, which counterproductively discourages innovation in the field. Our first contribution is a new importance-sampling based evaluation which corrects for this bias by annotating a new system{'}s predictions on-demand via crowdsourcing. We show this eliminates bias and reduces variance using data from the 2015 TAC KBP task. Our second contribution is an implementation of our method made publicly available as an online KBP evaluation service. We pilot the service by testing diverse state-of-the-art systems on the TAC KBP 2016 corpus and obtain accurate scores in a cost effective manner.","Copenhagen, Denmark","Chaganty, Arun  and
Paranjape, Ashwin  and
Liang, Percy  and
Manning, Christopher D.",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1109,September,1038--1048,Association for Computational Linguistics,Importance sampling for unbiased on-demand evaluation of knowledge base population,https://aclanthology.org/D17-1109,2017,,,,,
1000,inproceedings,elsner-shain-2017-speech,"We present the first unsupervised LSTM speech segmenter as a cognitive model of the acquisition of words from unsegmented input. Cognitive biases toward phonological and syntactic predictability in speech are rooted in the limitations of human memory (Baddeley et al., 1998); compressed representations are easier to acquire and retain in memory. To model the biases introduced by these memory limitations, our system uses an LSTM-based encoder-decoder with a small number of hidden units, then searches for a segmentation that minimizes autoencoding loss. Linguistically meaningful segments (e.g. words) should share regular patterns of features that facilitate decoder performance in comparison to random segmentations, and we show that our learner discovers these patterns when trained on either phoneme sequences or raw acoustics. To our knowledge, ours is the first fully unsupervised system to be able to segment both symbolic and acoustic representations of speech.","Copenhagen, Denmark","Elsner, Micha  and
Shain, Cory",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1112,September,1070--1080,Association for Computational Linguistics,Speech segmentation with a neural encoder model of working memory,https://aclanthology.org/D17-1112,2017,,,,,
1001,inproceedings,joseph-etal-2017-constance,"Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on Twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop ConStance, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, ConStance simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by ConStance outperforms a variety of baselines at predicting political stance, while the model{'}s interpretable parameters shed light on the effects of each context.","Copenhagen, Denmark","Joseph, Kenneth  and
Friedland, Lisa  and
Hobbs, William  and
Lazer, David  and
Tsur, Oren",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1116,September,1115--1124,Association for Computational Linguistics,{C}on{S}tance: Modeling Annotation Contexts to Improve Stance Classification,https://aclanthology.org/D17-1116,2017,,,,,
1002,inproceedings,sperber-etal-2017-neural,"The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM{'}s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.","Copenhagen, Denmark","Sperber, Matthias  and
Neubig, Graham  and
Niehues, Jan  and
Waibel, Alex",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1145,September,1380--1389,Association for Computational Linguistics,Neural Lattice-to-Sequence Models for Uncertain Inputs,https://aclanthology.org/D17-1145,2017,,,,,
1003,inproceedings,pujara-etal-2017-sparsity,"Knowledge graph (KG) embedding techniques use structured relationships between entities to learn low-dimensional representations of entities and relations. One prominent goal of these approaches is to improve the quality of knowledge graphs by removing errors and adding missing facts. Surprisingly, most embedding techniques have been evaluated on benchmark datasets consisting of dense and reliable subsets of human-curated KGs, which tend to be fairly complete and have few errors. In this paper, we consider the problem of applying embedding techniques to KGs extracted from text, which are often incomplete and contain errors. We compare the sparsity and unreliability of different KGs and perform empirical experiments demonstrating how embedding approaches degrade as sparsity and unreliability increase.","Copenhagen, Denmark","Pujara, Jay  and
Augustine, Eriq  and
Getoor, Lise",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1184,September,1751--1756,Association for Computational Linguistics,Sparsity and Noise: Where Knowledge Graph Embeddings Fall Short,https://aclanthology.org/D17-1184,2017,,,,,
1004,inproceedings,wang-etal-2017-steering,"We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.","Copenhagen, Denmark","Wang, Di  and
Jojic, Nebojsa  and
Brockett, Chris  and
Nyberg, Eric",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1228,September,2140--2150,Association for Computational Linguistics,Steering Output Style and Topic in Neural Response Generation,https://aclanthology.org/D17-1228,2017,,,,,
1005,inproceedings,tran-etal-2017-preserving,"This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.","Copenhagen, Denmark","Tran, Quan Hung  and
Zukerman, Ingrid  and
Haffari, Gholamreza",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1229,September,2151--2156,Association for Computational Linguistics,Preserving Distributional Information in Dialogue Act Classification,https://aclanthology.org/D17-1229,2017,,,,,
1006,inproceedings,sap-etal-2017-connotation,"The framing of an action influences how we perceive its actor. We introduce connotation frames of power and agency, a pragmatic formalism organized using frame semantic representations, to model how different levels of power and agency are implicitly projected on actors through their actions. We use the new power and agency frames to measure the subtle, but prevalent, gender bias in the portrayal of modern film characters and provide insights that deviate from the well-known Bechdel test. Our contributions include an extended lexicon of connotation frames along with a web interface that provides a comprehensive analysis through the lens of connotation frames.","Copenhagen, Denmark","Sap, Maarten  and
Prasettio, Marcella Cindy  and
Holtzman, Ari  and
Rashkin, Hannah  and
Choi, Yejin",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1247,September,2329--2334,Association for Computational Linguistics,Connotation Frames of Power and Agency in Modern Films,https://aclanthology.org/D17-1247,2017,,,,,
1007,inproceedings,schwartz-etal-2017-assessing,"Recommendations are often rated for their subjective quality, but few researchers have studied comment quality in terms of objective utility. We explore recommendation quality assessment with respect to both subjective (i.e. users{'} ratings) and objective (i.e., did it influence? did it improve decisions?) metrics in a massive online geopolitical forecasting system, ultimately comparing linguistic characteristics of each quality metric. Using a variety of features, we predict all types of quality with better accuracy than the simple yet strong baseline of comment length. Looking at the most predictive content illustrates rater biases; for example, forecasters are subjectively biased in favor of comments mentioning business transactions or dealings as well as material things, even though such comments do not indeed prove any more useful objectively. Additionally, more complex sentence constructions, as evidenced by subordinate conjunctions, are characteristic of comments leading to objective improvements in forecasting.","Copenhagen, Denmark","Schwartz, H. Andrew  and
Rouhizadeh, Masoud  and
Bishop, Michael  and
Tetlock, Philip  and
Mellers, Barbara  and
Ungar, Lyle",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1250,September,2348--2357,Association for Computational Linguistics,Assessing Objective Recommendation Quality through Political Forecasting,https://aclanthology.org/D17-1250,2017,,,,,
1008,inproceedings,ma-etal-2017-investigation,"Monolingual evaluation of Machine Translation (MT) aims to simplify human assessment by requiring assessors to compare the meaning of the MT output with a reference translation, opening up the task to a much larger pool of genuinely qualified evaluators. Monolingual evaluation runs the risk, however, of bias in favour of MT systems that happen to produce translations superficially similar to the reference and, consistent with this intuition, previous investigations have concluded monolingual assessment to be strongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results for show no significant evidence of reference bias in monolingual evaluation of MT.","Copenhagen, Denmark","Ma, Qingsong  and
Graham, Yvette  and
Baldwin, Timothy  and
Liu, Qun",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1262,September,2476--2485,Association for Computational Linguistics,Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation,https://aclanthology.org/D17-1262,2017,,,,,
1009,inproceedings,mathur-etal-2017-sequence,"Manual data annotation is a vital component of NLP research. When designing annotation tasks, properties of the annotation interface can unintentionally lead to artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where annotations of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.","Copenhagen, Denmark","Mathur, Nitika  and
Baldwin, Timothy  and
Cohn, Trevor",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1306,September,2860--2865,Association for Computational Linguistics,Sequence Effects in Crowdsourced Annotations,https://aclanthology.org/D17-1306,2017,,,,,
1010,inproceedings,zhao-etal-2017-men,"Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33{\%} more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68{\%} at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5{\%} and 40.5{\%} for multilabel classification and visual semantic role labeling, respectivelyã€‚","Copenhagen, Denmark","Zhao, Jieyu  and
Wang, Tianlu  and
Yatskar, Mark  and
Ordonez, Vicente  and
Chang, Kai-Wei",Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,10.18653/v1/D17-1323,September,2979--2989,Association for Computational Linguistics,Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints,https://aclanthology.org/D17-1323,2017,,,,,
1011,inproceedings,shwartz-dagan-2016-cogalex,"We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.","Osaka, Japan","Shwartz, Vered  and
Dagan, Ido",Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),,December,80--85,The COLING 2016 Organizing Committee,{C}og{AL}ex-{V} Shared Task: {L}ex{NET} - Integrated Path-based and Distributional Method for the Identification of Semantic Relations,https://aclanthology.org/W16-5310,2016,,,,,
1012,inproceedings,zweigenbaum-etal-2016-supervised,"In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F=0.904 on a random sample of 24 clinical texts with soft line breaks, and F=0.898 on a larger set of mixed clinical texts which may or may not contain soft line breaks, a fairly high value for a method with no manual annotation.","Osaka, Japan","Zweigenbaum, Pierre  and
Grouin, Cyril  and
Lavergne, Thomas",Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),,December,80--88,The COLING 2016 Organizing Committee,Supervised classification of end-of-lines in clinical text with no manual annotation,https://aclanthology.org/W16-5109,2016,,,,,
1013,inproceedings,iwai-etal-2016-method,"In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a {``}generate and validate{''} framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a {``}generate and validate{''} framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation.","Osaka, Japan","Iwai, Miki  and
Takeuchi, Koichi  and
Kageura, Kyo  and
Ishibashi, Kazuya",Proceedings of the 5th International Workshop on Computational Terminology (Computerm2016),,December,30--40,The COLING 2016 Organizing Committee,A Method of Augmenting Bilingual Terminology by Taking Advantage of the Conceptual Systematicity of Terminologies,https://aclanthology.org/W16-4705,2016,,,,,
1014,inproceedings,beloucif-etal-2016-improving,"We introduce a new statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction. We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages, by using the output language semantic structure to further narrow down ITG constraints. This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality. We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training. We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments. Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages. In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.","Osaka, Japan","Beloucif, Meriem  and
Saers, Markus  and
Wu, Dekai",Proceedings of the Sixth Workshop on Hybrid Approaches to Translation ({H}y{T}ra6),,December,51--60,The COLING 2016 Organizing Committee,Improving word alignment for low resource languages using {E}nglish monolingual {SRL},https://aclanthology.org/W16-4507,2016,,,,,
1015,inproceedings,schneevogt-paggio-2016-effect,"Recent studies have demonstrated gender and cultural differences in the recognition of emotions in facial expressions. However, most studies were conducted on American subjects. In this paper, we explore the generalizability of several findings to a non-American culture in the form of Danish subjects. We conduct an emotion recognition task followed by two stereotype questionnaires with different genders and age groups. While recent findings (Krems et al., 2015) suggest that women are biased to see anger in neutral facial expressions posed by females, in our sample both genders assign higher ratings of anger to all emotions expressed by females. Furthermore, we demonstrate an effect of gender on the fear-surprise-confusion observed by Tomkins and McCarter (1964); females overpredict fear, while males overpredict surprise.","Osaka, Japan","Schneevogt, Daniela  and
Paggio, Patrizia","Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",,December,11--19,The COLING 2016 Organizing Committee,The Effect of Gender and Age Differences on the Recognition of Emotions from Facial Expressions,https://aclanthology.org/W16-4302,2016,,,,,
1016,article,gutierrez-etal-2016-detecting,"Understanding cross-cultural differences has important implications for world affairs and many aspects of the life of society. Yet, the majority of text-mining methods to date focus on the analysis of monolingual texts. In contrast, we present a statistical model that simultaneously learns a set of common topics from multilingual, non-parallel data and automatically discovers the differences in perspectives on these topics across linguistic communities. We perform a behavioural evaluation of a subset of the differences identified by our model in English and Spanish to investigate their psychological validity.","Cambridge, MA","Guti{\'e}rrez, E.D.  and
Shutova, Ekaterina  and
Lichtenstein, Patricia  and
de Melo, Gerard  and
Gilardi, Luca",,10.1162/tacl_a_00082,,47--60,MIT Press,Detecting Cross-Cultural Differences Using a Multilingual Topic Model,https://aclanthology.org/Q16-1004,2016,Transactions of the Association for Computational Linguistics,4,,,
1017,article,gulordava-merlo-2016-multi,"The growing work in multi-lingual parsing faces the challenge of fair comparative evaluation and performance analysis across languages and their treebanks. The difficulty lies in teasing apart the properties of treebanks, such as their size or average sentence length, from those of the annotation scheme, and from the linguistic properties of languages. We propose a method to evaluate the effects of word order of a language on dependency parsing performance, while controlling for confounding treebank properties. The method uses artificially-generated treebanks that are minimal permutations of actual treebanks with respect to two word order properties: word order variation and dependency lengths. Based on these artificial data on twelve languages, we show that longer dependencies and higher word order variability degrade parsing performance. Our method also extends to minimal pairs of individual sentences, leading to a finer-grained understanding of parsing errors.","Cambridge, MA","Gulordava, Kristina  and
Merlo, Paola",,10.1162/tacl_a_00103,,343--356,MIT Press,Multi-lingual Dependency Parsing Evaluation: a Large-scale Analysis of Word Order Properties using Artificial Data,https://aclanthology.org/Q16-1025,2016,Transactions of the Association for Computational Linguistics,4,,,
1018,article,arora-etal-2016-latent,"Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods. This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.","Cambridge, MA","Arora, Sanjeev  and
Li, Yuanzhi  and
Liang, Yingyu  and
Ma, Tengyu  and
Risteski, Andrej",,10.1162/tacl_a_00106,,385--399,MIT Press,A Latent Variable Model Approach to {PMI}-based Word Embeddings,https://aclanthology.org/Q16-1028,2016,Transactions of the Association for Computational Linguistics,4,,,
1019,inproceedings,yaneva-etal-2016-corpus,"The paper presents a corpus of text data and its corresponding gaze fixations obtained from autistic and non-autistic readers. The data was elicited through reading comprehension testing combined with eye-tracking recording. The corpus consists of 1034 content words tagged with their POS, syntactic role and three gaze-based measures corresponding to the autistic and control participants. The reading skills of the participants were measured through multiple-choice questions and, based on the answers given, they were divided into groups of skillful and less-skillful readers. This division of the groups informs researchers on whether particular fixations were elicited from skillful or less-skillful readers and allows a fair between-group comparison for two levels of reading ability. In addition to describing the process of data collection and corpus development, we present a study on the effect that word length has on reading in autism. The corpus is intended as a resource for investigating the particular linguistic constructions which pose reading difficulties for people with autism and hopefully, as a way to inform future text simplification research intended for this population.","Portoro{\v{z}}, Slovenia","Yaneva, Victoria  and
Temnikova, Irina  and
Mitkov, Ruslan",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,480--487,European Language Resources Association (ELRA),A Corpus of Text Data and Gaze Fixations from Autistic and Non-Autistic Adults,https://aclanthology.org/L16-1077,2016,,,,,
1020,inproceedings,sidarenka-2016-potts,"In this paper, we introduce a novel comprehensive dataset of 7,992 German tweets, which were manually annotated by two human experts with fine-grained opinion relations. A rich annotation scheme used for this corpus includes such sentiment-relevant elements as opinion spans, their respective sources and targets, emotionally laden terms with their possible contextual negations and modifiers. Various inter-annotator agreement studies, which were carried out at different stages of work on these data (at the initial training phase, upon an adjudication step, and after the final annotation run), reveal that labeling evaluative judgements in microblogs is an inherently difficult task even for professional coders. These difficulties, however, can be alleviated by letting the annotators revise each other{'}s decisions. Once rechecked, the experts can proceed with the annotation of further messages, staying at a fairly high level of agreement.","Portoro{\v{z}}, Slovenia","Sidarenka, Uladzimir",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,1133--1141,European Language Resources Association (ELRA),{P}ot{TS}: The {P}otsdam {T}witter Sentiment Corpus,https://aclanthology.org/L16-1181,2016,,,,,
1021,inproceedings,maynard-bontcheva-2016-challenges,"This paper discusses the challenges in carrying out fair comparative evaluations of sentiment analysis systems. Firstly, these are due to differences in corpus annotation guidelines and sentiment class distribution. Secondly, different systems often make different assumptions about how to interpret certain statements, e.g. tweets with URLs. In order to study the impact of these on evaluation results, this paper focuses on tweet sentiment analysis in particular. One existing and two newly created corpora are used, and the performance of four different sentiment analysis systems is reported; we make our annotated datasets and sentiment analysis applications publicly available. We see considerable variations in results across the different corpora, which calls into question the validity of many existing annotated datasets and evaluations, and we make some observations about both the systems and the datasets as a result.","Portoro{\v{z}}, Slovenia","Maynard, Diana  and
Bontcheva, Kalina",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,1142--1148,European Language Resources Association (ELRA),Challenges of Evaluating Sentiment Analysis Tools on Social Media,https://aclanthology.org/L16-1182,2016,,,,,
1022,inproceedings,postma-etal-2016-addressing,"Word Sense Disambiguation (WSD) systems tend to have a strong bias towards assigning the Most Frequent Sense (MFS), which results in high performance on the MFS but in a very low performance on the less frequent senses. We addressed the MFS bias in WSD systems by combining the output from a WSD system with a set of mostly static features to create a MFS classifier to decide when to and not to choose the MFS. The output from this MFS classifier, which is based on the Random Forest algorithm, is then used to modify the output from the original WSD system. We applied our classifier to one of the state-of-the-art supervised WSD systems, i.e. IMS, and to of the best state-of-the-art unsupervised WSD systems, i.e. UKB. Our main finding is that we are able to improve the system output in terms of choosing between the MFS and the less frequent senses. When we apply the MFS classifier to fine-grained WSD, we observe an improvement on the less frequent sense cases, whereas we maintain the overall recall.","Portoro{\v{z}}, Slovenia","Postma, Marten  and
Izquierdo, Ruben  and
Agirre, Eneko  and
Rigau, German  and
Vossen, Piek",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,1695--1700,European Language Resources Association (ELRA),Addressing the {MFS} Bias in {WSD} systems,https://aclanthology.org/L16-1268,2016,,,,,
1023,inproceedings,el-haj-etal-2016-learning,"Attribution bias refers to the tendency of people to attribute successes to their own abilities but failures to external factors. In a business context an internal factor might be the restructuring of the firm and an external factor might be an unfavourable change in exchange or interest rates. In accounting research, the presence of an attribution bias has been demonstrated for the narrative sections of the annual financial reports. Previous studies have applied manual content analysis to this problem but in this paper we present novel work to automate the analysis of attribution bias through using machine learning algorithms. Previous studies have only applied manual content analysis on a small scale to reveal such a bias in the narrative section of annual financial reports. In our work a group of experts in accounting and finance labelled and annotated a list of 32,449 sentences from a random sample of UK Preliminary Earning Announcements (PEAs) to allow us to examine whether sentences in PEAs contain internal or external attribution and which kinds of attributions are linked to positive or negative performance. We wished to examine whether human annotators could agree on coding this difficult task and whether Machine Learning (ML) could be applied reliably to replicate the coding process on a much larger scale. Our best machine learning algorithm correctly classified performance sentences with 70{\%} accuracy and detected tone and attribution in financial PEAs with accuracy of 79{\%}.","Portoro{\v{z}}, Slovenia","El-Haj, Mahmoud  and
Rayson, Paul  and
Young, Steve  and
Moore, Andrew  and
Walker, Martin  and
Schleicher, Thomas  and
Athanasakou, Vasiliki",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,1820--1825,European Language Resources Association (ELRA),Learning Tone and Attribution for Financial Text Mining,https://aclanthology.org/L16-1287,2016,,,,,
1024,inproceedings,gamback-das-2016-comparing,"Social media texts are often fairly informal and conversational, and when produced by bilinguals tend to be written in several different languages simultaneously, in the same way as conversational speech. The recent availability of large social media corpora has thus also made large-scale code-switched resources available for research. The paper addresses the issues of evaluation and comparison these new corpora entail, by defining an objective measure of corpus level complexity of code-switched texts. It is also shown how this formal measure can be used in practice, by applying it to several code-switched corpora.","Portoro{\v{z}}, Slovenia","Gamb{\""a}ck, Bj{\""o}rn  and
Das, Amitava",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,1850--1855,European Language Resources Association (ELRA),Comparing the Level of Code-Switching in Corpora,https://aclanthology.org/L16-1292,2016,,,,,
1025,inproceedings,hoenen-2016-wikipedia,"In this paper, we investigate a covert labeling cue, namely the probability that a title (by example of the Wikipedia titles) is a noun. If this probability is very large, any list such as or comparable to the Wikipedia titles can be used as a reliable word-class (or part-of-speech tag) predictor or noun lexicon. This may be especially useful in the case of Low Resource Languages (LRL) where labeled data is lacking and putatively for Natural Language Processing (NLP) tasks such as Word Sense Disambiguation, Sentiment Analysis and Machine Translation. Profitting from the ease of digital publication on the web as opposed to print, LRL speaker communities produce resources such as Wikipedia and Wiktionary, which can be used for an assessment. We provide statistical evidence for a strong noun bias for the Wikipedia titles from 2 corpora (English, Persian) and a dictionary (Japanese) and for a typologically balanced set of 17 languages including LRLs. Additionally, we conduct a small experiment on predicting noun tags for out-of-vocabulary items in part-of-speech tagging for English.","Portoro{\v{z}}, Slovenia","Hoenen, Armin",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,2114--2118,European Language Resources Association (ELRA),{W}ikipedia Titles As Noun Tag Predictors,https://aclanthology.org/L16-1335,2016,,,,,
1026,inproceedings,batanovic-etal-2016-reliable,"Collecting data for sentiment analysis in resource-limited languages carries a significant risk of sample selection bias, since the small quantities of available data are most likely not representative of the whole population. Ignoring this bias leads to less robust machine learning classifiers and less reliable evaluation results. In this paper we present a dataset balancing algorithm that minimizes the sample selection bias by eliminating irrelevant systematic differences between the sentiment classes. We prove its superiority over the random sampling method and we use it to create the Serbian movie review dataset â€• SerbMR â€• the first balanced and topically uniform sentiment analysis dataset in Serbian. In addition, we propose an incremental way of finding the optimal combination of simple text processing options and machine learning features for sentiment classification. Several popular classifiers are used in conjunction with this evaluation approach in order to establish strong but reliable baselines for sentiment analysis in Serbian.","Portoro{\v{z}}, Slovenia","Batanovi{\'c}, Vuk  and
Nikoli{\'c}, Bo{\v{s}}ko  and
Milosavljevi{\'c}, Milan",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,2688--2696,European Language Resources Association (ELRA),Reliable Baselines for Sentiment Analysis in Resource-Limited Languages: The {S}erbian Movie Review Dataset,https://aclanthology.org/L16-1427,2016,,,,,
1027,inproceedings,temnikova-etal-2016-applying,"The goal of the cognitive machine translation (MT) evaluation approach is to build classifiers which assign post-editing effort scores to new texts. The approach helps estimate fair compensation for post-editors in the translation industry by evaluating the cognitive difficulty of post-editing MT output. The approach counts the number of errors classified in different categories on the basis of how much cognitive effort they require in order to be corrected. In this paper, we present the results of applying an existing cognitive evaluation approach to Modern Standard Arabic (MSA). We provide a comparison of the number of errors and categories of errors in three MSA texts of different MT quality (without any language-specific adaptation), as well as a comparison between MSA texts and texts from three Indo-European languages (Russian, Spanish, and Bulgarian), taken from a previous experiment. The results show how the error distributions change passing from the MSA texts of worse MT quality to MSA texts of better MT quality, as well as a similarity in distinguishing the texts of better MT quality for all four languages.","Portoro{\v{z}}, Slovenia","Temnikova, Irina  and
Zaghouani, Wajdi  and
Vogel, Stephan  and
Habash, Nizar",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,3644--3651,European Language Resources Association (ELRA),Applying the Cognitive Machine Translation Evaluation Approach to {A}rabic,https://aclanthology.org/L16-1578,2016,,,,,
1028,inproceedings,fulgoni-etal-2016-empirical,News sources frame issues in different ways in order to appeal or control the perception of their readers. We present a large scale study of news articles from partisan sources in the US across a variety of different issues. We first highlight that differences between sides exist by predicting the political leaning of articles of unseen political bias. Framing can be driven by different types of morality that each group values. We emphasize differences in framing of different news building on the moral foundations theory quantified using hand crafted lexicons. Our results show that partisan sources frame political issues differently both in terms of words usage and through the moral foundations they relate to.,"Portoro{\v{z}}, Slovenia","Fulgoni, Dean  and
Carpenter, Jordan  and
Ungar, Lyle  and
Preo{\c{t}}iuc-Pietro, Daniel",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,3730--3736,European Language Resources Association (ELRA),An Empirical Exploration of Moral Foundations Theory in Partisan News Sources,https://aclanthology.org/L16-1591,2016,,,,,
1029,inproceedings,brognaux-etal-2016-combining,"Text-to-speech has long been centered on the production of an intelligible message of good quality. More recently, interest has shifted to the generation of more natural and expressive speech. A major issue of existing approaches is that they usually rely on a manual annotation in expressive styles, which tends to be rather subjective. A typical related issue is that the annotation is strongly influenced â€• and possibly biased â€• by the semantic content of the text (e.g. a shot or a fault may incite the annotator to tag that sequence as expressing a high degree of excitation, independently of its acoustic realization). This paper investigates the assumption that human annotation of basketball commentaries in excitation levels can be automatically improved on the basis of acoustic features. It presents two techniques for label correction exploiting a Gaussian mixture and a proportional-odds logistic regression. The automatically re-annotated corpus is then used to train HMM-based expressive speech synthesizers, the performance of which is assessed through subjective evaluations. The results indicate that the automatic correction of the annotation with Gaussian mixture helps to synthesize more contrasted excitation levels, while preserving naturalness.","Portoro{\v{z}}, Slovenia","Brognaux, Sandrine  and
Fran{\c{c}}ois, Thomas  and
Saerens, Marco",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,3872--3879,European Language Resources Association (ELRA),Combining Manual and Automatic Prosodic Annotation for Expressive Speech Synthesis,https://aclanthology.org/L16-1613,2016,,,,,
1030,inproceedings,santus-etal-2016-nine,"ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7{\%}, against a baseline of 57.2{\%} (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline. hypernyms-co-hyponyms 95.7{\%} vs. 69.8{\%}, hypernyms-random 91.8{\%} vs. 64.1{\%} and co-hyponyms-random 97.8{\%} vs. 79.4{\%}. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.","Portoro{\v{z}}, Slovenia","Santus, Enrico  and
Lenci, Alessandro  and
Chiu, Tin-Shing  and
Lu, Qin  and
Huang, Chu-Ren",Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),,May,4557--4564,European Language Resources Association (ELRA),Nine Features in a Random Forest to Learn Taxonomical Semantic Relations,https://aclanthology.org/L16-1722,2016,,,,,
1031,inproceedings,kim-choi-2016-mages,"This demo presents MAGES (multilingual angle-integrated grouping-based entity summarization), an entity summarization system for a large knowledge base such as DBpedia based on a entity-group-bound ranking in a single integrated entity space across multiple language-specific editions. MAGES offers a multilingual angle-integrated space model, which has the advantage of overcoming missing semantic tags (i.e., categories) caused by biases in different language communities, and can contribute to the creation of entity groups that are well-formed and more stable than the monolingual condition within it. MAGES can help people quickly identify the essential points of the entities when they search or browse a large volume of entity-centric data. Evaluation results on the same experimental data demonstrate that our system produces a better summary compared with other representative DBpedia entity summarization methods.","Osaka, Japan","Kim, Eun-kyung  and
Choi, Key-Sun","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",,December,203--207,The COLING 2016 Organizing Committee,{MAGES}: A Multilingual Angle-integrated Grouping-based Entity Summarization System,https://aclanthology.org/C16-2043,2016,,,,,
1032,inproceedings,more-tsarfaty-2016-data,"Parsing texts into universal dependencies (UD) in realistic scenarios requires infrastructure for the morphological analysis and disambiguation (MA{\&}D) of typologically different languages as a first tier. MA{\&}D is particularly challenging in morphologically rich languages (MRLs), where the ambiguous space-delimited tokens ought to be disambiguated with respect to their constituent morphemes, each morpheme carrying its own tag and a rich set features. Here we present a novel, language-agnostic, framework for MA{\&}D, based on a transition system with two variants {---} word-based and morpheme-based {---} and a dedicated transition to mitigate the biases of variable-length morpheme sequences. Our experiments on a Modern Hebrew case study show state of the art results, and we show that the morpheme-based MD consistently outperforms our word-based variant. We further illustrate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks.","Osaka, Japan","More, Amir  and
Tsarfaty, Reut","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",,December,337--348,The COLING 2016 Organizing Committee,Data-Driven Morphological Analysis and Disambiguation for Morphologically Rich Languages and {U}niversal {D}ependencies,https://aclanthology.org/C16-1033,2016,,,,,
1033,inproceedings,garimella-etal-2016-identifying,"Personal writings have inspired researchers in the fields of linguistics and psychology to study the relationship between language and culture to better understand the psychology of people across different cultures. In this paper, we explore this relation by developing cross-cultural word models to identify words with cultural bias {--} i.e., words that are used in significantly different ways by speakers from different cultures. Focusing specifically on two cultures: United States and Australia, we identify a set of words with significant usage differences, and further investigate these words through feature analysis and topic modeling, shedding light on the attributes of language that contribute to these differences.","Osaka, Japan","Garimella, Aparna  and
Mihalcea, Rada  and
Pennebaker, James","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",,December,674--683,The COLING 2016 Organizing Committee,Identifying Cross-Cultural Differences in Word Usage,https://aclanthology.org/C16-1065,2016,,,,,
1034,inproceedings,hu-etal-2016-different,"Recent work for learning word representations has applied successfully to many NLP applications, such as sentiment analysis and question answering. However, most of these models assume a single vector per word type without considering polysemy and homonymy. In this paper, we present an extension to the CBOW model which not only improves the quality of embeddings but also makes embeddings suitable for polysemy. It differs from most of the related work in that it learns one semantic center embedding and one context bias instead of training multiple embeddings per word type. Different context leads to different bias which is defined as the weighted average embeddings of local context. Experimental results on similarity task and analogy task show that the word representations learned by the proposed method outperform the competitive baselines.","Osaka, Japan","Hu, Wenpeng  and
Zhang, Jiajun  and
Zheng, Nan","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",,December,762--771,The COLING 2016 Organizing Committee,Different Contexts Lead to Different Word Embeddings,https://aclanthology.org/C16-1073,2016,,,,,
1035,inproceedings,terkik-etal-2016-analyzing,"University students in the United States are routinely asked to provide feedback on the quality of the instruction they have received. Such feedback is widely used by university administrators to evaluate teaching ability, despite growing evidence that students assign lower numerical scores to women and people of color, regardless of the actual quality of instruction. In this paper, we analyze students{'} written comments on faculty evaluation forms spanning eight years and five STEM disciplines in order to determine whether open-ended comments reflect these same biases. First, we apply sentiment analysis techniques to the corpus of comments to determine the overall affect of each comment. We then use this information, in combination with other features, to explore whether there is bias in how students describe their instructors. We show that while the gender of the evaluated instructor does not seem to affect students{'} expressed level of overall satisfaction with their instruction, it does strongly influence the language that they use to describe their instructors and their experience in class.","Osaka, Japan","Terkik, Andamlak  and
Prud{'}hommeaux, Emily  and
Ovesdotter Alm, Cecilia  and
Homan, Christopher  and
Franklin, Scott","Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",,December,868--876,The COLING 2016 Organizing Committee,Analyzing Gender Bias in Student Evaluations,https://aclanthology.org/C16-1083,2016,,,,,
1036,inproceedings,qian-etal-2016-modal,"Classical theories of discourse semantics, such as Discourse Representation Theory (DRT), Dynamic Predicate Logic (DPL), predict that an indefinite noun phrase cannot serve as antecedent for an anaphor if the noun phrase is, but the anaphor is not, in the scope of a modal expression. However, this prediction meets with counterexamples. The phenomenon modal subordination is one of them. In general, modal subordination is concerned with more than two modalities, where the modality in subsequent sentences is interpreted in a context {`}subordinate{'} to the one created by the first modal expression. In other words, subsequent sentences are interpreted as being conditional on the scenario introduced in the first sentence. One consequence is that the anaphoric potential of indefinites may extend beyond the standard limits of accessibility constraints. This paper aims to give a formal interpretation on modal subordination. The theoretical backbone of the current work is Type Theoretic Dynamic Logic (TTDL), which is a Montagovian account of discourse semantics. Different from other dynamic theories, TTDL was built on classical mathematical and logical tools, such as Î»-calculus and Church{'}s theory of types. Hence it is completely compositional and does not suffer from the destructive assignment problem. We will review the basic set-up of TTDL and then present Kratzer{'}s theory on natural language modality. After that, by integrating the notion of conversation background, in particular, the modal base usage, we offer an extension of TTDL (called Modal-TTDL, or M-TTDL in short) which properly deals with anaphora across modality. The formal relation between Modal-TTDL and TTDL will be discussed as well. We uncover the difficulty of specific sense distinctions by investigating distributional bias and reducing the sparsity of existing small-scale corpora used in prior work. We build a semantically enriched model for modal sense classification by designing novel features related to lexical, proposition-level and discourse-level semantic factors. Besides improved classification performance, closer examination of interpretable feature sets unveils relevant semantic and contextual factors in modal sense classification. Finally, we investigate genre effects on modal sense distribution and how they affect classification performance. Our investigations uncover the difficulty of specific sense distinctions and how they are affected by training set size and distributional bias. Our large-scale experiments confirm that semantically enriched models outperform models built on shallow feature sets. Cross-genre experiments shed light on differences in sense distributions across genres and confirm that semantically enriched models have high generalization capacity, especially in unstable distributional settings.",,"Qian, Sai  and
de Groote, Philippe  and
Amblard, Maxime","Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning",,sept,,CSLI Publications,Modal Subordination in Type Theoretic Dynamic Logic,https://aclanthology.org/2016.lilt-14.2,2016,,,,,
1037,inproceedings,moon-etal-2016-selective,"Modal auxiliaries have different readings, depending on the context in which they occur (Kratzer, 1981). Several projects have attempted to classify uses of modal auxiliaries in corpora according to their reading using supervised machine learning techniques (e.g., Rubinstein et al., 2013, Ruppenhofer {\&} Rehbein, 2012). In each study, traditional taxonomic labels, such as {`}epistemic{'} and {`}deontic{'} are used by human annotators to label instances of modal auxiliaries in a corpus. In order to achieve higher agreement among annotators, results in these previous studies are reported after collapsing some of the initial categories. The results show that human annotators have fairly good agreement on some of the categories, such as whether or not a use is epistemic, but poor agreement on others. They also show that annotators agree more on modals such as might than on modals such as could. In this study, we used traditional taxonomic categories on sentences containing modal auxiliary verbs that were randomly extracted from the English Gigaword 4th edition corpus (Parker et al., 2009). The lowest inner-annotator agreement using traditional taxonomic labels occurred with uses of could, with raw agreements of 42{\%}âˆ’48{\%} (Îº = 0.196âˆ’0.259), compared to might, for instance, with raw agreement of 98{\%}. In response to the low numbers, rather than collapsing traditional categories, we tried a new method of classifying uses of could with respect to where the reading situates the eventuality being described relative to the speech time. For example, the sentence {`}Jess could swim.{'} is about a swimming eventuality in the past leading up to the time of speech, if it is read as being an ability. The sentence is about a swimming eventuality in the future, if it is read as being a statement about a possibility. The classification labels we propose are crucial in separating uses of could that have actuality inferences (Bhatt, 1999, Hacquard, 2006) from uses that do not. For the temporal location of the event described by a use of could, using four category labels, we achieved 73{\%}âˆ’90{\%} raw agreement (Îº = 0.614âˆ’0.744). Sequence of tense contexts (Abusch, 1997) present a major factor in the difficulty of determining the temporal properties present in uses of could. Among three annotators, we achieved raw agreement scores of 89{\%}âˆ’96{\%}(Îº =0.779âˆ’0.919{\%}) on identification of sequence of tense contexts. We discuss the role of our findings with respect to textual entailment.",,"Moon, Lori  and
Kirvaitis, Patricija  and
Madden, Noreen","Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning",,sept,,CSLI Publications,Selective Annotation of Modal Readings: Delving into the Difficult Data,https://aclanthology.org/2016.lilt-14.6,2016,,,,,
1038,inproceedings,chen-etal-2016-guided,"In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3{\%}. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1{\%} BLEU absolute.","Austin, TX, USA","Chen, Wenhu  and
Matusov, Evgeny  and
Khadivi, Shahram  and
Peter, Jan-Thorsten",Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,,October 28 - November 1,121--134,The Association for Machine Translation in the Americas,Guided Alignment Training for Topic-Aware Neural Machine Translation,https://aclanthology.org/2016.amta-researchers.10,2016,,,,,
